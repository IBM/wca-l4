{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) introduces IBM watsonx Code Assistant's generative AI capabilities and lays the groundwork for the hands-on training that will follow. [6 min] i. Automation is indispensable to modern IT strategy Despite the innovations and advancements made in the domain of automation, IBM sellers and partners know first-hand from discussions with clients that many businesses are still struggling to keep up with their IT operations. The rapid pace of technological innovation\u2014 in particular, areas such as AI and machine learning \u2014are obviously challenging for any organization to strategize and plan around. But smaller, more practical challenges also stand in the way of these businesses. The fact remains that IT operations, and wrangling those operations in an efficient and streamlined manner, remains a difficult problem to solve. Three primary pain points that IBM consistently hears from the marketplace include: an ever-increasing skills gap in IT management; that Day 2 operations continue to be labor-intensive, mostly manual endeavors; and that the complexity of the systems needing to be managed are out-pacing many organization\u2019s ability to adapt. All of these pain points are potential automation challenges to be solved. Each of them impedes a company's ability to move quickly and adapt for the future. And as such, for many IBM clients, solving these automation challenges have become an indispensable element in their strategy to modernize IT. UNPRECEDENTED RATE OF GENERATIVE AI ADOPTION Even though generative AI is relatively new, the widespread popularity of ChatGPT has created significant interest in the notion of large language models (LLMs) and foundation models (FMs) \u2014 and what they can do for business. It took quite some time for enterprises to start moving toward traditional AI. In contrast, generative AI has experienced massive early adoption: 80% of enterprises are already working with, or planning to leverage FMs, and plan to adopt generative AI in their use cases and workflow. Moreover, the following data points to an ever-growing adoption trend for generative AI: Scale Zeitgeist 2023 AI Readiness Report notes that with the companies they reviewed, 21% have generative AI models in production; 29% are experimenting with generative AI and another 31% are planning to work with generative AI models; a total of 81% are either working with or planning to work with generative AI models Goldman Sachs has estimated that generative AI will have a very deep economic impact \u2013 raising global Gross Domestic Product (GDP) by 7% within 10 years, reflecting the technology\u2019s huge potential. Boston Consulting Group (BCG) noted that generative AI is expected to represent 30% of the overall market by 2025 ii. Generative AI-assisted code lifecycle management achieves what LLMs alone cannot Following the debut of OpenAI's ChatGPT, the marketplace has been awash with competing large language model (LLM) and generative AI-based assistants. It's one thing to train and deploy an LLM; it's another thing entirely to make it applicable and tangibly beneficial for business. What separates IBM watsonx Code Assistant (WCA) offerings from competing vendors in the marketplace? The design and implementation of WCA is purposely built to assist, using generative AI, software and code lifecycle management. In short, generative AI-assisted code lifecycle management helps to achieve what large language models cannot achieve on their own. It is what distinguished WCA from other code assistants in the marketplace today. Code lifecycle management begins with understanding client code, through training across a myriad of programming languages and specializations in paradigms such as Ansible Automation Platform, and applies that understanding across a client\u2019s application and runtime environments. Users are able to plan next steps based on generative AI analysis of their existing application code. Operations teams can rapidly transform their codebases with optimized design and architecture that is recommended according to IBM Granite's best-practice models. Administrators can validate the outcomes with automatically generated unit tests. Afterwards, they can deploy those services and applications using automated processes like Ansible's automation engine. Over the course of that application or code's lifecycle, generative AI can maintain healthy operations with runtime insights. iii. Introducing the IBM watsonx Code Assistant product family IBM watsonx Code Assistant is the flagship offering in a suite of generative AI code assistant products, which also include offerings for Ansible Automation Platform (IBM watsonx Code Assistant for Red Hat Ansible Lightspeed) and IBM Systems modernization (IBM watsonx Code Assistant for Z). These solutions accelerate software development tasks with AI-powered capabilities including context-aware code generation, explanation, documentation, translation, and unit test generation. It does so while maintaining the principles of trust, security, and compliance with regards to IBM client's data and intellectual property. Developers and IT Operators can utilize WCA to speed up application modernization efforts and generate Ansible-based automation jobs to rapidly scale out (or scale up) IT environments. IBM watsonx Code Assistant products are powered by IBM Granite foundation models that include state-of-the-art large language models designed for code. For offerings such as WCA for Ansible Lightspeed and WCA for Z, bespoke code models\u2014 tailored to working with Ansible Automation Platform and COBOL-to-Z use cases, respectively \u2014are invoked. Universally true for all of the watsonx Code Assistant offerings is that they are geared towards helping IT teams create high-quality code using AI-generated recommendations, based on natural language requests or existing source code. These AI models, and the recommendations they generate, are seamlessly integrated via extensions with the world's most popular developer integrated development environments (IDE), including Visual Studio Code and Eclipse. Granite is IBM\u2019s flagship brand of open and proprietary LLMs, spanning multiple modalities. Granite models exist for code, languages, time series, and GeoSpatial \u2014 with additional modalities expected in future. IBM Granite code models are a series of decoder-only models for code generative tasks, trained with code written in 116 different programming languages. The Granite code models family consists of models ranging in size from 3 to 34 billion parameters, in both a base model and instruction-following model variants. These models have a range of uses, from complex application modernization tasks to on-device memory-constrained use cases. The larger the block size for a particular language on this chart, the larger percentage of training corpus data of that language was used to train the Granite code model. Languages and formats such as Java, C, JSON, JavaScript, HTML, and PHP are subjects in which the model \u201cMajors\u201d and excels. Other languages such as Ruby, SQL, and Swift could be considered \u201cMinors\u201d where the generalized code model can work with the language, but has less training data to base those recommendations on. These percentages and training data volumes will continue to evolve as the Granite code models mature. WATSONX CODE ASSISTANT vs. WCA FOR ANSIBLE LIGHTSPEED? For those familiar with other IBM watsonx Code Assistant offerings\u2014 such as WCA for Red Hat Ansible Lightspeed and WCA for Z \u2014the generalized code model approach, as seen here, differs from the specialized code model approach of those two aforementioned offerings. The WCA for Ansible Lightspeed flavor of IBM Granite code models specializes (Majors) only in Ansible Playbooks and YAML Similarly, the IBM Granite code model used by WCA for Z specializes in transforming COBOL mainframe code into modernized Java code Ansible Playbooks (YAML) and mainframe (COBOL) code are both supported (Minor) languages for the generalized IBM Granite code models\u2014 and therefore are supported by IBM watsonx Code Assistant \u2014but if a client wishes to specialize in those particular languages and frameworks, they would be well advised to utilize the bespoke WCA for Ansible Lightspeed and WCA for Z offerings, respectively, to do so. iv. Solution architecture of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed IBM watsonx Code Assistant for Red Hat Ansible Lightspeed meets developers where they are: with a rich plugin via VS Code extensions, where developers input their prompts directly in the code editor. Prompts are sent to the Ansible Lightspeed service, and the service sends a suggestion back (a completion ) that\u2019s powered by IBM Granite LLMs for code. It is important to note that all data in transit is encrypted and ephemeral so users can be confident and have trust in the security of the service during this exchange. In terms of data security, client Ansible playbooks and customized models that they may potentially have are stored in client-owned Cloud Object Storage and are not shared with IBM, Red Hat, or any other clients. In order to utilize IBM watsonx Code Assistant for Red Hat Ansible Lightspeed, a client must have an existing license for Red Hat Ansible Automation Platform (the \u201dred tile\u201d component in the center of the diagram), as well as a license for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed (the \u201cblue tile\u201d on the right of the diagram). Generative AI has recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. For example, if trained on a large dataset of Ansible Playbooks, generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style or standards of that particular company. WHAT ARE PLAYBOOKS? Ansible Playbooks instruct Ansible\u2019s automation engine on how to execute tasks in a step-by-step manner. Playbooks defines roles, tasks, handlers, and other configurations; in turn, these attributes allow developers and users to codify complex orchestration scenarios. Conceptually, think of a Playbook as a recipe book for system administration: each recipe (or Playbook) spells out the steps required to achieve a particular system state or to complete a given operation. One of the standout features of Ansible Playbooks is that they are idempotent : executing Playbooks multiple times on the same system won't create additional \"side effects\" (unintended operations or creation of unwanted artifacts) after the first successful run. This ensures consistency and reliability across deployments of the Red Hat Ansible Automation Platform ( AAP ). As you will see throughout the hands-on training material, generative AI models provide a natural language prompt to users which in turn is understood and translated by the AI models into the necessary Ansible Task code. For example, a user might describe a desired system state in plain language ( \"I want a Playbook to install and start an Apache web server\" ) and the model will generate the appropriate Ansible Tasks for a Playbook. All of this is achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within a company with limited Ansible or programming expertise will be able to produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing of AI-generated code will be needed before being put into production. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality. v. Lab objectives The material covered for this hands-on training is intended to prepare IBM sellers and business partners with the skills necessary to create Ansible automation tasks using the generative AI capabilities of WCA. The curriculum will leverage WCA's generative AI code recommendations for automating cloud-based and infrastructure-based automation tasks. In-depth explanations accompanying Ansible Playbook templates will also explain: How WCA uses natural language prompts , as well as Ansible Playbook contents, to generate contextually-aware Task code recommendations Post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How WCA provides content source matching attribution and \"explainability\" for all AI-generated content Leveraging WCA's model tuning capabilities to tailor content and code recommendations to an organization's standards, best practices, and programming styles vi. Next steps The module ahead will outline the evaluation criteria for IBM sellers and business partners. Afterwards, you will setup your local environment with the necessary pre-requisites for getting started with the hands-on material.","title":"Introduction"},{"location":"#_1","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) introduces IBM watsonx Code Assistant's generative AI capabilities and lays the groundwork for the hands-on training that will follow. [6 min]","title":""},{"location":"#i-automation-is-indispensable-to-modern-it-strategy","text":"Despite the innovations and advancements made in the domain of automation, IBM sellers and partners know first-hand from discussions with clients that many businesses are still struggling to keep up with their IT operations. The rapid pace of technological innovation\u2014 in particular, areas such as AI and machine learning \u2014are obviously challenging for any organization to strategize and plan around. But smaller, more practical challenges also stand in the way of these businesses. The fact remains that IT operations, and wrangling those operations in an efficient and streamlined manner, remains a difficult problem to solve. Three primary pain points that IBM consistently hears from the marketplace include: an ever-increasing skills gap in IT management; that Day 2 operations continue to be labor-intensive, mostly manual endeavors; and that the complexity of the systems needing to be managed are out-pacing many organization\u2019s ability to adapt. All of these pain points are potential automation challenges to be solved. Each of them impedes a company's ability to move quickly and adapt for the future. And as such, for many IBM clients, solving these automation challenges have become an indispensable element in their strategy to modernize IT. UNPRECEDENTED RATE OF GENERATIVE AI ADOPTION Even though generative AI is relatively new, the widespread popularity of ChatGPT has created significant interest in the notion of large language models (LLMs) and foundation models (FMs) \u2014 and what they can do for business. It took quite some time for enterprises to start moving toward traditional AI. In contrast, generative AI has experienced massive early adoption: 80% of enterprises are already working with, or planning to leverage FMs, and plan to adopt generative AI in their use cases and workflow. Moreover, the following data points to an ever-growing adoption trend for generative AI: Scale Zeitgeist 2023 AI Readiness Report notes that with the companies they reviewed, 21% have generative AI models in production; 29% are experimenting with generative AI and another 31% are planning to work with generative AI models; a total of 81% are either working with or planning to work with generative AI models Goldman Sachs has estimated that generative AI will have a very deep economic impact \u2013 raising global Gross Domestic Product (GDP) by 7% within 10 years, reflecting the technology\u2019s huge potential. Boston Consulting Group (BCG) noted that generative AI is expected to represent 30% of the overall market by 2025","title":"i. Automation is indispensable to modern IT strategy"},{"location":"#ii-generative-ai-assisted-code-lifecycle-management-achieves-what-llms-alone-cannot","text":"Following the debut of OpenAI's ChatGPT, the marketplace has been awash with competing large language model (LLM) and generative AI-based assistants. It's one thing to train and deploy an LLM; it's another thing entirely to make it applicable and tangibly beneficial for business. What separates IBM watsonx Code Assistant (WCA) offerings from competing vendors in the marketplace? The design and implementation of WCA is purposely built to assist, using generative AI, software and code lifecycle management. In short, generative AI-assisted code lifecycle management helps to achieve what large language models cannot achieve on their own. It is what distinguished WCA from other code assistants in the marketplace today. Code lifecycle management begins with understanding client code, through training across a myriad of programming languages and specializations in paradigms such as Ansible Automation Platform, and applies that understanding across a client\u2019s application and runtime environments. Users are able to plan next steps based on generative AI analysis of their existing application code. Operations teams can rapidly transform their codebases with optimized design and architecture that is recommended according to IBM Granite's best-practice models. Administrators can validate the outcomes with automatically generated unit tests. Afterwards, they can deploy those services and applications using automated processes like Ansible's automation engine. Over the course of that application or code's lifecycle, generative AI can maintain healthy operations with runtime insights.","title":"ii. Generative AI-assisted code lifecycle management achieves what LLMs alone cannot"},{"location":"#iii-introducing-the-ibm-watsonx-code-assistant-product-family","text":"IBM watsonx Code Assistant is the flagship offering in a suite of generative AI code assistant products, which also include offerings for Ansible Automation Platform (IBM watsonx Code Assistant for Red Hat Ansible Lightspeed) and IBM Systems modernization (IBM watsonx Code Assistant for Z). These solutions accelerate software development tasks with AI-powered capabilities including context-aware code generation, explanation, documentation, translation, and unit test generation. It does so while maintaining the principles of trust, security, and compliance with regards to IBM client's data and intellectual property. Developers and IT Operators can utilize WCA to speed up application modernization efforts and generate Ansible-based automation jobs to rapidly scale out (or scale up) IT environments. IBM watsonx Code Assistant products are powered by IBM Granite foundation models that include state-of-the-art large language models designed for code. For offerings such as WCA for Ansible Lightspeed and WCA for Z, bespoke code models\u2014 tailored to working with Ansible Automation Platform and COBOL-to-Z use cases, respectively \u2014are invoked. Universally true for all of the watsonx Code Assistant offerings is that they are geared towards helping IT teams create high-quality code using AI-generated recommendations, based on natural language requests or existing source code. These AI models, and the recommendations they generate, are seamlessly integrated via extensions with the world's most popular developer integrated development environments (IDE), including Visual Studio Code and Eclipse. Granite is IBM\u2019s flagship brand of open and proprietary LLMs, spanning multiple modalities. Granite models exist for code, languages, time series, and GeoSpatial \u2014 with additional modalities expected in future. IBM Granite code models are a series of decoder-only models for code generative tasks, trained with code written in 116 different programming languages. The Granite code models family consists of models ranging in size from 3 to 34 billion parameters, in both a base model and instruction-following model variants. These models have a range of uses, from complex application modernization tasks to on-device memory-constrained use cases. The larger the block size for a particular language on this chart, the larger percentage of training corpus data of that language was used to train the Granite code model. Languages and formats such as Java, C, JSON, JavaScript, HTML, and PHP are subjects in which the model \u201cMajors\u201d and excels. Other languages such as Ruby, SQL, and Swift could be considered \u201cMinors\u201d where the generalized code model can work with the language, but has less training data to base those recommendations on. These percentages and training data volumes will continue to evolve as the Granite code models mature. WATSONX CODE ASSISTANT vs. WCA FOR ANSIBLE LIGHTSPEED? For those familiar with other IBM watsonx Code Assistant offerings\u2014 such as WCA for Red Hat Ansible Lightspeed and WCA for Z \u2014the generalized code model approach, as seen here, differs from the specialized code model approach of those two aforementioned offerings. The WCA for Ansible Lightspeed flavor of IBM Granite code models specializes (Majors) only in Ansible Playbooks and YAML Similarly, the IBM Granite code model used by WCA for Z specializes in transforming COBOL mainframe code into modernized Java code Ansible Playbooks (YAML) and mainframe (COBOL) code are both supported (Minor) languages for the generalized IBM Granite code models\u2014 and therefore are supported by IBM watsonx Code Assistant \u2014but if a client wishes to specialize in those particular languages and frameworks, they would be well advised to utilize the bespoke WCA for Ansible Lightspeed and WCA for Z offerings, respectively, to do so.","title":"iii. Introducing the IBM watsonx Code Assistant product family"},{"location":"#iv-solution-architecture-of-ibm-watsonx-code-assistant-for-red-hat-ansible-lightspeed","text":"IBM watsonx Code Assistant for Red Hat Ansible Lightspeed meets developers where they are: with a rich plugin via VS Code extensions, where developers input their prompts directly in the code editor. Prompts are sent to the Ansible Lightspeed service, and the service sends a suggestion back (a completion ) that\u2019s powered by IBM Granite LLMs for code. It is important to note that all data in transit is encrypted and ephemeral so users can be confident and have trust in the security of the service during this exchange. In terms of data security, client Ansible playbooks and customized models that they may potentially have are stored in client-owned Cloud Object Storage and are not shared with IBM, Red Hat, or any other clients. In order to utilize IBM watsonx Code Assistant for Red Hat Ansible Lightspeed, a client must have an existing license for Red Hat Ansible Automation Platform (the \u201dred tile\u201d component in the center of the diagram), as well as a license for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed (the \u201cblue tile\u201d on the right of the diagram). Generative AI has recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. For example, if trained on a large dataset of Ansible Playbooks, generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style or standards of that particular company. WHAT ARE PLAYBOOKS? Ansible Playbooks instruct Ansible\u2019s automation engine on how to execute tasks in a step-by-step manner. Playbooks defines roles, tasks, handlers, and other configurations; in turn, these attributes allow developers and users to codify complex orchestration scenarios. Conceptually, think of a Playbook as a recipe book for system administration: each recipe (or Playbook) spells out the steps required to achieve a particular system state or to complete a given operation. One of the standout features of Ansible Playbooks is that they are idempotent : executing Playbooks multiple times on the same system won't create additional \"side effects\" (unintended operations or creation of unwanted artifacts) after the first successful run. This ensures consistency and reliability across deployments of the Red Hat Ansible Automation Platform ( AAP ). As you will see throughout the hands-on training material, generative AI models provide a natural language prompt to users which in turn is understood and translated by the AI models into the necessary Ansible Task code. For example, a user might describe a desired system state in plain language ( \"I want a Playbook to install and start an Apache web server\" ) and the model will generate the appropriate Ansible Tasks for a Playbook. All of this is achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within a company with limited Ansible or programming expertise will be able to produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing of AI-generated code will be needed before being put into production. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality.","title":"iv. Solution architecture of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed"},{"location":"#v-lab-objectives","text":"The material covered for this hands-on training is intended to prepare IBM sellers and business partners with the skills necessary to create Ansible automation tasks using the generative AI capabilities of WCA. The curriculum will leverage WCA's generative AI code recommendations for automating cloud-based and infrastructure-based automation tasks. In-depth explanations accompanying Ansible Playbook templates will also explain: How WCA uses natural language prompts , as well as Ansible Playbook contents, to generate contextually-aware Task code recommendations Post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How WCA provides content source matching attribution and \"explainability\" for all AI-generated content Leveraging WCA's model tuning capabilities to tailor content and code recommendations to an organization's standards, best practices, and programming styles","title":"v. Lab objectives"},{"location":"#vi-next-steps","text":"The module ahead will outline the evaluation criteria for IBM sellers and business partners. Afterwards, you will setup your local environment with the necessary pre-requisites for getting started with the hands-on material.","title":"vi. Next steps"},{"location":"appmod1/1/","text":"Objectives and Requirements Application Modernization - WebSphere to Liberty i. Section 1","title":"1. Objectives and requirements"},{"location":"appmod1/1/#objectives-and-requirementsapplication-modernization-websphere-to-liberty","text":"","title":"Objectives and RequirementsApplication Modernization - WebSphere to Liberty"},{"location":"appmod1/1/#i-section-1","text":"","title":"i. Section 1"},{"location":"appmod2/1/","text":"Objectives and Requirements Application Modernization - Upgrading Java i. Section 1","title":"1. Objectives and requirements"},{"location":"appmod2/1/#objectives-and-requirementsapplication-modernization-upgrading-java","text":"","title":"Objectives and RequirementsApplication Modernization - Upgrading Java"},{"location":"appmod2/1/#i-section-1","text":"","title":"i. Section 1"},{"location":"on-premises/1/","text":"Objectives and Requirements On-Premises Installation and Deployment i. Introduction and hands-on objectives The On-Premises Installation and Deployment module provides comprehensive instructions for how to prepare, configure, and deploy a simulated on-premises cluster for IBM watsonx Code Assistant (WCA) on environments hosted by IBM Technology Zone (ITZ). By completing this module, participants will have learned and applied the skills necessary for deploying WCA on a client's on-premises infrastructure. The complete stack of technologies and services that you will deploy include: Red Hat OpenShift Container Platform v4.18 : a unified application development platform that lets clients build, modernize, and deploy applications at scale on their choice of hybrid cloud infrastructure. IBM Cloud Pak for Data v5.1.x : a set of services comprising a data fabric solution for data governance, data engineering, data analysis, and AI lifecycle tasks. IBM Software Hub v5.1 : a cloud-native solution that clients use to install, manage, and monitor IBM solutions on Red Hat OpenShift Container Platform. IBM watsonx Code Assistant v5.1 : a generative AI coding companion that provides contextually aware assistance for programming languages. Special acknowledgement and thanks to IBM colleagues Coralie Jonvel, Nelson Nunes, and Noe Samaille for adaptation of their deployment instructions for watsonx.ai on Red Hat OpenShift. INSERT ARCHITECTURE GRAPHIC AND DESCRIPTION HERE GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. ii. Infrastructure and resource requirements Requirements specific to the hands-on environment are outlined in the section below. Comprehensive details about the hardware requirements for x86_64 cluster services are available from IBM Software Hub documentation. Although the hands-on environment that will be provisioned in the next module utilizes a templated, pre-defined ITZ infrastructure configuration, it will be useful for those enrolled to understand the resources required to reproduce a similar cluster in real-world client scenarios. This includes details about the CPU, memory, GPU, and other hardware components required to support the necessary cluster services. IBM Software Hub platform Additional details available from IBM Documentation Node Role Number of Services Minimum Available vCPU Minimum Memory Minimum Storage Control plane 3 (for high availability) 4 vCPU per node (This configuration supports up to 24 worker nodes.) 16 GB RAM per node. This configuration supports up to 24 worker nodes. No additional storage is needed for IBM Software Hub. Infra 3 (recommended) 4 vCPU per node. This configuration supports up to 27 worker nodes. 24 GB RAM per node (This configuration supports up to 27 worker nodes.) See the Red Hat OpenShift Container Platform documentation for sizing guidance. Worker (compute) 3 or more worker (compute) nodes 16 vCPU per node Minimum : 64 GB RAM per node Recommended : 128 GB RAM per node 300 GB of storage space per node for storing container images locally. If you plan to install watsonx.ai, increase the storage to 500 GB per node. Load balancer 2 load balancer nodes 2 vCPU per node 4 GB RAM per node. Add another 4 GB of RAM for access restrictions and security control. Add 100 GB of root storage for access restrictions and security control. IBM Cloud Pak Foundational Services Additional details available from IBM Documentation vCPU Memory Storage Notes 4 vCPU 5 GB RAM Reference the v4.10 hardware requirements and recommendations . Required. IBM Cloud Pak Foundational Services are installed once for each instance of IBM Software Hub on the cluster. Red Hat OpenShift Container Platform (single node) Additional details available from IBM Documentation VM Role Minimum Available vCPU Minimum Memory Minimum Storage Bastion node 4 vCPU 8 GB RAM Allocate a minimum of 500 GB of disk space. The disk can be: in the same disk as the general bastion node storage; in a separate disk on the bastion node; or on external storage. Worker (compute) 16 vCPU 64 GB RAM Allocate a minimum of 300 GB of disk space on the node for image storage. IBM watsonx Code Assistant Additional details available from IBM Documentation vCPU Memory Storage Notes Operator pods: 0.1 vCPU Operator pods: 0.256 GB RAM Persistent storage: 120 GB Minimum resources for an installation with a single replica per service Catalog pods: 0.01 vCPU Catalog pods: 0.05 GB RAM Ephemeral storage: 0.4 GB The service requires at least two GPUs Operand: 7 vCPU Operand: 25 GB RAM Image storage: Up to 107 GB with all models GPU support is limited to: NVIDIA H100 GPUs with 80 GB RAM iii. Prerequisites checklist Register for an IBM Technology Zone account Participants require access to ITZ in order to reserve an environment and complete the hands-on work. If you do not yet have an account with the ITZ, you will need to register for one . Obtain an IBM Entitlement API key Participants require an entitlement API key to proceed with the on-premises installation. In order to retrieve the key: Use your IBMid and password to log in to the Container Software Library . Click the Entitlement keys [A] tab from the navigation menu. Click Add new key [B] to generate a new entitlement key. Select Copy [C] to capture the entitlement key to the clipboard. Paste and save the entitlement key to a text file on your local machine. iv. Next steps In the following module, you will provision an OpenShift Container Platform cluster via IBM Technology Zone, which will serve as the basis for the on-premises environment.","title":"1. Objectives and requirements"},{"location":"on-premises/1/#objectives-and-requirementson-premises-installation-and-deployment","text":"","title":"Objectives and RequirementsOn-Premises Installation and Deployment"},{"location":"on-premises/1/#i-introduction-and-hands-on-objectives","text":"The On-Premises Installation and Deployment module provides comprehensive instructions for how to prepare, configure, and deploy a simulated on-premises cluster for IBM watsonx Code Assistant (WCA) on environments hosted by IBM Technology Zone (ITZ). By completing this module, participants will have learned and applied the skills necessary for deploying WCA on a client's on-premises infrastructure. The complete stack of technologies and services that you will deploy include: Red Hat OpenShift Container Platform v4.18 : a unified application development platform that lets clients build, modernize, and deploy applications at scale on their choice of hybrid cloud infrastructure. IBM Cloud Pak for Data v5.1.x : a set of services comprising a data fabric solution for data governance, data engineering, data analysis, and AI lifecycle tasks. IBM Software Hub v5.1 : a cloud-native solution that clients use to install, manage, and monitor IBM solutions on Red Hat OpenShift Container Platform. IBM watsonx Code Assistant v5.1 : a generative AI coding companion that provides contextually aware assistance for programming languages. Special acknowledgement and thanks to IBM colleagues Coralie Jonvel, Nelson Nunes, and Noe Samaille for adaptation of their deployment instructions for watsonx.ai on Red Hat OpenShift. INSERT ARCHITECTURE GRAPHIC AND DESCRIPTION HERE GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum.","title":"i. Introduction and hands-on objectives"},{"location":"on-premises/1/#ii-infrastructure-and-resource-requirements","text":"Requirements specific to the hands-on environment are outlined in the section below. Comprehensive details about the hardware requirements for x86_64 cluster services are available from IBM Software Hub documentation. Although the hands-on environment that will be provisioned in the next module utilizes a templated, pre-defined ITZ infrastructure configuration, it will be useful for those enrolled to understand the resources required to reproduce a similar cluster in real-world client scenarios. This includes details about the CPU, memory, GPU, and other hardware components required to support the necessary cluster services. IBM Software Hub platform Additional details available from IBM Documentation Node Role Number of Services Minimum Available vCPU Minimum Memory Minimum Storage Control plane 3 (for high availability) 4 vCPU per node (This configuration supports up to 24 worker nodes.) 16 GB RAM per node. This configuration supports up to 24 worker nodes. No additional storage is needed for IBM Software Hub. Infra 3 (recommended) 4 vCPU per node. This configuration supports up to 27 worker nodes. 24 GB RAM per node (This configuration supports up to 27 worker nodes.) See the Red Hat OpenShift Container Platform documentation for sizing guidance. Worker (compute) 3 or more worker (compute) nodes 16 vCPU per node Minimum : 64 GB RAM per node Recommended : 128 GB RAM per node 300 GB of storage space per node for storing container images locally. If you plan to install watsonx.ai, increase the storage to 500 GB per node. Load balancer 2 load balancer nodes 2 vCPU per node 4 GB RAM per node. Add another 4 GB of RAM for access restrictions and security control. Add 100 GB of root storage for access restrictions and security control. IBM Cloud Pak Foundational Services Additional details available from IBM Documentation vCPU Memory Storage Notes 4 vCPU 5 GB RAM Reference the v4.10 hardware requirements and recommendations . Required. IBM Cloud Pak Foundational Services are installed once for each instance of IBM Software Hub on the cluster. Red Hat OpenShift Container Platform (single node) Additional details available from IBM Documentation VM Role Minimum Available vCPU Minimum Memory Minimum Storage Bastion node 4 vCPU 8 GB RAM Allocate a minimum of 500 GB of disk space. The disk can be: in the same disk as the general bastion node storage; in a separate disk on the bastion node; or on external storage. Worker (compute) 16 vCPU 64 GB RAM Allocate a minimum of 300 GB of disk space on the node for image storage. IBM watsonx Code Assistant Additional details available from IBM Documentation vCPU Memory Storage Notes Operator pods: 0.1 vCPU Operator pods: 0.256 GB RAM Persistent storage: 120 GB Minimum resources for an installation with a single replica per service Catalog pods: 0.01 vCPU Catalog pods: 0.05 GB RAM Ephemeral storage: 0.4 GB The service requires at least two GPUs Operand: 7 vCPU Operand: 25 GB RAM Image storage: Up to 107 GB with all models GPU support is limited to: NVIDIA H100 GPUs with 80 GB RAM","title":"ii. Infrastructure and resource requirements"},{"location":"on-premises/1/#iii-prerequisites-checklist","text":"Register for an IBM Technology Zone account Participants require access to ITZ in order to reserve an environment and complete the hands-on work. If you do not yet have an account with the ITZ, you will need to register for one . Obtain an IBM Entitlement API key Participants require an entitlement API key to proceed with the on-premises installation. In order to retrieve the key: Use your IBMid and password to log in to the Container Software Library . Click the Entitlement keys [A] tab from the navigation menu. Click Add new key [B] to generate a new entitlement key. Select Copy [C] to capture the entitlement key to the clipboard. Paste and save the entitlement key to a text file on your local machine.","title":"iii. Prerequisites checklist"},{"location":"on-premises/1/#iv-next-steps","text":"In the following module, you will provision an OpenShift Container Platform cluster via IBM Technology Zone, which will serve as the basis for the on-premises environment.","title":"iv. Next steps"},{"location":"on-premises/2/","text":"Reserve an Environment On-Premises Installation and Deployment If you require assistance or run into issues with the hands-on lab, help is available. Environment issues: The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues: If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #wca-ansible-techzone-support Slack channel. i. Configuring the IBM Technology Zone reservation The foundation for the on-premises environment utilizes the OpenShift Cluster (VMware on IBM Cloud) - UPI - Public template from the collection of IBM Technolgy Zone (ITZ) Certified Base Images . Click the link below to request a reservation directly from ITZ: URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options , select Reserve now [A] . Supply additional details about the ITZ reservation request: RESERVATON POLICY NOTICE After selecting Education for the Purpose field, you may receive a pop-up notification stating that this environment is now being redirected to the OCP base image hosted On-Prem for Education and Test . You can safely ignore this notice and close it by clicking the X in the top-right corner. Do not configure using the Poughkeepsie-based resource that the notice attempts to redirect you to \u2014 it will not allow you to configure the necessary hardware specifications. Continue with the ITZ reservation request form as detailed below. If the pop-up appears again later in the configuration steps, continue to disregard the notice. Field Value Name Give your reservation a unique name. Purpose Education Purpose Description Give your reservation a unique description. Preferred Geography Select the region and data center geographically closest to your location. End Date and Time Select a time and date for when the reservation will expire. OpenShift Version 4.16 Worker Node Count 3 Worker Node Flavor 32 vCPU x 128 GB - 300 GB ephemeral storage Storage ODF - 2 TB OCP/Kubernetes Cluster Network 10.128.0.0/14 OCP/Kubernetes Service Network 172.30.0.0/16 Enable nested hardware virtualization on workers No When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . Navigate to the My Reservations tab of the ITZ to monitor the progress of your reservation. While \"Provisioning\" [A] , the reservation will be marked with a yellow tile. Wait for the ITZ reservation to be marked as \"Ready\" [B] before continuing. PROVISIONING TIMES This operation will take approximately 90 - 120 minutes to complete. ii. Accessing the cluster Once the cluster has been successfully deployed, you will receive an email with the header: Reservation Ready on IBM Technology Zone . Confirm that the ITZ email states that Status Update: Ready [A] . Follow the link provided in the email, or access the My Reservations tab on ITZ to access your reservation. Scroll down the page until you reach the Reservation Details section. Record the following connection details for the OpenShift Container Platform (OCP) cluster to a notepad: Desktop URL (interchangeable with OCP Dashboard URL ) [A] Cluster Admin Username [B] Cluster Admin Password [C] API URL [D] Bastion Username [E] Bastion Password [F] Bastion SSH Connection [G] Click the blue Open your IBM Cloud environment button at the top of the page to launch a new browser window for accessing the OCP cluster. Choose the kube:admin log in option and then provide the following credentials: Username: kubeadmin Password: Cluster Admin Password recorded in Step 6 At this stage, you should have successfully logged in to the OCP Dashboard. iii. Next steps In the following module, you will access and configure the cluster's bastion node.","title":"2. Reserve an environment"},{"location":"on-premises/2/#reserve-an-environmenton-premises-installation-and-deployment","text":"If you require assistance or run into issues with the hands-on lab, help is available. Environment issues: The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues: If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #wca-ansible-techzone-support Slack channel.","title":"Reserve an EnvironmentOn-Premises Installation and Deployment"},{"location":"on-premises/2/#i-configuring-the-ibm-technology-zone-reservation","text":"The foundation for the on-premises environment utilizes the OpenShift Cluster (VMware on IBM Cloud) - UPI - Public template from the collection of IBM Technolgy Zone (ITZ) Certified Base Images . Click the link below to request a reservation directly from ITZ: URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options , select Reserve now [A] . Supply additional details about the ITZ reservation request: RESERVATON POLICY NOTICE After selecting Education for the Purpose field, you may receive a pop-up notification stating that this environment is now being redirected to the OCP base image hosted On-Prem for Education and Test . You can safely ignore this notice and close it by clicking the X in the top-right corner. Do not configure using the Poughkeepsie-based resource that the notice attempts to redirect you to \u2014 it will not allow you to configure the necessary hardware specifications. Continue with the ITZ reservation request form as detailed below. If the pop-up appears again later in the configuration steps, continue to disregard the notice. Field Value Name Give your reservation a unique name. Purpose Education Purpose Description Give your reservation a unique description. Preferred Geography Select the region and data center geographically closest to your location. End Date and Time Select a time and date for when the reservation will expire. OpenShift Version 4.16 Worker Node Count 3 Worker Node Flavor 32 vCPU x 128 GB - 300 GB ephemeral storage Storage ODF - 2 TB OCP/Kubernetes Cluster Network 10.128.0.0/14 OCP/Kubernetes Service Network 172.30.0.0/16 Enable nested hardware virtualization on workers No When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . Navigate to the My Reservations tab of the ITZ to monitor the progress of your reservation. While \"Provisioning\" [A] , the reservation will be marked with a yellow tile. Wait for the ITZ reservation to be marked as \"Ready\" [B] before continuing. PROVISIONING TIMES This operation will take approximately 90 - 120 minutes to complete.","title":"i. Configuring the IBM Technology Zone reservation"},{"location":"on-premises/2/#ii-accessing-the-cluster","text":"Once the cluster has been successfully deployed, you will receive an email with the header: Reservation Ready on IBM Technology Zone . Confirm that the ITZ email states that Status Update: Ready [A] . Follow the link provided in the email, or access the My Reservations tab on ITZ to access your reservation. Scroll down the page until you reach the Reservation Details section. Record the following connection details for the OpenShift Container Platform (OCP) cluster to a notepad: Desktop URL (interchangeable with OCP Dashboard URL ) [A] Cluster Admin Username [B] Cluster Admin Password [C] API URL [D] Bastion Username [E] Bastion Password [F] Bastion SSH Connection [G] Click the blue Open your IBM Cloud environment button at the top of the page to launch a new browser window for accessing the OCP cluster. Choose the kube:admin log in option and then provide the following credentials: Username: kubeadmin Password: Cluster Admin Password recorded in Step 6 At this stage, you should have successfully logged in to the OCP Dashboard.","title":"ii. Accessing the cluster"},{"location":"on-premises/2/#_1","text":"","title":""},{"location":"on-premises/2/#iii-next-steps","text":"In the following module, you will access and configure the cluster's bastion node.","title":"iii. Next steps"},{"location":"on-premises/3/","text":"Bastion Host Setup On-Premises Installation and Deployment The following section is based off of IBM Documentation write-ups that detail how to install IBM Software Hub on a Red Hat OpenShift Container Platform cluster. Reference the instructions in full at the following resource: Installing the IBM Software Hub command-line interface . i. Connect to the bastion host To access and configure the bastion host node, open a Terminal (Windows Terminal or the Terminal built into VS Code are good alternatives if you're on a PC). Copy the Bastion SSH Connection recorded in Step 6 of the previous module and paste it into the terminal console. Hit Enter to create an SSH connection to the bastion host. When prompted Are you sure you want to continue connecting (yes/no/fingerprint)? , enter yes and hit Enter to proceed. The console will return a Welcome to IBM Technology Zone once connected to the bastion host, at which point you must authenticate. Authenticate when prompted to do so by providing the Bastion Password . If the console now reads [itzuser@localhost ~]$ then you have successfully accessed the bastion host. ii. OpenShift command line interface (oc) Next, install the OpenShift Command Line Interface (CLI), designated oc , to programmatically perform work with the bastion node. ELEVATED PERMISSIONS Execute the following command in the Terminal console to ensure that subsequent actions taken via the Terminal console are done with elevated permissions. This will save you needing to re-authenticate again for future commands. sudo bash Retrieve the OCP Dashboard URL (recorded in Step 6 of the previous module). Obtain the OpenShift Base Domain by extracting the portion of the URL that matches the position highlighted in the sample URL below. Extract the characters following .apps. up to and including .com . Do not include the /dashboards addendum. https://console-openshift-console.apps. 678a250b79141644e78804e0.ocp.techzone.ibm.com In this example, the value of the OpenShift base domain is 678a250b79141644e78804e0.ocp.techzone.ibm.com Record your OCP cluster's value to a notepad for future reference. The following instruction set, when executed within a Terminal window, will install the OpenShift CLI on the bastion host node. However, the instructions require some modification before they will successfully execute. Install OpenShift CLI export OPENSHIFT_BASE_DOMAIN = <CHANGE_ME> wget --no-check-certificate https://downloads-openshift-console.apps. ${ OPENSHIFT_BASE_DOMAIN } /amd64/linux/oc.tar tar -xvf oc.tar chmod +x oc sudo mv oc /usr/local/bin/oc Copy the instructions above and paste into a notepad. Replace the highlighted <CHANGE_ME> text with the OpenShift base domain value recorded in Step 2. Copy the modified notepad instructions to your clipboard and paste into your Terminal console. Press Enter to execute the instructions. The setup should only take a moment to complete. Once finished, try typing oc into the console window and hit Enter . The console output should verify that oc (the OpenShift CLI) has been successfully installed on the bastion host. iii. Podman install IBM Cloud Pak for Data (CP4D)'s installer requires containers, for which you will need to install Podman on the cluster nodes via the bastion host. Using the connected Terminal console, execute the following instruction to install Podman: sudo yum install -y podman The operation will take approximately 2 minutes to complete. After a successful operation, the console will return the message Complete! alongside a summary of the installed components. iv. Environment variables Set the environment variables needed for installation of CP4D on the cluster. The list is quite extensive and long, so rather than set these one at a time it's recommended that you first compile them into a single file on the bastion host. Afterwards, you can set all the variables automatically using the single file. Below is a code block containing all of the necessary CP4D environment variables. Copy the contents of the entire block to your clipboard and paste into a notepad. CP4D Environment Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #=============================================================================== # Cloud Pak for Data installation variables #=============================================================================== #------------------------------------------------------------------------------ # Client workstation #------------------------------------------------------------------------------ # Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI. # # To export these variables, you must uncomment each command in this sec-tion. #export CPD_CLI_MANAGE_WORKSPACE=<enter a fully qualified directory> # following lines could be used for environment with self-signed certificates and hostnames not resolved by DNS server #export OLM_UTILS_LAUNCH_ARGS=\"-v ./api-wxai.pem:/etc/k8scert --env K8S_AUTH_SSL_CA_CERT=/etc/k8scert --add-host oauth-openshift.apps.ocpinstall.gym.lan:192.168.252.4 --add-host api.ocpinstall.gym.lan:192.168.252.3\" export PATH = \"/home/itzuser/cpd-cli-linux-EE-14.1.0-1189\" : $PATH #----------------------------------------------------------------------------- # Cluster #------------------------------------------------------------------------------ export OCP_URL = api.<REPLACE THIS VALUE>:6443 #export OPENSHIFT_TYPE=<enter your deployment type> #export IMAGE_ARCH=amd64 export OCP_USERNAME = kubeadmin export OCP_PASSWORD = <REPLACE THIS VALUE> # export OCP_TOKEN=<enter your token> export SERVER_ARGUMENTS = \"--server= ${ OCP_URL } \" export LOGIN_ARGUMENTS = \"--username= ${ OCP_USERNAME } --password= ${ OCP_PASSWORD } \" # export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\" export CPDM_OC_LOGIN = \"cpd-cli manage login-to-ocp ${ SERVER_ARGUMENTS } ${ LOGIN_ARGUMENTS } \" export OC_LOGIN = \"oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } \" #------------------------------------------------------------------------------ # Projects #------------------------------------------------------------------------------ export PROJECT_LICENSE_SERVICE = cpd-license export PROJECT_SCHEDULING_SERVICE = cpd-scheduling export PROJECT_CPD_INST_OPERATORS = cpd-operators export PROJECT_CPD_INST_OPERANDS = cpd-watsonx # export PROJECT_CPD_INSTANCE_TETHERED=<enter your tethered project> # export PROJECT_CPD_INSTANCE_TETHERED_LIST=<a comma-separated list of teth-ered projects> #------------------------------------------------------------------------------ # Storage #------------------------------------------------------------------------------ export STG_CLASS_BLOCK = ocs-storagecluster-ceph-rbd export STG_CLASS_FILE = ocs-storagecluster-cephfs #------------------------------------------------------------------------------ # IBM Entitled Registry #------------------------------------------------------------------------------ export IBM_ENTITLEMENT_KEY = <REPLACE THIS VALUE> #------------------------------------------------------------------------------ # Cloud Pak for Data version #------------------------------------------------------------------------------ export VERSION = 5 .1.0 #------------------------------------------------------------------------------ # Components #------------------------------------------------------------------------------ #export COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_plat-form # export COMPONENTS_TO_SKIP=<component-ID-1>,<component-ID-2> #export COMPONENTS=ibm-cert-manager,ibm-licensing,cpfs,scheduler,cpd_plat-form,wml,ws,watsonx_ai export COMPONENTS = cpd_platform,wca You must make modifications to Line 19 , Line 23 , and Line 49 of the CP4D Environment Variables to tailor the variables to your specific cluster. Line 19: substitute the <REPLACE THIS VALUE> placeholder for export OCP_URL= with to the value of OpenShift Base Domain that was recorded in Step 2. Do not replace the .api. or :6443: components , as these are required. Your modified Line 19 should resemble the following: export OCP_URL = api.678a250b79141644e78804e0.ocp.techzone.ibm.com:6443 Line 23: set the value of export OCP_PASSWORD= equal to the value of Cluster Admin Password recorded in Step 6 of the previous module. For example: export OCP_PASSWORD = password1234 Line 49: set the value of export IBM_ENTITLEMENT_KEY= equal to the value of the key specific to your IBM account. Reference the IBM Entitlement API Key that was generated in the iii. Prerequisites checklist section of Module 1 (Objectives and Requirements) . Instructions for how to generate an IBM Entitlement API Key are provided in that section. For example: export IBM_ENTITLEMENT_KEY = verylongAPIkey1234 With your Terminal console, execute the following instruction to open the vi editor and create a shell script named cpd_vars.sh on the bastion host: vi cpd_vars.sh Copy the modified CP4D Environment Variables contents from the notepad (Step 7) to your machine's clipboard. Switch back to the Terminal where the VI editor is now open and press the I key to enable inserting text. Press Cmd + V (or Ctrl + V ) to paste the contents from your clipboard. To save, press Esc and then type :wq followed by Enter to write the file and exit the editor. v. Cloud Pak for Data command line interface (cpd-cli) Now that the environment variables have been set, the next step towards installing CP4D is preparing the command line interface ( cpd-cli ). Copy the following code block and execute it within the console to install cpd-cli : wget https://github.com/IBM/cpd-cli/releases/download/v14.1.0/cpd-cli-linux-EE-14.1.0.tgz tar -xzf cpd-cli-linux-EE-14.1.0.tgz export PATH = \" $( pwd ) /cpd-cli-linux-EE-14.1.0-1189\" : $PATH The operation will take approximately 1 minute to complete. Verify that the CLI has been successfully integrated with the following command: cpd-cli The output from the console should resemble the screenshot below. Verify the status of the restarted container by typing podman ps and Enter , which should return the result of a single container running on the bastion host. echo $PATH cpd-cli manage restart-container The operation will take approximately 1 minute to complete. After a successful operation, the console will return a pair of messages that resemble the following: [ SUCCESS ] 2025 -03-19T17:51:35.317974Z Successfully started the container olm-utils-play-v3 [ SUCCESS ] 2025 -03-19T17:51:35.318017Z Container olm-utils-play-v3 has been re-created Source the newly-configured environment variables with the following command: source cpd_vars.sh Test that the login for oc command line is now functioning properly: oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } The command line should return back with a prompt describing You have access to ... projects which indicates that oc and the environment variables have been configured. vi. Next steps At this stage the bastion host node has been fully configured ahead of installing the necessary software, which will be covered in the subsequent modules. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"3. Bastion host setup"},{"location":"on-premises/3/#bastion-host-setupon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation write-ups that detail how to install IBM Software Hub on a Red Hat OpenShift Container Platform cluster. Reference the instructions in full at the following resource: Installing the IBM Software Hub command-line interface .","title":"Bastion Host SetupOn-Premises Installation and Deployment"},{"location":"on-premises/3/#i-connect-to-the-bastion-host","text":"To access and configure the bastion host node, open a Terminal (Windows Terminal or the Terminal built into VS Code are good alternatives if you're on a PC). Copy the Bastion SSH Connection recorded in Step 6 of the previous module and paste it into the terminal console. Hit Enter to create an SSH connection to the bastion host. When prompted Are you sure you want to continue connecting (yes/no/fingerprint)? , enter yes and hit Enter to proceed. The console will return a Welcome to IBM Technology Zone once connected to the bastion host, at which point you must authenticate. Authenticate when prompted to do so by providing the Bastion Password . If the console now reads [itzuser@localhost ~]$ then you have successfully accessed the bastion host.","title":"i. Connect to the bastion host"},{"location":"on-premises/3/#ii-openshift-command-line-interface-oc","text":"Next, install the OpenShift Command Line Interface (CLI), designated oc , to programmatically perform work with the bastion node. ELEVATED PERMISSIONS Execute the following command in the Terminal console to ensure that subsequent actions taken via the Terminal console are done with elevated permissions. This will save you needing to re-authenticate again for future commands. sudo bash Retrieve the OCP Dashboard URL (recorded in Step 6 of the previous module). Obtain the OpenShift Base Domain by extracting the portion of the URL that matches the position highlighted in the sample URL below. Extract the characters following .apps. up to and including .com . Do not include the /dashboards addendum. https://console-openshift-console.apps. 678a250b79141644e78804e0.ocp.techzone.ibm.com In this example, the value of the OpenShift base domain is 678a250b79141644e78804e0.ocp.techzone.ibm.com Record your OCP cluster's value to a notepad for future reference. The following instruction set, when executed within a Terminal window, will install the OpenShift CLI on the bastion host node. However, the instructions require some modification before they will successfully execute. Install OpenShift CLI export OPENSHIFT_BASE_DOMAIN = <CHANGE_ME> wget --no-check-certificate https://downloads-openshift-console.apps. ${ OPENSHIFT_BASE_DOMAIN } /amd64/linux/oc.tar tar -xvf oc.tar chmod +x oc sudo mv oc /usr/local/bin/oc Copy the instructions above and paste into a notepad. Replace the highlighted <CHANGE_ME> text with the OpenShift base domain value recorded in Step 2. Copy the modified notepad instructions to your clipboard and paste into your Terminal console. Press Enter to execute the instructions. The setup should only take a moment to complete. Once finished, try typing oc into the console window and hit Enter . The console output should verify that oc (the OpenShift CLI) has been successfully installed on the bastion host.","title":"ii. OpenShift command line interface (oc)"},{"location":"on-premises/3/#iii-podman-install","text":"IBM Cloud Pak for Data (CP4D)'s installer requires containers, for which you will need to install Podman on the cluster nodes via the bastion host. Using the connected Terminal console, execute the following instruction to install Podman: sudo yum install -y podman The operation will take approximately 2 minutes to complete. After a successful operation, the console will return the message Complete! alongside a summary of the installed components.","title":"iii. Podman install"},{"location":"on-premises/3/#iv-environment-variables","text":"Set the environment variables needed for installation of CP4D on the cluster. The list is quite extensive and long, so rather than set these one at a time it's recommended that you first compile them into a single file on the bastion host. Afterwards, you can set all the variables automatically using the single file. Below is a code block containing all of the necessary CP4D environment variables. Copy the contents of the entire block to your clipboard and paste into a notepad. CP4D Environment Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #=============================================================================== # Cloud Pak for Data installation variables #=============================================================================== #------------------------------------------------------------------------------ # Client workstation #------------------------------------------------------------------------------ # Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI. # # To export these variables, you must uncomment each command in this sec-tion. #export CPD_CLI_MANAGE_WORKSPACE=<enter a fully qualified directory> # following lines could be used for environment with self-signed certificates and hostnames not resolved by DNS server #export OLM_UTILS_LAUNCH_ARGS=\"-v ./api-wxai.pem:/etc/k8scert --env K8S_AUTH_SSL_CA_CERT=/etc/k8scert --add-host oauth-openshift.apps.ocpinstall.gym.lan:192.168.252.4 --add-host api.ocpinstall.gym.lan:192.168.252.3\" export PATH = \"/home/itzuser/cpd-cli-linux-EE-14.1.0-1189\" : $PATH #----------------------------------------------------------------------------- # Cluster #------------------------------------------------------------------------------ export OCP_URL = api.<REPLACE THIS VALUE>:6443 #export OPENSHIFT_TYPE=<enter your deployment type> #export IMAGE_ARCH=amd64 export OCP_USERNAME = kubeadmin export OCP_PASSWORD = <REPLACE THIS VALUE> # export OCP_TOKEN=<enter your token> export SERVER_ARGUMENTS = \"--server= ${ OCP_URL } \" export LOGIN_ARGUMENTS = \"--username= ${ OCP_USERNAME } --password= ${ OCP_PASSWORD } \" # export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\" export CPDM_OC_LOGIN = \"cpd-cli manage login-to-ocp ${ SERVER_ARGUMENTS } ${ LOGIN_ARGUMENTS } \" export OC_LOGIN = \"oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } \" #------------------------------------------------------------------------------ # Projects #------------------------------------------------------------------------------ export PROJECT_LICENSE_SERVICE = cpd-license export PROJECT_SCHEDULING_SERVICE = cpd-scheduling export PROJECT_CPD_INST_OPERATORS = cpd-operators export PROJECT_CPD_INST_OPERANDS = cpd-watsonx # export PROJECT_CPD_INSTANCE_TETHERED=<enter your tethered project> # export PROJECT_CPD_INSTANCE_TETHERED_LIST=<a comma-separated list of teth-ered projects> #------------------------------------------------------------------------------ # Storage #------------------------------------------------------------------------------ export STG_CLASS_BLOCK = ocs-storagecluster-ceph-rbd export STG_CLASS_FILE = ocs-storagecluster-cephfs #------------------------------------------------------------------------------ # IBM Entitled Registry #------------------------------------------------------------------------------ export IBM_ENTITLEMENT_KEY = <REPLACE THIS VALUE> #------------------------------------------------------------------------------ # Cloud Pak for Data version #------------------------------------------------------------------------------ export VERSION = 5 .1.0 #------------------------------------------------------------------------------ # Components #------------------------------------------------------------------------------ #export COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_plat-form # export COMPONENTS_TO_SKIP=<component-ID-1>,<component-ID-2> #export COMPONENTS=ibm-cert-manager,ibm-licensing,cpfs,scheduler,cpd_plat-form,wml,ws,watsonx_ai export COMPONENTS = cpd_platform,wca You must make modifications to Line 19 , Line 23 , and Line 49 of the CP4D Environment Variables to tailor the variables to your specific cluster. Line 19: substitute the <REPLACE THIS VALUE> placeholder for export OCP_URL= with to the value of OpenShift Base Domain that was recorded in Step 2. Do not replace the .api. or :6443: components , as these are required. Your modified Line 19 should resemble the following: export OCP_URL = api.678a250b79141644e78804e0.ocp.techzone.ibm.com:6443 Line 23: set the value of export OCP_PASSWORD= equal to the value of Cluster Admin Password recorded in Step 6 of the previous module. For example: export OCP_PASSWORD = password1234 Line 49: set the value of export IBM_ENTITLEMENT_KEY= equal to the value of the key specific to your IBM account. Reference the IBM Entitlement API Key that was generated in the iii. Prerequisites checklist section of Module 1 (Objectives and Requirements) . Instructions for how to generate an IBM Entitlement API Key are provided in that section. For example: export IBM_ENTITLEMENT_KEY = verylongAPIkey1234 With your Terminal console, execute the following instruction to open the vi editor and create a shell script named cpd_vars.sh on the bastion host: vi cpd_vars.sh Copy the modified CP4D Environment Variables contents from the notepad (Step 7) to your machine's clipboard. Switch back to the Terminal where the VI editor is now open and press the I key to enable inserting text. Press Cmd + V (or Ctrl + V ) to paste the contents from your clipboard. To save, press Esc and then type :wq followed by Enter to write the file and exit the editor.","title":"iv. Environment variables"},{"location":"on-premises/3/#v-cloud-pak-for-data-command-line-interface-cpd-cli","text":"Now that the environment variables have been set, the next step towards installing CP4D is preparing the command line interface ( cpd-cli ). Copy the following code block and execute it within the console to install cpd-cli : wget https://github.com/IBM/cpd-cli/releases/download/v14.1.0/cpd-cli-linux-EE-14.1.0.tgz tar -xzf cpd-cli-linux-EE-14.1.0.tgz export PATH = \" $( pwd ) /cpd-cli-linux-EE-14.1.0-1189\" : $PATH The operation will take approximately 1 minute to complete. Verify that the CLI has been successfully integrated with the following command: cpd-cli The output from the console should resemble the screenshot below. Verify the status of the restarted container by typing podman ps and Enter , which should return the result of a single container running on the bastion host. echo $PATH cpd-cli manage restart-container The operation will take approximately 1 minute to complete. After a successful operation, the console will return a pair of messages that resemble the following: [ SUCCESS ] 2025 -03-19T17:51:35.317974Z Successfully started the container olm-utils-play-v3 [ SUCCESS ] 2025 -03-19T17:51:35.318017Z Container olm-utils-play-v3 has been re-created Source the newly-configured environment variables with the following command: source cpd_vars.sh Test that the login for oc command line is now functioning properly: oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } The command line should return back with a prompt describing You have access to ... projects which indicates that oc and the environment variables have been configured.","title":"v. Cloud Pak for Data command line interface (cpd-cli)"},{"location":"on-premises/3/#vi-next-steps","text":"At this stage the bastion host node has been fully configured ahead of installing the necessary software, which will be covered in the subsequent modules. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"vi. Next steps"},{"location":"on-premises/4/","text":"Cluster Preparation On-Premises Installation and Deployment The following section is based off of IBM Documentation detailing how to prepare an OpenShift cluster for IBM Cloud Pak for Data. Reference the instructions in full at the following resource: Preparing Your Cluster for IBM Software Hub . i. Change the process IDs limit With a newly installed cluster, a KubeletConfig will need to be manually created before the cluster's process IDs can be modified. This file will define the podPidsLimit and maxPods variables for the environment. Copy the contents of the following code block and then execute within your Terminal console to generate a new KubeletConfig file: oc apply -f - << EOF apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpd-watsonx-kubeletconfig spec: kubeletConfig: podPidsLimit: 16384 podsPerCore: 0 maxPods: 500 machineConfigPoolSelector: matchExpressions: - key: pools.operator.machineconfiguration.openshift.io/worker operator: Exists EOF KUBELETCONFIG TEST You can test whether a KubeletConfig file exists on the system by executing the following command: oc get kubeletconfig Use the CP4D command line ( cpd-cli ) to log into OCP by executing the following code: cpd-cli manage login-to-ocp \\ --username = ${ OCP_USERNAME } \\ --password = ${ OCP_PASSWORD } \\ --server = ${ OCP_URL } The operation will take approximately 1 minute to complete. After a successful run, the console should return a pair of messages that resemble the following: [ SUCCESS ] 2025 -03-19T18:04:23.978349Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T18:04:23.978412Z The login-to-ocp command ran successfully. ii. Update cluster global pull secret Use cpd-cli to manage the creation or updating of the global image pull secret via the add-icr-cred-to-global-pull-secret command. Execute the following command within the Terminal console: cpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key = ${ IBM_ENTITLEMENT_KEY } The console will return two [SUCCESS] statements indicating a successful run. Now you must update all nodes across the cluster using OpenShift command line ( oc ). Execute the following instructions via the Terminal console: oc login ${ OCP_URL } --username = ${ OCP_USERNAME } --password = ${ OCP_PASSWORD } oc get mcp Once completed, execute the following statement to check the status of the cluster nodes (this can be performed periodically to track the progress of the node updates): cpd-cli manage oc get nodes Wait until the STATUS returns for all nodes (3 master nodes, 3 storage nodes, and 3 worker nodes) all report as Ready . iii. Next steps In the following module, you will install the necessary prerequisite software required to deploy IBM Cloud Pak for Data and IBM watsonx Code Assistant. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"4. Cluster preparation"},{"location":"on-premises/4/#cluster-preparationon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation detailing how to prepare an OpenShift cluster for IBM Cloud Pak for Data. Reference the instructions in full at the following resource: Preparing Your Cluster for IBM Software Hub .","title":"Cluster PreparationOn-Premises Installation and Deployment"},{"location":"on-premises/4/#i-change-the-process-ids-limit","text":"With a newly installed cluster, a KubeletConfig will need to be manually created before the cluster's process IDs can be modified. This file will define the podPidsLimit and maxPods variables for the environment. Copy the contents of the following code block and then execute within your Terminal console to generate a new KubeletConfig file: oc apply -f - << EOF apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpd-watsonx-kubeletconfig spec: kubeletConfig: podPidsLimit: 16384 podsPerCore: 0 maxPods: 500 machineConfigPoolSelector: matchExpressions: - key: pools.operator.machineconfiguration.openshift.io/worker operator: Exists EOF KUBELETCONFIG TEST You can test whether a KubeletConfig file exists on the system by executing the following command: oc get kubeletconfig Use the CP4D command line ( cpd-cli ) to log into OCP by executing the following code: cpd-cli manage login-to-ocp \\ --username = ${ OCP_USERNAME } \\ --password = ${ OCP_PASSWORD } \\ --server = ${ OCP_URL } The operation will take approximately 1 minute to complete. After a successful run, the console should return a pair of messages that resemble the following: [ SUCCESS ] 2025 -03-19T18:04:23.978349Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T18:04:23.978412Z The login-to-ocp command ran successfully.","title":"i. Change the process IDs limit"},{"location":"on-premises/4/#ii-update-cluster-global-pull-secret","text":"Use cpd-cli to manage the creation or updating of the global image pull secret via the add-icr-cred-to-global-pull-secret command. Execute the following command within the Terminal console: cpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key = ${ IBM_ENTITLEMENT_KEY } The console will return two [SUCCESS] statements indicating a successful run. Now you must update all nodes across the cluster using OpenShift command line ( oc ). Execute the following instructions via the Terminal console: oc login ${ OCP_URL } --username = ${ OCP_USERNAME } --password = ${ OCP_PASSWORD } oc get mcp Once completed, execute the following statement to check the status of the cluster nodes (this can be performed periodically to track the progress of the node updates): cpd-cli manage oc get nodes Wait until the STATUS returns for all nodes (3 master nodes, 3 storage nodes, and 3 worker nodes) all report as Ready .","title":"ii. Update cluster global pull secret"},{"location":"on-premises/4/#iii-next-steps","text":"In the following module, you will install the necessary prerequisite software required to deploy IBM Cloud Pak for Data and IBM watsonx Code Assistant. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"iii. Next steps"},{"location":"on-premises/5/","text":"Install Prerequisite Software On-Premises Installation and Deployment Review the latest documentation on IBM Software Hub to determine the appropriate version needed for your client opportunity and OpenShift cluster version. The following module follows the documentation for installing IBM Software Hub's cert-manager-operator.v.13.0 for OpenShift Container Platform (OCP) v4.16. i. Install the Red Hat OpenShift cert-manager After the release of IBM Software Hub , previous methods for installing IBM Cloud Pak for Data (CP4D) that relied on IBM Cert Manager are no longer required. IBM Software Hub is now the recommended path and will be the method adhered in the following section. First create the prerequisite project namespace by executing the following instruction: oc new-project cert-manager-operator Afterwards, return to your web browser and open the OCP Dashboard ( Step 8 of Module 2 ). From the left-hand navigation menu, navigate to Operators > OperatorHub [A] . Into the filter box [B] , type cert-manager Operator for Red Hat OpenShift and click [C] the filtered result with the same name. From the configuration screen, select the following options: Channel [A] : stable-v1.13 Version [B] : 1.13.0 When ready, click the blue Install [C] button A new page will load, summarizing the settings of the proposed Operator. Verify that the details are correct: Update channel [A] : stable-v1.13 Version [B] : 1.13.0 Installation mode [C] : A specific namespace on the cluster Installed namespace [D] : Operator recommended Namespace: cert-manager-operator Update approval [E] : Manual When ready, scroll down to the bottom of the page and click the blue Install [F] button. The page will refresh periodically as the Operator is installed on the cluster. This may take several minutes to complete. When prompted with a Manual approval required splashscreen, click Approve [G] Wait until the page refreshes with the message Installed operator: ready for use and then click View Operator [H] to view the finalized Operator You can also track the progress of the Operator installation by drilling down into Operators > Installed Operators from the left-hand side of the OCP Dashboard. Scroll down until you locate the cert-manager Operator for Red Hat OpenShift entry in the table. Monitor the progress by observing changes to the fourth column of the table. Wait until Operator shows a status of Succeeded . On the OCP Dashboard, make sure you are under the Operators > Installed Operators view. Click the title of the cert-manager-Operator for Red Hat OpenShift to open the details panel From the tabs along the top of the page, click Subscription [A] Under the Subscription details header, there are three panels: Update channel , Update approval , and Update status * Click the hyperlinked 1 requires approval [B] text next to the Update Status panel A new page, InstallPlan details for install-12xyz , will load. Click the blue Preview InstallPlan [A] button When the panel refreshes, click the blue Approve button to begin updating to v1.13.1 Wait for the status to the right of install-12xyz to read Complete [B] From the left-hand navigation bar of the OCP Dashboard, once again navigate into Operators > Installed Operators to watch the progress of the update. The cert-manager Operator for Red Hat OpenShift will show a Status of Installing while the update is underway. Wait until the status changes to Succeeded [A] . Verify that cert-manager pods are up and running by executing the following command with your Terminal console: oc get pods -n cert-manager The console should return output closely resembling the following: NAME READY STATUS RESTARTS AGE cert-manager-bd7fbb9fc-wvbbt 1 /1 Running 0 3m39s cert-manager-cainjector-56cc5f9868-7g9z7 1 /1 Running 0 4m5s cert-manager-webhook-d4f79d7f7-9dg9w 1 /1 Running 0 4m9s At this point, the OpenShift cert-manager has been successfully deployed on the cluster. ii. Operators for GPUs GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. Although GPUs will not be available to deploy or interact with for the On-Premises Installation and Deployment L4 training modules, participants will still be able to practice and learn the skills needed to prepare a cluster for GPUs. This section will cover all of the necessary configuration and setup required to make GPUs available to an IBM watsonx Code Asssitant service \u2014 shy of actually getting to use the GPUs with the on-premises deployment. Participants will still be able to interact with GPU-powered instances in the latter IBM Cloud (SaaS) modules of the L4 curriculum. Services such as IBM watsonx Code Assistant (on-premises), which requires access to GPUs, need to install several Operators on the OpenShift cluster to support the management of NVIDIA software components. Those components, in turn, are needed to provision the GPUs for access by the cluster. IBM watsonx Code Assistant requires that the following Operators be installed: - Node Feature Discovery Operator : within the openshift-nfd namespace - NVIDIA GPU Operator : within the nvidia-gpu-operator namespace - Red Hat OpenShift AI iii. Install the Node Feature Discovery Operator The following section is based off a selection of the complete IBM Documentation available for Installing operators for services that require GPUs . The following section will provide the instructions necessary to replicate this procedure. Participants are welcome to practice this with the L4 environment provided \u2014 just be aware that no physical GPU hardware will be available or connected at the conclusion of these steps. First, you will use the Terminal console to programmatically create a namespace openshift-nfd for the Node Feature Discovery (NDF) Operator. The following instruction set will create a namespace Custom Resource (CR) that defines the openshift-ndf namespace and then saves the YAML file to nfd-namespace.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-nfd labels: name: openshift-nfd openshift.io/cluster-monitoring: \"true\" EOF Install the NDF Operator within the openshift-nfd namespace that was created in Step 11 by first defining the following objects. This will create an OperatorGroup CR and subsequently save the YAML file to operatorgroup.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd EOF Create a Subscription CR and save the YAML file to nfd-sub.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Check on the status of the OpenShift pods with the following command: oc get pods -n openshift-nfd Initially, the console may return a response such as No resources found in openshift-ndf namespace. This likely is because the pod is still provisioning. Check the status of the deployment with the following steps: Return to the OCP Dashboard and navigate to Operators > Installed Operators [A] Within the table, look for a resource named nfd [B] If the code from Step 13 was only recently executed, the Managed Namespaces of this resource may be showing as None and the Status as Unknown [C] Wait a few moments (and refresh the page if you wish) until the nfd resource is replaced by Node Feature Discovery Operator [D] , which should then belong to the openshift-nfd Managed Namespace and have a Succeeded Status [E] This operation will take approximately 2 minutes to complete. Wait and then execute the previous command from Step 14 a second time within the Terminal console. Observe the updated pod status. The console should report back the following to indicate that the Operator was successfully deployed: NAME READY STATUS RESTARTS AGE nfd-controller-manager-7f4c4cf577-wn96r 2 /2 Running 0 3m10s Create a NodeFeatureDiscovery CR and save the YAML file to NodeFeatureDiscovery.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: instance: \"\" # instance is empty by default operand: image: registry.redhat.io/openshift4/ose-node-feature-discovery-rhel9:v4.16 imagePullPolicy: Always workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time configurable ## and require a nfd-worker restart to take effect after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: cpu: cpuid: # NOTE: whitelist has priority over blacklist attributeBlacklist: - \"BMI1\" - \"BMI2\" - \"CLMUL\" - \"CMOV\" - \"CX16\" - \"ERMS\" - \"F16C\" - \"HTT\" - \"LZCNT\" - \"MMX\" - \"MMXEXT\" - \"NX\" - \"POPCNT\" - \"RDRAND\" - \"RDSEED\" - \"RDTSCP\" - \"SGX\" - \"SSE\" - \"SSE2\" - \"SSE3\" - \"SSE4.1\" - \"SSE4.2\" - \"SSSE3\" attributeWhitelist: kernel: kconfigFile: \"/path/to/kconfig\" configOpts: - \"NO_HZ\" - \"X86\" - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: - \"class\" - \"vendor\" EOF As before, check on the status of the OpenShift pods with the following command: oc get pods -n openshift-nfd This operation will take approximately 1 minutes to complete. Wait until all pods return a status of Running before continuing to Step 17. iv. Install the NVIDIA GPU Operator The following section is based off a selection of the complete NVIDIA Corporation documentation for Installing the NVIDIA GPU Operator on OpenShift . Create the nvidia-gpu-operator namespace by executing the following code block with a Terminal console: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Define the OperatorGroup within the same namespace: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Execute the following code block within a Terminal console to get the channel version (required for Step 20), as well as create the gpu-operator-certified Operator within the nvidia-gpu-operator namespace: export CHANNEL = $( oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath = '{.status.defaultChannel}' ; echo ) export CURRENT_CSV = $( oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"' $CHANNEL '\") | .currentCSV' ; echo ) oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"${CHANNEL}\" installPlanApproval: Manual name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"${CURRENT_CSV}\" EOF Execute the following command to verify the status of the install plan: oc get installplan -n nvidia-gpu-operator No resources found in nvidia-gpu-operator namespace. Troubleshooting: The nvidia-gpu-operator namespace is still being deployed. Wait for 1 minute and then try executing Step 20 again. The console will return a statement that Operator's APPROVAL status is currently set to Manual and incomplete ( APPROVED state is set to false ). NAME CSV APPROVAL APPROVED install-4rq4j gpu-operator-certified.v24.9.2 Manual false Therefore, you will need to approve the install plan by executing the following command: export INSTALL_PLAN = $( oc get installplan -n nvidia-gpu-operator -oname ) oc patch $INSTALL_PLAN -n nvidia-gpu-operator --type merge --patch '{\"spec\":{\"approved\":true }}' Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the update. Wait until the NVIDIA GPU Operator [A] appears in the table with Status set to Succeeded before continuing. Create a ClusterPolicy by executing the following instructions: oc get csv -n nvidia-gpu-operator ${ CURRENT_CSV } -ojsonpath ={ .metadata.annotations.alm-examples } | jq . [ 0 ] > clusterpolicy.json oc apply -f clusterpolicy.json Check the status of the pods. If GPUs are detected within the cluster, they will be included in the read-out. As the Level 4 demonstration environment is not equipped with GPU hardware, no GPU resources will be discovered by the oc get command. Execute the following command with the Terminal console: oc get pods -n nvidia-gpu-operator NOTE TO THE AUTHOR THIS WILL BE THE SECTION YOU WILL NEED TO FILL OUT WITH A DEDICATED CLUSTER WITH GPU ACCESS, PURELY FOR DEMONSTRATION PURPOSES. REFER TO 00:21:00 TIMESTAMP IN THE RECORDING. v. Install Red Hat OpenShift AI The following steps are extracted from the complete documentation available from IBM Documentation for Installing Red Hat OpenShift AI . IBM watsonx Code Assistant (on-premises) requires installation and configuration of Red Hat OpenShift AI on the OCP cluster. In the following section, you will: Install the Operator for Red Hat OpenShift AI (v2.13) Create a DSCInitialization instance Create a DataScienceCluster instance Edit the model inferencing configuration Create the redhat-ods-operator Project on the OCP cluster with the following command: oc new-project redhat-ods-operator Define the rhods-operator Operator Group within the project defined in Step 23: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: rhods-operator namespace: redhat-ods-operator EOF Create the Operator Subscription for rhods-operator within the same project: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: rhods-operator namespace: redhat-ods-operator spec: name: rhods-operator channel: stable-2.13 source: redhat-operators sourceNamespace: openshift-marketplace config: env: - name: \"DISABLE_DSC_CONFIG\" EOF This operation will take approximately 5 minutes to complete. Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the Red Hat OpenShift AI [A] Operator deployment. Create a DSCInitialization object default-dsci in redhat-ods-monitoring project: cat <<EOF |oc apply -f - apiVersion: dscinitialization.opendatahub.io/v1 kind: DSCInitialization metadata: name: default-dsci spec: applicationsNamespace: redhat-ods-applications monitoring: managementState: Managed namespace: redhat-ods-monitoring serviceMesh: managementState: Removed trustedCABundle: managementState: Managed customCABundle: \"\" EOF Monitor the progress of the DSCInitialization object by executing the following command: oc get dscinitialization This operation will take approximately 2 minutes to complete. Wait for the DSCInitialization object ( dscinitialization ) to return a Ready status before continuing on to Step 27. Create a DataScienceCluster object named default-dsc within the same project: cat <<EOF |oc apply -f - apiVersion: datasciencecluster.opendatahub.io/v1 kind: DataScienceCluster metadata: name: default-dsc spec: components: codeflare: managementState: Removed dashboard: managementState: Removed datasciencepipelines: managementState: Removed kserve: managementState: Managed defaultDeploymentMode: RawDeployment serving: managementState: Removed name: knative-serving kueue: managementState: Removed modelmeshserving: managementState: Removed ray: managementState: Removed trainingoperator: managementState: Managed trustyai: managementState: Removed workbenches: managementState: Removed EOF The Red Hat OpenShift AI Operator automatically installs and manages services that are listed as Managed . Services with a status of Removed are ignored and will not be installed. This operation will take approximately 2-3 minutes to complete. Execute the following instruction to check the status of pods within the redhat-ods-applications project: oc get pods -n redhat-ods-applications If no pods are detected, wait an additional 3 minutes and try again. Confirm that ALL FIVE of the following pods are deployed with a Running status before continuing to Step 29. NAME READY STATUS RESTARTS AGE kserve-controller-manager-7b745757cb-wx7qc 1 /1 Running 0 42s kubeflow-training-operator-659b5dcb99-5t8vt 1 /1 Running 0 56s odh-model-controller-b774fb859-2b4b4 1 /1 Running 0 117s odh-model-controller-b774fb859-4gxhv 1 /1 Running 0 117s odh-model-controller-b774fb859-fpnrl 1 /1 Running 0 117s Edit the inferenceservice-config configuration map within the redhat-ods-applications project. This is best achieved by using the OCP Dashboard via a web browser. From the OCP Dashboard, navigate into Workloads > Configmaps [A] From the Project [B] drop-down list, select redhat-ods-applications [C] Click the inferenceservice-config resource [D] and open the YAML tab [E] In the metadata.annotations section [F] (Lines 4-5) of the file, add a new Line 8 [G] which reads opendatahub.io/managed: 'false' The resulting YAML file should resemble the following: metadata : annotations : internal.config.kubernetes.io/previousKinds : ConfigMap internal.config.kubernetes.io/previousNames : inferenceservice-config internal.config.kubernetes.io/previousNamespaces : opendatahub opendatahub.io/managed : 'false' Within the same YAML file, look for Line 379 under the ingress section [A] : \"domainTemplate\" : \"{ .Name }-{ .Namespace }.{ .IngressDomain }\" , Replace Line 379 with the following (be sure to include the last , character) [B] : \"domainTemplate\" : \"example.com\" , Click Save [C] (bottom-left) to finalize the changes to the YAML file. If prompted with a Managed resource warning, click Save [D] again to confirm. vi. Next steps At this stage, all of the necessary prerequisites have been installed and you are ready to begin installation of an IBM Software Hub instance on the OCP cluster. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"5. Install prerequisite software"},{"location":"on-premises/5/#install-prerequisite-softwareon-premises-installation-and-deployment","text":"Review the latest documentation on IBM Software Hub to determine the appropriate version needed for your client opportunity and OpenShift cluster version. The following module follows the documentation for installing IBM Software Hub's cert-manager-operator.v.13.0 for OpenShift Container Platform (OCP) v4.16.","title":"Install Prerequisite SoftwareOn-Premises Installation and Deployment"},{"location":"on-premises/5/#i-install-the-red-hat-openshift-cert-manager","text":"After the release of IBM Software Hub , previous methods for installing IBM Cloud Pak for Data (CP4D) that relied on IBM Cert Manager are no longer required. IBM Software Hub is now the recommended path and will be the method adhered in the following section. First create the prerequisite project namespace by executing the following instruction: oc new-project cert-manager-operator Afterwards, return to your web browser and open the OCP Dashboard ( Step 8 of Module 2 ). From the left-hand navigation menu, navigate to Operators > OperatorHub [A] . Into the filter box [B] , type cert-manager Operator for Red Hat OpenShift and click [C] the filtered result with the same name. From the configuration screen, select the following options: Channel [A] : stable-v1.13 Version [B] : 1.13.0 When ready, click the blue Install [C] button A new page will load, summarizing the settings of the proposed Operator. Verify that the details are correct: Update channel [A] : stable-v1.13 Version [B] : 1.13.0 Installation mode [C] : A specific namespace on the cluster Installed namespace [D] : Operator recommended Namespace: cert-manager-operator Update approval [E] : Manual When ready, scroll down to the bottom of the page and click the blue Install [F] button. The page will refresh periodically as the Operator is installed on the cluster. This may take several minutes to complete. When prompted with a Manual approval required splashscreen, click Approve [G] Wait until the page refreshes with the message Installed operator: ready for use and then click View Operator [H] to view the finalized Operator You can also track the progress of the Operator installation by drilling down into Operators > Installed Operators from the left-hand side of the OCP Dashboard. Scroll down until you locate the cert-manager Operator for Red Hat OpenShift entry in the table. Monitor the progress by observing changes to the fourth column of the table. Wait until Operator shows a status of Succeeded . On the OCP Dashboard, make sure you are under the Operators > Installed Operators view. Click the title of the cert-manager-Operator for Red Hat OpenShift to open the details panel From the tabs along the top of the page, click Subscription [A] Under the Subscription details header, there are three panels: Update channel , Update approval , and Update status * Click the hyperlinked 1 requires approval [B] text next to the Update Status panel A new page, InstallPlan details for install-12xyz , will load. Click the blue Preview InstallPlan [A] button When the panel refreshes, click the blue Approve button to begin updating to v1.13.1 Wait for the status to the right of install-12xyz to read Complete [B] From the left-hand navigation bar of the OCP Dashboard, once again navigate into Operators > Installed Operators to watch the progress of the update. The cert-manager Operator for Red Hat OpenShift will show a Status of Installing while the update is underway. Wait until the status changes to Succeeded [A] . Verify that cert-manager pods are up and running by executing the following command with your Terminal console: oc get pods -n cert-manager The console should return output closely resembling the following: NAME READY STATUS RESTARTS AGE cert-manager-bd7fbb9fc-wvbbt 1 /1 Running 0 3m39s cert-manager-cainjector-56cc5f9868-7g9z7 1 /1 Running 0 4m5s cert-manager-webhook-d4f79d7f7-9dg9w 1 /1 Running 0 4m9s At this point, the OpenShift cert-manager has been successfully deployed on the cluster.","title":"i. Install the Red Hat OpenShift cert-manager"},{"location":"on-premises/5/#ii-operators-for-gpus","text":"GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. Although GPUs will not be available to deploy or interact with for the On-Premises Installation and Deployment L4 training modules, participants will still be able to practice and learn the skills needed to prepare a cluster for GPUs. This section will cover all of the necessary configuration and setup required to make GPUs available to an IBM watsonx Code Asssitant service \u2014 shy of actually getting to use the GPUs with the on-premises deployment. Participants will still be able to interact with GPU-powered instances in the latter IBM Cloud (SaaS) modules of the L4 curriculum. Services such as IBM watsonx Code Assistant (on-premises), which requires access to GPUs, need to install several Operators on the OpenShift cluster to support the management of NVIDIA software components. Those components, in turn, are needed to provision the GPUs for access by the cluster. IBM watsonx Code Assistant requires that the following Operators be installed: - Node Feature Discovery Operator : within the openshift-nfd namespace - NVIDIA GPU Operator : within the nvidia-gpu-operator namespace - Red Hat OpenShift AI","title":"ii. Operators for GPUs"},{"location":"on-premises/5/#iii-install-the-node-feature-discovery-operator","text":"The following section is based off a selection of the complete IBM Documentation available for Installing operators for services that require GPUs . The following section will provide the instructions necessary to replicate this procedure. Participants are welcome to practice this with the L4 environment provided \u2014 just be aware that no physical GPU hardware will be available or connected at the conclusion of these steps. First, you will use the Terminal console to programmatically create a namespace openshift-nfd for the Node Feature Discovery (NDF) Operator. The following instruction set will create a namespace Custom Resource (CR) that defines the openshift-ndf namespace and then saves the YAML file to nfd-namespace.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-nfd labels: name: openshift-nfd openshift.io/cluster-monitoring: \"true\" EOF Install the NDF Operator within the openshift-nfd namespace that was created in Step 11 by first defining the following objects. This will create an OperatorGroup CR and subsequently save the YAML file to operatorgroup.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd EOF Create a Subscription CR and save the YAML file to nfd-sub.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Check on the status of the OpenShift pods with the following command: oc get pods -n openshift-nfd Initially, the console may return a response such as No resources found in openshift-ndf namespace. This likely is because the pod is still provisioning. Check the status of the deployment with the following steps: Return to the OCP Dashboard and navigate to Operators > Installed Operators [A] Within the table, look for a resource named nfd [B] If the code from Step 13 was only recently executed, the Managed Namespaces of this resource may be showing as None and the Status as Unknown [C] Wait a few moments (and refresh the page if you wish) until the nfd resource is replaced by Node Feature Discovery Operator [D] , which should then belong to the openshift-nfd Managed Namespace and have a Succeeded Status [E] This operation will take approximately 2 minutes to complete. Wait and then execute the previous command from Step 14 a second time within the Terminal console. Observe the updated pod status. The console should report back the following to indicate that the Operator was successfully deployed: NAME READY STATUS RESTARTS AGE nfd-controller-manager-7f4c4cf577-wn96r 2 /2 Running 0 3m10s Create a NodeFeatureDiscovery CR and save the YAML file to NodeFeatureDiscovery.yaml . Copy and paste the following code block into the Terminal console, then hit Enter : oc apply -f - <<EOF apiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: instance: \"\" # instance is empty by default operand: image: registry.redhat.io/openshift4/ose-node-feature-discovery-rhel9:v4.16 imagePullPolicy: Always workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time configurable ## and require a nfd-worker restart to take effect after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: cpu: cpuid: # NOTE: whitelist has priority over blacklist attributeBlacklist: - \"BMI1\" - \"BMI2\" - \"CLMUL\" - \"CMOV\" - \"CX16\" - \"ERMS\" - \"F16C\" - \"HTT\" - \"LZCNT\" - \"MMX\" - \"MMXEXT\" - \"NX\" - \"POPCNT\" - \"RDRAND\" - \"RDSEED\" - \"RDTSCP\" - \"SGX\" - \"SSE\" - \"SSE2\" - \"SSE3\" - \"SSE4.1\" - \"SSE4.2\" - \"SSSE3\" attributeWhitelist: kernel: kconfigFile: \"/path/to/kconfig\" configOpts: - \"NO_HZ\" - \"X86\" - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: - \"class\" - \"vendor\" EOF As before, check on the status of the OpenShift pods with the following command: oc get pods -n openshift-nfd This operation will take approximately 1 minutes to complete. Wait until all pods return a status of Running before continuing to Step 17.","title":"iii. Install the Node Feature Discovery Operator"},{"location":"on-premises/5/#iv-install-the-nvidia-gpu-operator","text":"The following section is based off a selection of the complete NVIDIA Corporation documentation for Installing the NVIDIA GPU Operator on OpenShift . Create the nvidia-gpu-operator namespace by executing the following code block with a Terminal console: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Define the OperatorGroup within the same namespace: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Execute the following code block within a Terminal console to get the channel version (required for Step 20), as well as create the gpu-operator-certified Operator within the nvidia-gpu-operator namespace: export CHANNEL = $( oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath = '{.status.defaultChannel}' ; echo ) export CURRENT_CSV = $( oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"' $CHANNEL '\") | .currentCSV' ; echo ) oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"${CHANNEL}\" installPlanApproval: Manual name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"${CURRENT_CSV}\" EOF Execute the following command to verify the status of the install plan: oc get installplan -n nvidia-gpu-operator No resources found in nvidia-gpu-operator namespace. Troubleshooting: The nvidia-gpu-operator namespace is still being deployed. Wait for 1 minute and then try executing Step 20 again. The console will return a statement that Operator's APPROVAL status is currently set to Manual and incomplete ( APPROVED state is set to false ). NAME CSV APPROVAL APPROVED install-4rq4j gpu-operator-certified.v24.9.2 Manual false Therefore, you will need to approve the install plan by executing the following command: export INSTALL_PLAN = $( oc get installplan -n nvidia-gpu-operator -oname ) oc patch $INSTALL_PLAN -n nvidia-gpu-operator --type merge --patch '{\"spec\":{\"approved\":true }}' Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the update. Wait until the NVIDIA GPU Operator [A] appears in the table with Status set to Succeeded before continuing. Create a ClusterPolicy by executing the following instructions: oc get csv -n nvidia-gpu-operator ${ CURRENT_CSV } -ojsonpath ={ .metadata.annotations.alm-examples } | jq . [ 0 ] > clusterpolicy.json oc apply -f clusterpolicy.json Check the status of the pods. If GPUs are detected within the cluster, they will be included in the read-out. As the Level 4 demonstration environment is not equipped with GPU hardware, no GPU resources will be discovered by the oc get command. Execute the following command with the Terminal console: oc get pods -n nvidia-gpu-operator NOTE TO THE AUTHOR THIS WILL BE THE SECTION YOU WILL NEED TO FILL OUT WITH A DEDICATED CLUSTER WITH GPU ACCESS, PURELY FOR DEMONSTRATION PURPOSES. REFER TO 00:21:00 TIMESTAMP IN THE RECORDING.","title":"iv. Install the NVIDIA GPU Operator"},{"location":"on-premises/5/#v-install-red-hat-openshift-ai","text":"The following steps are extracted from the complete documentation available from IBM Documentation for Installing Red Hat OpenShift AI . IBM watsonx Code Assistant (on-premises) requires installation and configuration of Red Hat OpenShift AI on the OCP cluster. In the following section, you will: Install the Operator for Red Hat OpenShift AI (v2.13) Create a DSCInitialization instance Create a DataScienceCluster instance Edit the model inferencing configuration Create the redhat-ods-operator Project on the OCP cluster with the following command: oc new-project redhat-ods-operator Define the rhods-operator Operator Group within the project defined in Step 23: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: rhods-operator namespace: redhat-ods-operator EOF Create the Operator Subscription for rhods-operator within the same project: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: rhods-operator namespace: redhat-ods-operator spec: name: rhods-operator channel: stable-2.13 source: redhat-operators sourceNamespace: openshift-marketplace config: env: - name: \"DISABLE_DSC_CONFIG\" EOF This operation will take approximately 5 minutes to complete. Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the Red Hat OpenShift AI [A] Operator deployment. Create a DSCInitialization object default-dsci in redhat-ods-monitoring project: cat <<EOF |oc apply -f - apiVersion: dscinitialization.opendatahub.io/v1 kind: DSCInitialization metadata: name: default-dsci spec: applicationsNamespace: redhat-ods-applications monitoring: managementState: Managed namespace: redhat-ods-monitoring serviceMesh: managementState: Removed trustedCABundle: managementState: Managed customCABundle: \"\" EOF Monitor the progress of the DSCInitialization object by executing the following command: oc get dscinitialization This operation will take approximately 2 minutes to complete. Wait for the DSCInitialization object ( dscinitialization ) to return a Ready status before continuing on to Step 27. Create a DataScienceCluster object named default-dsc within the same project: cat <<EOF |oc apply -f - apiVersion: datasciencecluster.opendatahub.io/v1 kind: DataScienceCluster metadata: name: default-dsc spec: components: codeflare: managementState: Removed dashboard: managementState: Removed datasciencepipelines: managementState: Removed kserve: managementState: Managed defaultDeploymentMode: RawDeployment serving: managementState: Removed name: knative-serving kueue: managementState: Removed modelmeshserving: managementState: Removed ray: managementState: Removed trainingoperator: managementState: Managed trustyai: managementState: Removed workbenches: managementState: Removed EOF The Red Hat OpenShift AI Operator automatically installs and manages services that are listed as Managed . Services with a status of Removed are ignored and will not be installed. This operation will take approximately 2-3 minutes to complete. Execute the following instruction to check the status of pods within the redhat-ods-applications project: oc get pods -n redhat-ods-applications If no pods are detected, wait an additional 3 minutes and try again. Confirm that ALL FIVE of the following pods are deployed with a Running status before continuing to Step 29. NAME READY STATUS RESTARTS AGE kserve-controller-manager-7b745757cb-wx7qc 1 /1 Running 0 42s kubeflow-training-operator-659b5dcb99-5t8vt 1 /1 Running 0 56s odh-model-controller-b774fb859-2b4b4 1 /1 Running 0 117s odh-model-controller-b774fb859-4gxhv 1 /1 Running 0 117s odh-model-controller-b774fb859-fpnrl 1 /1 Running 0 117s Edit the inferenceservice-config configuration map within the redhat-ods-applications project. This is best achieved by using the OCP Dashboard via a web browser. From the OCP Dashboard, navigate into Workloads > Configmaps [A] From the Project [B] drop-down list, select redhat-ods-applications [C] Click the inferenceservice-config resource [D] and open the YAML tab [E] In the metadata.annotations section [F] (Lines 4-5) of the file, add a new Line 8 [G] which reads opendatahub.io/managed: 'false' The resulting YAML file should resemble the following: metadata : annotations : internal.config.kubernetes.io/previousKinds : ConfigMap internal.config.kubernetes.io/previousNames : inferenceservice-config internal.config.kubernetes.io/previousNamespaces : opendatahub opendatahub.io/managed : 'false' Within the same YAML file, look for Line 379 under the ingress section [A] : \"domainTemplate\" : \"{ .Name }-{ .Namespace }.{ .IngressDomain }\" , Replace Line 379 with the following (be sure to include the last , character) [B] : \"domainTemplate\" : \"example.com\" , Click Save [C] (bottom-left) to finalize the changes to the YAML file. If prompted with a Managed resource warning, click Save [D] again to confirm.","title":"v. Install Red Hat OpenShift AI"},{"location":"on-premises/5/#vi-next-steps","text":"At this stage, all of the necessary prerequisites have been installed and you are ready to begin installation of an IBM Software Hub instance on the OCP cluster. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"vi. Next steps"},{"location":"on-premises/6/","text":"Install IBM Software Hub On-Premises Installation and Deployment The following section is based off of IBM Documentation for Preparing to install an instance of IBM Software Hub . i. Install shared components Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the OpenShift Container Platform (OCP) cluster: a license service and a scheduler . Three environment variables were \"set\" earlier in this lab, which are pertinent now to the license service (as well as the scheduler) that IBM Software Hub requires. These include: $PROJECT_LICENSE_SERVICE $PROJECT_SCHEDULING_SERVICE $VERSION Refresh yourself on these variable's values by executing the following: echo $PROJECT_LICENSE_SERVICE echo $PROJECT_SCHEDULING_SERVICE echo $VERSION These variables were defined within the Cloud Pak for Data (CP4D) Environment Variable ( Step 8 of Module 3 ) on Lines 35, 36, and 53, respectively. Install the license service components by executing the following command: cpd-cli manage apply-cluster-components \\ --release = ${ VERSION } \\ --license_acceptance = true --licensing_ns = ${ PROJECT_LICENSE_SERVICE } This operation will take approximately 10 minutes to complete. After a successful operation, the console will return the following statements: [ \u2714 ] Cert manager is ready. [ SUCCESS ] 2025 -03-19T18:58:58.400864Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T18:58:58.400949Z The apply-cluster-components command ran successfully. Install the scheduling service by executing the following command: cpd-cli manage apply-scheduler \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --scheduler_ns = ${ PROJECT_SCHEDULING_SERVICE } This operation will take approximately 10 minutes to complete. After a successful operation, the console will return the following statements: [ SUCCESS ] 2025 -03-19T19:14:17.855196Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T19:14:17.855280Z The apply-scheduler command ran successfully. ii. Install an instance of IBM Software Hub Finally, the cluster has been prepared to a state where it is ready to install an instance of IBM Software Hub. Although the following section requires relatively few inputs from the participant, it will take approximately 45 minutes for the installation process to complete. Create the two new projects that will host your instance by executing the following instructions within a Terminal console: oc new-project ${ PROJECT_CPD_INST_OPERATORS } oc new-project ${ PROJECT_CPD_INST_OPERANDS } Apply the required permissions for an instance of IBM Software Hub: cpd-cli manage authorize-instance-topology \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } This operation will take approximately 2 minutes to complete. After a successful operation, the console will return the following statements: [ SUCCESS ] 2025 -03-19T19:17:06.452627Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T19:17:06.453087Z The authorize-instance-topology command ran successfully. Create the instance by executing the following code block: cpd-cli manage setup-instance \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --block_storage_class = ${ STG_CLASS_BLOCK } \\ --file_storage_class = ${ STG_CLASS_FILE } \\ --run_storage_tests = true This operation will take approximately 45 minutes to complete, after which the console will return: [ SUCCESS ] The cpd management server was successfully created in the cpd-watsonx project [ SUCCESS ] 2025 -03-18T00:09:51.194282Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-18T00:09:51.194342Z The setup-instance command ran successfully. iii. Next steps With IBM Software Hub now successfully deployed on the OCP cluster, the environment is primed for the installation of IBM watsonx Code Assistant. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"6. Install IBM Software Hub"},{"location":"on-premises/6/#install-ibm-software-hubon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation for Preparing to install an instance of IBM Software Hub .","title":"Install IBM Software HubOn-Premises Installation and Deployment"},{"location":"on-premises/6/#i-install-shared-components","text":"Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the OpenShift Container Platform (OCP) cluster: a license service and a scheduler . Three environment variables were \"set\" earlier in this lab, which are pertinent now to the license service (as well as the scheduler) that IBM Software Hub requires. These include: $PROJECT_LICENSE_SERVICE $PROJECT_SCHEDULING_SERVICE $VERSION Refresh yourself on these variable's values by executing the following: echo $PROJECT_LICENSE_SERVICE echo $PROJECT_SCHEDULING_SERVICE echo $VERSION These variables were defined within the Cloud Pak for Data (CP4D) Environment Variable ( Step 8 of Module 3 ) on Lines 35, 36, and 53, respectively. Install the license service components by executing the following command: cpd-cli manage apply-cluster-components \\ --release = ${ VERSION } \\ --license_acceptance = true --licensing_ns = ${ PROJECT_LICENSE_SERVICE } This operation will take approximately 10 minutes to complete. After a successful operation, the console will return the following statements: [ \u2714 ] Cert manager is ready. [ SUCCESS ] 2025 -03-19T18:58:58.400864Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T18:58:58.400949Z The apply-cluster-components command ran successfully. Install the scheduling service by executing the following command: cpd-cli manage apply-scheduler \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --scheduler_ns = ${ PROJECT_SCHEDULING_SERVICE } This operation will take approximately 10 minutes to complete. After a successful operation, the console will return the following statements: [ SUCCESS ] 2025 -03-19T19:14:17.855196Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T19:14:17.855280Z The apply-scheduler command ran successfully.","title":"i. Install shared components"},{"location":"on-premises/6/#ii-install-an-instance-of-ibm-software-hub","text":"Finally, the cluster has been prepared to a state where it is ready to install an instance of IBM Software Hub. Although the following section requires relatively few inputs from the participant, it will take approximately 45 minutes for the installation process to complete. Create the two new projects that will host your instance by executing the following instructions within a Terminal console: oc new-project ${ PROJECT_CPD_INST_OPERATORS } oc new-project ${ PROJECT_CPD_INST_OPERANDS } Apply the required permissions for an instance of IBM Software Hub: cpd-cli manage authorize-instance-topology \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } This operation will take approximately 2 minutes to complete. After a successful operation, the console will return the following statements: [ SUCCESS ] 2025 -03-19T19:17:06.452627Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T19:17:06.453087Z The authorize-instance-topology command ran successfully. Create the instance by executing the following code block: cpd-cli manage setup-instance \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --block_storage_class = ${ STG_CLASS_BLOCK } \\ --file_storage_class = ${ STG_CLASS_FILE } \\ --run_storage_tests = true This operation will take approximately 45 minutes to complete, after which the console will return: [ SUCCESS ] The cpd management server was successfully created in the cpd-watsonx project [ SUCCESS ] 2025 -03-18T00:09:51.194282Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-18T00:09:51.194342Z The setup-instance command ran successfully.","title":"ii. Install an instance of IBM Software Hub"},{"location":"on-premises/6/#iii-next-steps","text":"With IBM Software Hub now successfully deployed on the OCP cluster, the environment is primed for the installation of IBM watsonx Code Assistant. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"iii. Next steps"},{"location":"on-premises/7/","text":"Install IBM watsonx Code Assistant On-Premises Installation and Deployment The following section is based off of IBM Documentation for Installing watsonx Code Assistant v5.1.x . i. User responsibilities Following the release of Software Hub v5.x , watsonx Code Assistant (WCA) administrators need to only specify the watsonx-code-assistant component in order for the Software Hub to automatically fetch the required dependencies. To install WCA on-premises, you must be an instance administrator with permissions to install software in the following Red Hat OpenShift projects: The operators project for the instance: Operators for this instance of WCA are installed in the operators project. In the installation commands, the ${PROJECT_CPD_INST_OPERATORS} environment variable refers to the operators project. The operands project for the instance: The custom resources for the control plane and WCA are installed in the operands project. In the installation commands, the ${PROJECT_CPD_INST_OPERANDS} environment variable refers to the operands project. ii. System requirements Participants are encouraged to review the following requirements again before continuing with the install to verify that all the components are in place and to reinforce their understanding as to why such components are needed. IBM SOFTWARE HUB VERSIONING All of the components that are associated with an instance of IBM Software Hub must be installed at the same release version. For example, if the IBM Software Hub control plane is installed at Version 5.1.0 , you must install WCA at Version 5.1.0 as well. ENVIRONMENT VARIABLES The commands in this task use environment variables so that you can run the commands exactly as written. In the case of this lab, those variables are defined within the cpd_vars.sh manifest that was created in Module 3. If you want to read more about how to structure scripts like cpd_vars.sh , which define environment variables within a single source of truth, review the IBM Documentation . To use the environment variables from the script, you must source the environment variables before you run the commands in this task. For example, run: source ./cpd_vars.sh SECURITY CONTEXT CONSTRAINT WCA works with the default Red Hat OpenShift\u00ae Container Platform security context constraint, restricted-v2 . COMMON CORE SERVICES WCA requires the availability of IBM Software Hub common core services. If the common core services are not installed in the operands project for the instance, the common core services are automatically installed when you install WCA. The common core services installation increases the amount of time the installation takes to complete. STORAGE Storage classes must be specified ahead of time when installing WCA. The following storage classes are recommended. However, if you don't use these particular storage classes on future clusters you may deploy, ensure that you specify a storage class with an equivalent definition. Storage Notes Storage Classes OpenShift Data Foundation When you install the service, specify file storage and block storage. File storage: ocs-storagecluster-cephfs Block storage: ocs-storagecluster-ceph-rbd IBM Fusion Data Foundation When you install the service, specify file storage and block storage. File storage: ocs-storagecluster-cephfs Block storage: ocs-storagecluster-ceph-rbd IBM Fusion Global Data Platform When you install the service, specify the same storage class for both file storage and block storage. File storage: ibm-spectrum-scale-sc or ibm-storage-fusion-cp-sc Block storage: ibm-spectrum-scale-sc or ibm-storage-fusion-cp-sc IBM Storage Scale Container Native When you install the service, specify the same storage class for both file storage and block storage. File storage: ibm-spectrum-scale-sc Block storage: ibm-spectrum-scale-sc Portworx When you install the service, the --storage_vendor=portworx option ensures that the service uses the correct storage classes. File storage: portworx-shared-gp3 Block storage: portworx-elastic-sc NFS When you install the service, specify the same storage class for both file storage and block storage. File storage: managed-nfs-storage Block storage: managed-nfs-storage Amazon Elastic storage When you install the service, specify file storage and block storage. File storage is provided by Amazon Elastic File System. Block storage is provided by Amazon Elastic Block Store. File storage: efs-nfs-client Block storage: gp2-csi or gp3-csi NetApp Trident When you install the service, specify the same storage class for both file storage and block storage. File storage: ontap-nas Block storage: ontap-nas Nutanix Not supported. Not applicable. iii. Deployment checklist To review, the following tasks need to be completed before installing WCA on-premises. The various modules of this hands-on lab have already covered those applicable to the environment you are about to deploy. However, keep this checklist handy for future deployments you will be making with clients and in the field. Prerequisite Documentation The cluster meets the minimum requirements for installing WCA. System requirements The workstation from which you will run the installation is set up as a client workstation and includes the following command-line interfaces: IBM Software Hub CLI ( cpd-cli ) and OpenShift CLI ( oc ) Setting up a client workstation The IBM Software Hub control plane is installed. Installing an instance of IBM Software Hub For environments that use a private container registry, such as air-gapped environments, the WCA software images are mirrored to the private container registry. Mirroring images to a private container registry For environments that use a private container registry, such as air-gapped environments, the cpd-cli is configured to pull the olm-utils-v3 image from the private container registry. Pulling the olm-utils-v3 image from the private container registry The operators that are required to use GPUs are installed. Installing operators for services that require GPUs Red Hat OpenShift AI is installed. Installing Red Hat OpenShift AI iv. Fetching dependencies In the following section, participants will assume the role of an instance administrator and install WCA using the IBM Software Hub components set up in the previous module. Apply the entitlement for WCA by executing the following code block: cpd-cli manage apply-entitlement \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --entitlement = watsonx-code-assistant \\ --production = true Completion of this operation will take approximately 1 minute , after which the console will return: [ SUCCESS ] 2025 -03-19T23:02:06.847686Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T23:02:06.847741Z The apply-entitlement command ran successfully. Now that the components have been defined, install the services Operators by executing the following code block: cpd-cli manage apply-olm \\ --release = ${ VERSION } \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --components = ${ COMPONENTS } Completion of this operation will take approximately 15 minutes , after which the console will return: [ SUCCESS ] 2025 -03-19T23:16:42.690505Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T23:16:42.690774Z The apply-olm command ran successfully. v. Installing software Creating the custom resources for watsonx Code Assistant and kicking off the installation process is achieved with relatively few inputs from the administrator. Start the installation process for WCA by executing the following code block: cpd-cli manage apply-cr \\ --release = ${ VERSION } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --components = ${ COMPONENTS } \\ --block_storage_class = ${ STG_CLASS_BLOCK } \\ --file_storage_class = ${ STG_CLASS_FILE } \\ --license_acceptance = true Completion of this operation may take up to 2 hours , after which the console will return: [ SUCCESS ] ... The apply-cr command ran successfully Once the installation procedure of Step 3 has wrapped up, retrieve the details about the WCA instance by executing the following: cpd-cli manage get-cpd-instance-details \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --get_admin_initial_credentials = true The console will return multiple lines. Take note of the lines with headers CPD Url , CPD Username , and CPD Password . These have been highlighted in the code sample below. Copy the three variables from your Terminal's output to a notepad for reference later. [ INFO ] 2025 -03-20T03:20:17.481851Z Run command: podman exec -it olm-utils-play-v3 get-cpd-instance-details --cpd_instance_ns = cpd-watsonx --get_admin_initial_credentials = true CPD Url: cpd-cpd-watsonx.apps.67da04cc1b2e441b1d48a668.ocp.techzone.ibm.com CPD Username: cpadmin CPD Password: xEhwFROJ5rAL4DZ3ggTNKzlREXDYDYHU [ SUCCESS ] 2025 -03-20T03:20:18.949702Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-20T03:20:18.949759Z The get-cpd-instance-details command ran successfully. Using a web browser, navigate to the address specified by CPU Url . A page will load for the Administration Console which allows administrators to manage both their IBM Software Hub (CP4D) and IBM watsonx Code Assistant environments. Supply the CPD Username [A] and CPD Password [B] values recorded in Step 4 Click Login in [C] to proceed The web browser will load the IBM Software Hub dashboard, from which a number of administrative and monitoring options are available for you to inspect Verify that the IBM watsonx Code Assistant service was successfully installed: From the dashboard's navigation menu, drill down into Services > Services catalog [A] Search for the IBM watsonx Code Assistant service tile and confirm it is marked with a status of Enabled Click [B] on the tile to preview the service The web browser will load what essentially resembles the \"catalog\" view you might encounter on a online store portal such as IBM Cloud \u2014 this is not the administrative portal used for managing the WCA service To access the WCA management console, look to the top-right corner of the IBM Software Hub dashboard and click [A] the rectangular-dot icon. This will allow you to toggle between the Software Hub and WCA administrative portals. Click [B] the IBM watsonx Code Assistant button to switch portals The administrative portal for WCA will load within the web browser vi. Next steps Congratulations \u2014 you have successfully completed the installation of IBM watsonx Code Assistant within an on-premises environment! Explore and inspect various aspects of the on-premises environment as you wish, but keep in mind that without GPUs attached to the environment you will be significantly restricted in terms of usability. Some features may be unavailable or broken without such hardware. That is why for the Application Modernization hands-on modules, you will have full GPU access via the provided IBM Cloud environments. Those modules will delve deeply into the administrative and developer-oriented tasks that are limited for the on-premises environment. GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"7. Install watsonx Code Assistant"},{"location":"on-premises/7/#install-ibm-watsonx-code-assistanton-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation for Installing watsonx Code Assistant v5.1.x .","title":"Install IBM watsonx Code AssistantOn-Premises Installation and Deployment"},{"location":"on-premises/7/#i-user-responsibilities","text":"Following the release of Software Hub v5.x , watsonx Code Assistant (WCA) administrators need to only specify the watsonx-code-assistant component in order for the Software Hub to automatically fetch the required dependencies. To install WCA on-premises, you must be an instance administrator with permissions to install software in the following Red Hat OpenShift projects: The operators project for the instance: Operators for this instance of WCA are installed in the operators project. In the installation commands, the ${PROJECT_CPD_INST_OPERATORS} environment variable refers to the operators project. The operands project for the instance: The custom resources for the control plane and WCA are installed in the operands project. In the installation commands, the ${PROJECT_CPD_INST_OPERANDS} environment variable refers to the operands project.","title":"i. User responsibilities"},{"location":"on-premises/7/#ii-system-requirements","text":"Participants are encouraged to review the following requirements again before continuing with the install to verify that all the components are in place and to reinforce their understanding as to why such components are needed. IBM SOFTWARE HUB VERSIONING All of the components that are associated with an instance of IBM Software Hub must be installed at the same release version. For example, if the IBM Software Hub control plane is installed at Version 5.1.0 , you must install WCA at Version 5.1.0 as well. ENVIRONMENT VARIABLES The commands in this task use environment variables so that you can run the commands exactly as written. In the case of this lab, those variables are defined within the cpd_vars.sh manifest that was created in Module 3. If you want to read more about how to structure scripts like cpd_vars.sh , which define environment variables within a single source of truth, review the IBM Documentation . To use the environment variables from the script, you must source the environment variables before you run the commands in this task. For example, run: source ./cpd_vars.sh SECURITY CONTEXT CONSTRAINT WCA works with the default Red Hat OpenShift\u00ae Container Platform security context constraint, restricted-v2 . COMMON CORE SERVICES WCA requires the availability of IBM Software Hub common core services. If the common core services are not installed in the operands project for the instance, the common core services are automatically installed when you install WCA. The common core services installation increases the amount of time the installation takes to complete. STORAGE Storage classes must be specified ahead of time when installing WCA. The following storage classes are recommended. However, if you don't use these particular storage classes on future clusters you may deploy, ensure that you specify a storage class with an equivalent definition. Storage Notes Storage Classes OpenShift Data Foundation When you install the service, specify file storage and block storage. File storage: ocs-storagecluster-cephfs Block storage: ocs-storagecluster-ceph-rbd IBM Fusion Data Foundation When you install the service, specify file storage and block storage. File storage: ocs-storagecluster-cephfs Block storage: ocs-storagecluster-ceph-rbd IBM Fusion Global Data Platform When you install the service, specify the same storage class for both file storage and block storage. File storage: ibm-spectrum-scale-sc or ibm-storage-fusion-cp-sc Block storage: ibm-spectrum-scale-sc or ibm-storage-fusion-cp-sc IBM Storage Scale Container Native When you install the service, specify the same storage class for both file storage and block storage. File storage: ibm-spectrum-scale-sc Block storage: ibm-spectrum-scale-sc Portworx When you install the service, the --storage_vendor=portworx option ensures that the service uses the correct storage classes. File storage: portworx-shared-gp3 Block storage: portworx-elastic-sc NFS When you install the service, specify the same storage class for both file storage and block storage. File storage: managed-nfs-storage Block storage: managed-nfs-storage Amazon Elastic storage When you install the service, specify file storage and block storage. File storage is provided by Amazon Elastic File System. Block storage is provided by Amazon Elastic Block Store. File storage: efs-nfs-client Block storage: gp2-csi or gp3-csi NetApp Trident When you install the service, specify the same storage class for both file storage and block storage. File storage: ontap-nas Block storage: ontap-nas Nutanix Not supported. Not applicable.","title":"ii. System requirements"},{"location":"on-premises/7/#iii-deployment-checklist","text":"To review, the following tasks need to be completed before installing WCA on-premises. The various modules of this hands-on lab have already covered those applicable to the environment you are about to deploy. However, keep this checklist handy for future deployments you will be making with clients and in the field. Prerequisite Documentation The cluster meets the minimum requirements for installing WCA. System requirements The workstation from which you will run the installation is set up as a client workstation and includes the following command-line interfaces: IBM Software Hub CLI ( cpd-cli ) and OpenShift CLI ( oc ) Setting up a client workstation The IBM Software Hub control plane is installed. Installing an instance of IBM Software Hub For environments that use a private container registry, such as air-gapped environments, the WCA software images are mirrored to the private container registry. Mirroring images to a private container registry For environments that use a private container registry, such as air-gapped environments, the cpd-cli is configured to pull the olm-utils-v3 image from the private container registry. Pulling the olm-utils-v3 image from the private container registry The operators that are required to use GPUs are installed. Installing operators for services that require GPUs Red Hat OpenShift AI is installed. Installing Red Hat OpenShift AI","title":"iii. Deployment checklist"},{"location":"on-premises/7/#iv-fetching-dependencies","text":"In the following section, participants will assume the role of an instance administrator and install WCA using the IBM Software Hub components set up in the previous module. Apply the entitlement for WCA by executing the following code block: cpd-cli manage apply-entitlement \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --entitlement = watsonx-code-assistant \\ --production = true Completion of this operation will take approximately 1 minute , after which the console will return: [ SUCCESS ] 2025 -03-19T23:02:06.847686Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T23:02:06.847741Z The apply-entitlement command ran successfully. Now that the components have been defined, install the services Operators by executing the following code block: cpd-cli manage apply-olm \\ --release = ${ VERSION } \\ --cpd_operator_ns = ${ PROJECT_CPD_INST_OPERATORS } \\ --components = ${ COMPONENTS } Completion of this operation will take approximately 15 minutes , after which the console will return: [ SUCCESS ] 2025 -03-19T23:16:42.690505Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-19T23:16:42.690774Z The apply-olm command ran successfully.","title":"iv. Fetching dependencies"},{"location":"on-premises/7/#v-installing-software","text":"Creating the custom resources for watsonx Code Assistant and kicking off the installation process is achieved with relatively few inputs from the administrator. Start the installation process for WCA by executing the following code block: cpd-cli manage apply-cr \\ --release = ${ VERSION } \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --components = ${ COMPONENTS } \\ --block_storage_class = ${ STG_CLASS_BLOCK } \\ --file_storage_class = ${ STG_CLASS_FILE } \\ --license_acceptance = true Completion of this operation may take up to 2 hours , after which the console will return: [ SUCCESS ] ... The apply-cr command ran successfully Once the installation procedure of Step 3 has wrapped up, retrieve the details about the WCA instance by executing the following: cpd-cli manage get-cpd-instance-details \\ --cpd_instance_ns = ${ PROJECT_CPD_INST_OPERANDS } \\ --get_admin_initial_credentials = true The console will return multiple lines. Take note of the lines with headers CPD Url , CPD Username , and CPD Password . These have been highlighted in the code sample below. Copy the three variables from your Terminal's output to a notepad for reference later. [ INFO ] 2025 -03-20T03:20:17.481851Z Run command: podman exec -it olm-utils-play-v3 get-cpd-instance-details --cpd_instance_ns = cpd-watsonx --get_admin_initial_credentials = true CPD Url: cpd-cpd-watsonx.apps.67da04cc1b2e441b1d48a668.ocp.techzone.ibm.com CPD Username: cpadmin CPD Password: xEhwFROJ5rAL4DZ3ggTNKzlREXDYDYHU [ SUCCESS ] 2025 -03-20T03:20:18.949702Z You may find output and logs in the /home/itzuser/cpd-cli-workspace/olm-utils-workspace/work directory. [ SUCCESS ] 2025 -03-20T03:20:18.949759Z The get-cpd-instance-details command ran successfully. Using a web browser, navigate to the address specified by CPU Url . A page will load for the Administration Console which allows administrators to manage both their IBM Software Hub (CP4D) and IBM watsonx Code Assistant environments. Supply the CPD Username [A] and CPD Password [B] values recorded in Step 4 Click Login in [C] to proceed The web browser will load the IBM Software Hub dashboard, from which a number of administrative and monitoring options are available for you to inspect Verify that the IBM watsonx Code Assistant service was successfully installed: From the dashboard's navigation menu, drill down into Services > Services catalog [A] Search for the IBM watsonx Code Assistant service tile and confirm it is marked with a status of Enabled Click [B] on the tile to preview the service The web browser will load what essentially resembles the \"catalog\" view you might encounter on a online store portal such as IBM Cloud \u2014 this is not the administrative portal used for managing the WCA service To access the WCA management console, look to the top-right corner of the IBM Software Hub dashboard and click [A] the rectangular-dot icon. This will allow you to toggle between the Software Hub and WCA administrative portals. Click [B] the IBM watsonx Code Assistant button to switch portals The administrative portal for WCA will load within the web browser","title":"v. Installing software"},{"location":"on-premises/7/#vi-next-steps","text":"Congratulations \u2014 you have successfully completed the installation of IBM watsonx Code Assistant within an on-premises environment! Explore and inspect various aspects of the on-premises environment as you wish, but keep in mind that without GPUs attached to the environment you will be significantly restricted in terms of usability. Some features may be unavailable or broken without such hardware. That is why for the Application Modernization hands-on modules, you will have full GPU access via the provided IBM Cloud environments. Those modules will delve deeply into the administrative and developer-oriented tasks that are limited for the on-premises environment. GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. TROUBLESHOOTING: LOGGING IN AND SESSION TIMEOUTS Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <BASTION_PWD> placeholder with the password specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"vi. Next steps"},{"location":"saas/1/","text":"Objectives and Requirements IBM Cloud (SaaS) Configuration i. Software-as-a-Service (SaaS) plans For the purposes of the Level 4 training curriculum, the hands-on components have been split across both on-premises and Software-as-a-Service (SaaS) environments on IBM Cloud. Both are supported by IBM Technology Zone (ITZ) infrastructure. The On-Premises Installation and Deployment modules extensively cover the process of preparing, installing, and deploying a full-stack IBM watsonx Code Assistant (WCA) service \"on-premises\" (OpenShift on VMware). However, resource and budget constraints for ITZ teams means that GPUs are unavailable for on-premises training environments like the one deployed in those modules. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. These costs are beyond the scope of what can be supported for training with on-premises environments. Fortunately, participants do have access to GPUs for SaaS training environments on IBM Cloud. This module will guide you through the process of reserving the necessary Standard plan of WCA through ITZ and configuring the environment for use. Other modules in the L4 curriculum\u2014 such as Application Modernization - WebSphere to Liberty , Application Modernization - Upgrading Java , and Tuning & Customization \u2014will each leverage the SaaS environment you've configured in the following steps.","title":"1. Objectives and requirements"},{"location":"saas/1/#objectives-and-requirementsibm-cloud-saas-configuration","text":"","title":"Objectives and RequirementsIBM Cloud (SaaS) Configuration"},{"location":"saas/1/#i-software-as-a-service-saas-plans","text":"For the purposes of the Level 4 training curriculum, the hands-on components have been split across both on-premises and Software-as-a-Service (SaaS) environments on IBM Cloud. Both are supported by IBM Technology Zone (ITZ) infrastructure. The On-Premises Installation and Deployment modules extensively cover the process of preparing, installing, and deploying a full-stack IBM watsonx Code Assistant (WCA) service \"on-premises\" (OpenShift on VMware). However, resource and budget constraints for ITZ teams means that GPUs are unavailable for on-premises training environments like the one deployed in those modules. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. These costs are beyond the scope of what can be supported for training with on-premises environments. Fortunately, participants do have access to GPUs for SaaS training environments on IBM Cloud. This module will guide you through the process of reserving the necessary Standard plan of WCA through ITZ and configuring the environment for use. Other modules in the L4 curriculum\u2014 such as Application Modernization - WebSphere to Liberty , Application Modernization - Upgrading Java , and Tuning & Customization \u2014will each leverage the SaaS environment you've configured in the following steps.","title":"i. Software-as-a-Service (SaaS) plans"},{"location":"tuning/1/","text":"Getting Code Suggestions Tuning and Customization i. Section 1","title":"1. Getting code suggestions"},{"location":"tuning/1/#getting-code-suggestionstuning-and-customization","text":"","title":"Getting Code SuggestionsTuning and Customization"},{"location":"tuning/1/#i-section-1","text":"","title":"i. Section 1"},{"location":"tuning/2/","text":"Explaining Code Tuning and Customization i. Section 1","title":"2. Explaining code"},{"location":"tuning/2/#explaining-codetuning-and-customization","text":"","title":"Explaining CodeTuning and Customization"},{"location":"tuning/2/#i-section-1","text":"","title":"i. Section 1"},{"location":"tuning/3/","text":"Documenting Code Tuning and Customization i. Section 1","title":"3. Documenting code"},{"location":"tuning/3/#documenting-codetuning-and-customization","text":"","title":"Documenting CodeTuning and Customization"},{"location":"tuning/3/#i-section-1","text":"","title":"i. Section 1"},{"location":"tuning/4/","text":"Unit Test Generation Tuning and Customization i. Section 1","title":"4. Unit test generation"},{"location":"tuning/4/#unit-test-generationtuning-and-customization","text":"","title":"Unit Test GenerationTuning and Customization"},{"location":"tuning/4/#i-section-1","text":"","title":"i. Section 1"},{"location":"tuning/5/","text":"Upgrading Java Versions Tuning and Customization i. Section 1","title":"5. Upgrading Java versions"},{"location":"tuning/5/#upgrading-java-versionstuning-and-customization","text":"","title":"Upgrading Java VersionsTuning and Customization"},{"location":"tuning/5/#i-section-1","text":"","title":"i. Section 1"}]}