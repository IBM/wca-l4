{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) introduces IBM watsonx Code Assistant's generative AI capabilities and lays the groundwork for the hands-on training that will follow. [6 min] i. Automation is indispensable to modern IT strategy Despite the innovations and advancements made in the domain of automation, IBM sellers and partners know first-hand from discussions with clients that many businesses are still struggling to keep up with their IT operations. The rapid pace of technological innovation\u2014 in particular, areas such as AI and machine learning \u2014are obviously challenging for any organization to strategize and plan around. But smaller, more practical challenges also stand in the way of these businesses. The fact remains that IT operations, and wrangling those operations in an efficient and streamlined manner, remains a difficult problem to solve. Three primary pain points that IBM consistently hears from the marketplace include: an ever-increasing skills gap in IT management; that Day 2 operations continue to be labor-intensive, mostly manual endeavors; and that the complexity of the systems needing to be managed are out-pacing many organization\u2019s ability to adapt. All of these pain points are potential automation challenges to be solved. Each of them impedes a company's ability to move quickly and adapt for the future. And as such, for many IBM clients, solving these automation challenges have become an indispensable element in their strategy to modernize IT. UNPRECEDENTED RATE OF GENERATIVE AI ADOPTION Even though generative AI is relatively new, the widespread popularity of ChatGPT has created significant interest in the notion of large language models (LLMs) and foundation models (FMs) \u2014 and what they can do for business. It took quite some time for enterprises to start moving toward traditional AI. In contrast, generative AI has experienced massive early adoption: 80% of enterprises are already working with, or planning to leverage FMs, and plan to adopt generative AI in their use cases and workflow. Moreover, the following data points to an ever-growing adoption trend for generative AI: Scale Zeitgeist 2023 AI Readiness Report notes that with the companies they reviewed, 21% have generative AI models in production; 29% are experimenting with generative AI and another 31% are planning to work with generative AI models; a total of 81% are either working with or planning to work with generative AI models Goldman Sachs has estimated that generative AI will have a very deep economic impact \u2013 raising global Gross Domestic Product (GDP) by 7% within 10 years, reflecting the technology\u2019s huge potential. Boston Consulting Group (BCG) noted that generative AI is expected to represent 30% of the overall market by 2025 ii. Generative AI-assisted code lifecycle management achieves what LLMs alone cannot Following the debut of OpenAI's ChatGPT, the marketplace has been awash with competing large language model (LLM) and generative AI-based assistants. It's one thing to train and deploy an LLM; it's another thing entirely to make it applicable and tangibly beneficial for business. What separates IBM watsonx Code Assistant (WCA) offerings from competing vendors in the marketplace? The design and implementation of WCA is purposely built to assist, using generative AI, software and code lifecycle management. In short, generative AI-assisted code lifecycle management helps to achieve what large language models cannot achieve on their own. It is what distinguished WCA from other code assistants in the marketplace today. Code lifecycle management begins with understanding client code, through training across a myriad of programming languages and specializations in paradigms such as Ansible Automation Platform, and applies that understanding across a client\u2019s application and runtime environments. Users are able to plan next steps based on generative AI analysis of their existing application code. Operations teams can rapidly transform their codebases with optimized design and architecture that is recommended according to IBM Granite's best-practice models. Administrators can validate the outcomes with automatically generated unit tests. Afterwards, they can deploy those services and applications using automated processes like Ansible's automation engine. Over the course of that application or code's lifecycle, generative AI can maintain healthy operations with runtime insights. iii. Introducing the IBM watsonx Code Assistant product family IBM watsonx Code Assistant is the flagship offering in a suite of generative AI code assistant products, which also include offerings for Ansible Automation Platform (IBM watsonx Code Assistant for Red Hat Ansible Lightspeed) and IBM Systems modernization (IBM watsonx Code Assistant for Z). These solutions accelerate software development tasks with AI-powered capabilities including context-aware code generation, explanation, documentation, translation, and unit test generation. It does so while maintaining the principles of trust, security, and compliance with regards to IBM client's data and intellectual property. Developers and IT Operators can utilize WCA to speed up application modernization efforts and generate Ansible-based automation jobs to rapidly scale out (or scale up) IT environments. IBM watsonx Code Assistant products are powered by IBM Granite foundation models that include state-of-the-art large language models designed for code. For offerings such as WCA for Ansible Lightspeed and WCA for Z, bespoke code models\u2014 tailored to working with Ansible Automation Platform and COBOL-to-Z use cases, respectively \u2014are invoked. Universally true for all of the watsonx Code Assistant offerings is that they are geared towards helping IT teams create high-quality code using AI-generated recommendations, based on natural language requests or existing source code. These AI models, and the recommendations they generate, are seamlessly integrated via extensions with the world's most popular developer integrated development environments (IDE), including Visual Studio Code and Eclipse. Granite is IBM\u2019s flagship brand of open and proprietary LLMs, spanning multiple modalities. Granite models exist for code, languages, time series, and GeoSpatial \u2014 with additional modalities expected in future. IBM Granite code models are a series of decoder-only models for code generative tasks, trained with code written in 116 different programming languages. The Granite code models family consists of models ranging in size from 3 to 34 billion parameters, in both a base model and instruction-following model variants. These models have a range of uses, from complex application modernization tasks to on-device memory-constrained use cases. The larger the block size for a particular language on this chart, the larger percentage of training corpus data of that language was used to train the Granite code model. Languages and formats such as Java, C, JSON, JavaScript, HTML, and PHP are subjects in which the model \u201cMajors\u201d and excels. Other languages such as Ruby, SQL, and Swift could be considered \u201cMinors\u201d where the generalized code model can work with the language, but has less training data to base those recommendations on. These percentages and training data volumes will continue to evolve as the Granite code models mature. WATSONX CODE ASSISTANT vs. WCA FOR ANSIBLE LIGHTSPEED? For those familiar with other IBM watsonx Code Assistant offerings\u2014 such as WCA for Red Hat Ansible Lightspeed and WCA for Z \u2014the generalized code model approach, as seen here, differs from the specialized code model approach of those two aforementioned offerings. The WCA for Ansible Lightspeed flavor of IBM Granite code models specializes (Majors) only in Ansible Playbooks and YAML Similarly, the IBM Granite code model used by WCA for Z specializes in transforming COBOL mainframe code into modernized Java code Ansible Playbooks (YAML) and mainframe (COBOL) code are both supported (Minor) languages for the generalized IBM Granite code models\u2014 and therefore are supported by IBM watsonx Code Assistant \u2014but if a client wishes to specialize in those particular languages and frameworks, they would be well advised to utilize the bespoke WCA for Ansible Lightspeed and WCA for Z offerings, respectively, to do so. iv. Solution architecture of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed IBM watsonx Code Assistant for Red Hat Ansible Lightspeed meets developers where they are: with a rich plugin via VS Code extensions, where developers input their prompts directly in the code editor. Prompts are sent to the Ansible Lightspeed service, and the service sends a suggestion back (a completion ) that\u2019s powered by IBM Granite LLMs for code. It is important to note that all data in transit is encrypted and ephemeral so users can be confident and have trust in the security of the service during this exchange. In terms of data security, client Ansible playbooks and customized models that they may potentially have are stored in client-owned Cloud Object Storage and are not shared with IBM, Red Hat, or any other clients. In order to utilize IBM watsonx Code Assistant for Red Hat Ansible Lightspeed, a client must have an existing license for Red Hat Ansible Automation Platform (the \u201dred tile\u201d component in the center of the diagram), as well as a license for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed (the \u201cblue tile\u201d on the right of the diagram). Generative AI has recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. For example, if trained on a large dataset of Ansible Playbooks, generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style or standards of that particular company. WHAT ARE PLAYBOOKS? Ansible Playbooks instruct Ansible\u2019s automation engine on how to execute tasks in a step-by-step manner. Playbooks defines roles, tasks, handlers, and other configurations; in turn, these attributes allow developers and users to codify complex orchestration scenarios. Conceptually, think of a Playbook as a recipe book for system administration: each recipe (or Playbook) spells out the steps required to achieve a particular system state or to complete a given operation. One of the standout features of Ansible Playbooks is that they are idempotent : executing Playbooks multiple times on the same system won't create additional \"side effects\" (unintended operations or creation of unwanted artifacts) after the first successful run. This ensures consistency and reliability across deployments of the Red Hat Ansible Automation Platform ( AAP ). As you will see throughout the hands-on training material, generative AI models provide a natural language prompt to users which in turn is understood and translated by the AI models into the necessary Ansible Task code. For example, a user might describe a desired system state in plain language ( \"I want a Playbook to install and start an Apache web server\" ) and the model will generate the appropriate Ansible Tasks for a Playbook. All of this is achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within a company with limited Ansible or programming expertise will be able to produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing of AI-generated code will be needed before being put into production. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality. v. Lab objectives The material covered for this hands-on training is intended to prepare IBM sellers and business partners with the skills necessary to create Ansible automation tasks using the generative AI capabilities of WCA. The curriculum will leverage WCA's generative AI code recommendations for automating cloud-based and infrastructure-based automation tasks. In-depth explanations accompanying Ansible Playbook templates will also explain: How WCA uses natural language prompts , as well as Ansible Playbook contents, to generate contextually-aware Task code recommendations Post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How WCA provides content source matching attribution and \"explainability\" for all AI-generated content Leveraging WCA's model tuning capabilities to tailor content and code recommendations to an organization's standards, best practices, and programming styles vi. Next steps The module ahead will outline the evaluation criteria for IBM sellers and business partners. Afterwards, you will setup your local environment with the necessary pre-requisites for getting started with the hands-on material.","title":"Introduction"},{"location":"#_1","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) introduces IBM watsonx Code Assistant's generative AI capabilities and lays the groundwork for the hands-on training that will follow. [6 min]","title":""},{"location":"#i-automation-is-indispensable-to-modern-it-strategy","text":"Despite the innovations and advancements made in the domain of automation, IBM sellers and partners know first-hand from discussions with clients that many businesses are still struggling to keep up with their IT operations. The rapid pace of technological innovation\u2014 in particular, areas such as AI and machine learning \u2014are obviously challenging for any organization to strategize and plan around. But smaller, more practical challenges also stand in the way of these businesses. The fact remains that IT operations, and wrangling those operations in an efficient and streamlined manner, remains a difficult problem to solve. Three primary pain points that IBM consistently hears from the marketplace include: an ever-increasing skills gap in IT management; that Day 2 operations continue to be labor-intensive, mostly manual endeavors; and that the complexity of the systems needing to be managed are out-pacing many organization\u2019s ability to adapt. All of these pain points are potential automation challenges to be solved. Each of them impedes a company's ability to move quickly and adapt for the future. And as such, for many IBM clients, solving these automation challenges have become an indispensable element in their strategy to modernize IT. UNPRECEDENTED RATE OF GENERATIVE AI ADOPTION Even though generative AI is relatively new, the widespread popularity of ChatGPT has created significant interest in the notion of large language models (LLMs) and foundation models (FMs) \u2014 and what they can do for business. It took quite some time for enterprises to start moving toward traditional AI. In contrast, generative AI has experienced massive early adoption: 80% of enterprises are already working with, or planning to leverage FMs, and plan to adopt generative AI in their use cases and workflow. Moreover, the following data points to an ever-growing adoption trend for generative AI: Scale Zeitgeist 2023 AI Readiness Report notes that with the companies they reviewed, 21% have generative AI models in production; 29% are experimenting with generative AI and another 31% are planning to work with generative AI models; a total of 81% are either working with or planning to work with generative AI models Goldman Sachs has estimated that generative AI will have a very deep economic impact \u2013 raising global Gross Domestic Product (GDP) by 7% within 10 years, reflecting the technology\u2019s huge potential. Boston Consulting Group (BCG) noted that generative AI is expected to represent 30% of the overall market by 2025","title":"i. Automation is indispensable to modern IT strategy"},{"location":"#ii-generative-ai-assisted-code-lifecycle-management-achieves-what-llms-alone-cannot","text":"Following the debut of OpenAI's ChatGPT, the marketplace has been awash with competing large language model (LLM) and generative AI-based assistants. It's one thing to train and deploy an LLM; it's another thing entirely to make it applicable and tangibly beneficial for business. What separates IBM watsonx Code Assistant (WCA) offerings from competing vendors in the marketplace? The design and implementation of WCA is purposely built to assist, using generative AI, software and code lifecycle management. In short, generative AI-assisted code lifecycle management helps to achieve what large language models cannot achieve on their own. It is what distinguished WCA from other code assistants in the marketplace today. Code lifecycle management begins with understanding client code, through training across a myriad of programming languages and specializations in paradigms such as Ansible Automation Platform, and applies that understanding across a client\u2019s application and runtime environments. Users are able to plan next steps based on generative AI analysis of their existing application code. Operations teams can rapidly transform their codebases with optimized design and architecture that is recommended according to IBM Granite's best-practice models. Administrators can validate the outcomes with automatically generated unit tests. Afterwards, they can deploy those services and applications using automated processes like Ansible's automation engine. Over the course of that application or code's lifecycle, generative AI can maintain healthy operations with runtime insights.","title":"ii. Generative AI-assisted code lifecycle management achieves what LLMs alone cannot"},{"location":"#iii-introducing-the-ibm-watsonx-code-assistant-product-family","text":"IBM watsonx Code Assistant is the flagship offering in a suite of generative AI code assistant products, which also include offerings for Ansible Automation Platform (IBM watsonx Code Assistant for Red Hat Ansible Lightspeed) and IBM Systems modernization (IBM watsonx Code Assistant for Z). These solutions accelerate software development tasks with AI-powered capabilities including context-aware code generation, explanation, documentation, translation, and unit test generation. It does so while maintaining the principles of trust, security, and compliance with regards to IBM client's data and intellectual property. Developers and IT Operators can utilize WCA to speed up application modernization efforts and generate Ansible-based automation jobs to rapidly scale out (or scale up) IT environments. IBM watsonx Code Assistant products are powered by IBM Granite foundation models that include state-of-the-art large language models designed for code. For offerings such as WCA for Ansible Lightspeed and WCA for Z, bespoke code models\u2014 tailored to working with Ansible Automation Platform and COBOL-to-Z use cases, respectively \u2014are invoked. Universally true for all of the watsonx Code Assistant offerings is that they are geared towards helping IT teams create high-quality code using AI-generated recommendations, based on natural language requests or existing source code. These AI models, and the recommendations they generate, are seamlessly integrated via extensions with the world's most popular developer integrated development environments (IDE), including Visual Studio Code and Eclipse. Granite is IBM\u2019s flagship brand of open and proprietary LLMs, spanning multiple modalities. Granite models exist for code, languages, time series, and GeoSpatial \u2014 with additional modalities expected in future. IBM Granite code models are a series of decoder-only models for code generative tasks, trained with code written in 116 different programming languages. The Granite code models family consists of models ranging in size from 3 to 34 billion parameters, in both a base model and instruction-following model variants. These models have a range of uses, from complex application modernization tasks to on-device memory-constrained use cases. The larger the block size for a particular language on this chart, the larger percentage of training corpus data of that language was used to train the Granite code model. Languages and formats such as Java, C, JSON, JavaScript, HTML, and PHP are subjects in which the model \u201cMajors\u201d and excels. Other languages such as Ruby, SQL, and Swift could be considered \u201cMinors\u201d where the generalized code model can work with the language, but has less training data to base those recommendations on. These percentages and training data volumes will continue to evolve as the Granite code models mature. WATSONX CODE ASSISTANT vs. WCA FOR ANSIBLE LIGHTSPEED? For those familiar with other IBM watsonx Code Assistant offerings\u2014 such as WCA for Red Hat Ansible Lightspeed and WCA for Z \u2014the generalized code model approach, as seen here, differs from the specialized code model approach of those two aforementioned offerings. The WCA for Ansible Lightspeed flavor of IBM Granite code models specializes (Majors) only in Ansible Playbooks and YAML Similarly, the IBM Granite code model used by WCA for Z specializes in transforming COBOL mainframe code into modernized Java code Ansible Playbooks (YAML) and mainframe (COBOL) code are both supported (Minor) languages for the generalized IBM Granite code models\u2014 and therefore are supported by IBM watsonx Code Assistant \u2014but if a client wishes to specialize in those particular languages and frameworks, they would be well advised to utilize the bespoke WCA for Ansible Lightspeed and WCA for Z offerings, respectively, to do so.","title":"iii. Introducing the IBM watsonx Code Assistant product family"},{"location":"#iv-solution-architecture-of-ibm-watsonx-code-assistant-for-red-hat-ansible-lightspeed","text":"IBM watsonx Code Assistant for Red Hat Ansible Lightspeed meets developers where they are: with a rich plugin via VS Code extensions, where developers input their prompts directly in the code editor. Prompts are sent to the Ansible Lightspeed service, and the service sends a suggestion back (a completion ) that\u2019s powered by IBM Granite LLMs for code. It is important to note that all data in transit is encrypted and ephemeral so users can be confident and have trust in the security of the service during this exchange. In terms of data security, client Ansible playbooks and customized models that they may potentially have are stored in client-owned Cloud Object Storage and are not shared with IBM, Red Hat, or any other clients. In order to utilize IBM watsonx Code Assistant for Red Hat Ansible Lightspeed, a client must have an existing license for Red Hat Ansible Automation Platform (the \u201dred tile\u201d component in the center of the diagram), as well as a license for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed (the \u201cblue tile\u201d on the right of the diagram). Generative AI has recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. For example, if trained on a large dataset of Ansible Playbooks, generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style or standards of that particular company. WHAT ARE PLAYBOOKS? Ansible Playbooks instruct Ansible\u2019s automation engine on how to execute tasks in a step-by-step manner. Playbooks defines roles, tasks, handlers, and other configurations; in turn, these attributes allow developers and users to codify complex orchestration scenarios. Conceptually, think of a Playbook as a recipe book for system administration: each recipe (or Playbook) spells out the steps required to achieve a particular system state or to complete a given operation. One of the standout features of Ansible Playbooks is that they are idempotent : executing Playbooks multiple times on the same system won't create additional \"side effects\" (unintended operations or creation of unwanted artifacts) after the first successful run. This ensures consistency and reliability across deployments of the Red Hat Ansible Automation Platform ( AAP ). As you will see throughout the hands-on training material, generative AI models provide a natural language prompt to users which in turn is understood and translated by the AI models into the necessary Ansible Task code. For example, a user might describe a desired system state in plain language ( \"I want a Playbook to install and start an Apache web server\" ) and the model will generate the appropriate Ansible Tasks for a Playbook. All of this is achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within a company with limited Ansible or programming expertise will be able to produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing of AI-generated code will be needed before being put into production. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality.","title":"iv. Solution architecture of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed"},{"location":"#v-lab-objectives","text":"The material covered for this hands-on training is intended to prepare IBM sellers and business partners with the skills necessary to create Ansible automation tasks using the generative AI capabilities of WCA. The curriculum will leverage WCA's generative AI code recommendations for automating cloud-based and infrastructure-based automation tasks. In-depth explanations accompanying Ansible Playbook templates will also explain: How WCA uses natural language prompts , as well as Ansible Playbook contents, to generate contextually-aware Task code recommendations Post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How WCA provides content source matching attribution and \"explainability\" for all AI-generated content Leveraging WCA's model tuning capabilities to tailor content and code recommendations to an organization's standards, best practices, and programming styles","title":"v. Lab objectives"},{"location":"#vi-next-steps","text":"The module ahead will outline the evaluation criteria for IBM sellers and business partners. Afterwards, you will setup your local environment with the necessary pre-requisites for getting started with the hands-on material.","title":"vi. Next steps"},{"location":"generating/","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min] i. Generating Code with IBM watsonx Code Assistant for Red Hat Ansible Lightspeed An Ansible Task is a statement in Ansible's automation script (the YAML-based Playbooks you will be working with) that declares a single action to be executed. This might be installing a package, copying a file, or shutting down a service on a remote machine. Each Task represents an idempotent operation (an action that can be repeated multiple times and deliver the same result every time) that aligns the remote managed node to the specified state. Idempotent operations also ensure consistency across multiple executions, guaranteeing the same steps are taken on each execution of the task. After you have learned the fundamentals of generating Ansible Task code blocks using IBM watsonx Code Assistant for Red Hat Ansible Lightspeed ( WCA ), you'll be ready to shape and tailor the AI-generated code recommendations using WCA's model tuning capabilities. ii. Single task Ansible operations The process of creating AI-generated code recommendations is as simple as modifying the natural language (plain English) Task descriptions of an action that is to be executed, which always start with - name: and are followed by some description of the task to be performed. Ansible Tasks are often preceded with the prefix # , indicating developer comments or documentation. After the natural language description of the automation Task has been set by the user, WCA handles the rest. WCA is also capable of generating multiple Ansible Tasks from more complex natural language descriptions\u2014 what is referred to as multi-task code generation \u2014which you will experiment with later in this module. However, to get started, let's begin with the basics of generating code for single task use cases. Begin by opening the install_cockpit_single-task.yml Playbook from the list of assets in the Explorer browser. Click the Explorer tab from the left-hand interface [A] Drill down into the Install and configure Cockpit using Ansible subdirectory [B] Double-click the install_cockpit_single-task.yml Playbook A replica of the Playbook code is also included below in the documentation The red highlighting within the editor reminds users that the tasks: section contains no valid -name: task definitions. This is part of WCA's code validation process which runs automatically and alerts users to syntax errors in their code. You can safely ignore these warnings for now, as you will be un-commenting and generating valid -name: task definitions in the following steps. ~/Documents/ansible-wca-demo-kit/install and configure Cockpit using Ansible/install_cockpit_single-task.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 --- - name : Install and configure Cockpit hosts : rhel become : true # module_defaults: # ansible.builtin.service: # enabled: true # state: started tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - The suggestion included Ansible best practices by using Fully Qualified Collection name. # - name: Install cockpit package # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used \"ansible.builtin.template\" module based on the \".j2\" file extension. # # Note - The suggestion set the file permissions (\"0644\"), owner, and group based on Ansible best practices. # - name: Copy cockpit.conf.j2 to /etc/cockpit # TASK 3 # # 3a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used the generic \"Start and enable service\" prompt # # and full Playbook context to infer the recommendation should start the \"cockpit\" service. # # 3b. Uncomment the \"module_defaults\" section at the top of the Playbook. # # 3c. Clear current task suggestion and request updated suggestion. # # Note - Ansible Lightspeed used the full Playbook context and evaluated the \"module_defaults\" # # when generating a suggestion. # # The updated suggestion no longer includes \"enabled:\" and \"state:\" arguments. # - name: Start and enable service # TASK 4 # # 4a. Uncomment task description below and generate an Ansible Playbook task suggestion. # # 4b. Save the Playbook. # - name: Wait 15 seconds port 9090 The install_cockpit_single-task.yml Playbook code above warrants some explanation before we move on with making AI-generated modifications to it: Line 2 essentially marks the beginning of the Playbook instructions, the purpose of which is to automate the process of installing and configuring Cockpit for Red Hat Ansible. Lines 3-4 define variables that will remain static throughout the remainder of the Playbook. These variables will be referenced by the AI-generated code suggestions at a later stage. This is a key capability of the offering and one which you will explore in much finer details later on in this module. Lines 6-9 are variables which have been commented out and therefore are invisible to the execution of the Ansible script and not examined by WCA for context when generating code recommendations. You will experiment with how removing the # comment blocks impacts the recommendations of task block code. \"Uncommenting\" these lines of code will make them viable for execution and these lines will afterwards be considered as valid Playbook \"context\" for AI code generation. Locate TASK 1 on Line 15 of the YAML file, which handles installation of Cockpit for Ansible. Cockpit is an interactive server administration interface that provides a graphical overview of statistics and configurations for a system or systems within a network. # - name: Install cockpit package Pay attention to the indentation and characters used on Line 15 , which in sequence from left to right are as follows: begins with Tab (or Space whitespaces) for indentation a # character to \"comment out\" the line's contents a whitespace Space character - name: which signifies the start of a Task definition and finally the natural language description of the Task INDENTATION LEVELS AND WHITESPACE Similar to Python, Ansible and YAML-based Playbooks are very sensitive to whitespacing and indentation. Indentations (such as the Tab in this example) denote different hierarchies and code nesting levels within the YAML structure. You may use Space instead of Tab if you prefer, but be sure to use indentations consistently : choose to use either Tab or Space for indenting lines of code, and do not interchange between the two. To generate code for TASK 1 , first uncomment the line of code (remove the # character from the start of a line). Highlight the line(s) of code you wish to uncomment and then press Cmd + ? for macOS or Ctrl + ? for Windows You can repeat those keystrokes with the line(s) selected to toggle between commenting or uncommenting lines of code Tip: commented out lines of code in VS Code will appear as green text Afterwards, Line 15 should look like the following \u2014 beginning with a single Tab - name : Install cockpit package Now you are ready to begin generating code. Place your cursor on Line 15 and hit Enter Wait for WCA to engage and generate the suggested (in grey, italicized text ) code block for executing the task This temporary code suggestion is entirely generated by AI As a user, you have the option to either: Accept the code recommendation as-given by pressing Tab Modify the recommended code by highlighting and replacing the italicized text FAILED TO CONNECT TO THE SERVER / \"YOU DON'T HAVE ACCESS TO IBM WATSONX...\" This warning will occur when the Ansible plugin for VS Code needs to be re-authenticated with WCA. It can occur after an extended period of inactivity or a system restart. For example, if your lab environment is running inside a VM, pausing or restarting the VM may produce this error. To re-authenticate: Sign out from the VS Code application by clicking the User icon [A] in the bottom-left corner of the interface, hover over your username, and then click Sign Out [B] If you are running this environment inside a virtual machine (VM) , closing and restarting the VM will not resolve the issue \u2014 you must sign out from the VS Code application, not the VM Once logged out, follow from Step 7 of the Setup & Troubleshooting to re-authenticate with WCA CODE RECOMMENDATIONS ARE NOT GENERATING Ansible Lightspeed and WCA will only generate code recommendations for Ansible Playbooks and YAML files. VS Code will typically auto-detect the programming language of the document you're working with, but on occassion you may need to manually specify the language. Even if working with a YAML file, you'll still need to specify the language mode as Ansible for the Lightspeed plugin to engage. To set the language mode correctly: In the bottom-right corner of the VS Code interface, hover over the Select Language Mode toggle [A] A console will appear at the top of VS Code with a drop-down list of options [B] Click Ansible from the suggested languages, or enter the text yourself and hit Enter Confirm that the Select Language Mode toggle in the bottom-right corner displays Ansible \"ANSIBLE-LINT IS NOT AVAILABLE.\" ansible-lint checks Playbooks for practices and behavior that could potentially be improved and can fix some of the most common ones for you. It will constantly check your Ansible syntax as you type and provide recommendations for how to improve it. You can safely ignore this error if it occurs during the lab exercises If you wish to install ansible-lint on your local machine, execute the following instruction within a Terminal console: python3 -m pip install --upgrade --user ansible-lint Hit Tab to accept the suggested code and then compare with the SOLUTION tab below. TEMPLATE SOLUTION # TASK 1 - name : Install cockpit package # TASK 1 - name : Install cockpit package ansible.builtin.package : name : cockpit state : present As part of the plain-text description of the Task, WCA was asked to include the cockpit Role, part of the Red Hat Enterprise Linux System Roles Certified Content Collection. The AI-generated code suggestion invoked a Fully Qualified Collection Name ( FQCN ) - ansible.builtin.package Making use of FQCNs where possible is a recommended best practice and is a prime example of the many ways in which the offering infuses post-processing capabilities within the AI-generated code produced by WCA. Additional examples of infusing best-practices into AI-generated code recommendations can be found in TASK 2 ( Line 21 of the unmodified template or Line 25 after Step 5 ): Uncomment - name: Copy cockpit.conf.j2 to /etc/cockpit Hit Enter to generate the task code recommendation and accept the AI-suggested code (without modifications) by pressing Tab Compare your results with the SOLUTION tab below TEMPLATE SOLUTION # TASK 2 - name : Copy cockpit.conf.j2 to /etc/cockpit # TASK 2 - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' The AI-generated code recommendation will copy cockpit.conf to the target host. Take note of the fact that the recommendation included the mode: argument and set the Linux file permissions to 0644 , neither of which were things explicitly requested in the Task -name description, but are both additions which adhere to best practices around defining Ansible automaton tasks. Setting a file permission to 0644 specifies read and write permissions for User and Group levels within the Linux OS, and provides only read permissions to all others. iii. Multi-task Ansible operations Up to this point, we've kept a narrow aperture on AI-generated recommendations for single tasks \u2014 examining and experimenting with generating Ansible code task by task, one at a time. However, a powerful WCA feature is the ability to combine multiple task descriptions into a single natural language prompt; in turn, WCA is able to parse that instruction, decompose the instruction into discrete Ansible Task parts, and return a complete code recommendation for achieving the author's intended goal. Syntactically, multiple tasks are combined into a single natural language expression through the use of ampersand ( & ) characters. Simply write out all the automation task descriptions on a single line, separating each description with a & character. The line must also begin with a # character for reasons that will be explained shortly. To illustrate, let's look at a multi-task Ansible Playbook: install_cockpit_multi-task.yml The contents of this Playbook should look familiar to you already: it is essentially the same Playbook examined in Steps 1-6 ( install_cockpit_single-task.yml ), re-written in an equivalent multi-task expression Each of the Task descriptions from the previous Playbook have been consolidated into a single description on Line 12 , separated by & characters ~/Documents/ansible-wca-demo-kit/install and configure Cockpit using Ansible/install_cockpit_multi-task.yml 1 2 3 4 5 6 7 8 9 10 11 12 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : # Install cockpit package & Copy cockpit.conf.j2 to /etc/cockpit & Start and enable service & Wait 15 seconds port 9090 There are two crucial distinctions between single task and multi-task code generation: formatting and execution. Formatting : Notice that Line 12 does not begin with -name: , as was the case with single task descriptions Execution : In order to generate AI code recommendations for multi-task descriptions, Line 12 must stay commented out (the # must remain at the start of the line) What is the rationale behind this? When WCA's generative AI capabilities parse Line 12 , its output will include multiple -name: tasks, each containing potentially multiple lines of instructions, based on how many & -delineated task descriptions are included on the line. Therefore, the way in which the code generation step is executed on Line 12 is a consequence of the formatting decision. Execution of a code generation step on a commented-out ( # ) line containing & delineators is recognized by WCA as a unique case that will be acted upon as a multi-task statement. Place your cursor at the end of Line 12 , and without removing the # character, press Enter to execute the code generation step. Be aware that generating code for multi-task descriptions will take longer compared to a single task. Compare the MULTI-TASK solution tab with the SINGLE TASK solution (copied over from Step 6 ). How did the multi-task code generation fare compared to the single task approach? MULTI-TASK SINGLE TASK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : - name : Install cockpit package ansible.builtin.package : name : cockpit state : present - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' - name : Start and enable service ansible.builtin.service : name : cockpit.socket - name : Wait 15 seconds port 9090 ansible.builtin.wait_for : port : 9090 delay : 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : - name : Install cockpit package ansible.builtin.package : name : cockpit state : present - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' - name : Start and enable service ansible.builtin.service : name : cockpit state : started enabled : true - name : Wait 15 seconds port 9090 ansible.builtin.wait_for : port : 9090 delay : 15 Comparing the two results, the only notable difference between the two approaches are Lines 27-29 from the SINGLE TASK generative AI approach. Both the SINGLE TASK and MULTI-TASK suggestions for that particular task satisfy the request made by the user. However, whether the single task or multi-task approach resulted in a better code suggestion is up to the judgement of the programmer. Nearly 90% of the remaining code was identical between the two approaches and was achieved in far fewer lines of code (and less typing) using the multi-task approach. The variability of generative AI suggestions is a fascinating topic and one that we will dive more deeply into with the module ahead. Before moving on to other product features, experiment by creating a new Ansible Playbook in your workspace using the code template below. Suggestions will be given on how to perform the same automation task using single and multi-task generation approaches. Save the YAML file as create_ec2_single_multi.yml (if you forget to save the file, WCA will not generate recommendations) Copy the following code block to your clipboard using the + icon in the top-right corner of the panel and paste into the newly created YAML file HOW TO CREATE NEW YAML PLAYBOOKS Note: You need to copy and paste the contents of the Playbook into a New File... within the same Lightspeed project directory that was used for the previous lab modules in order for the VS Code extension to engage. To create a new YAML Playbook within a VS Code environment: a. Copy the contents of the Playbook to clipboard using the button in the top-right corner of the lab guide code block. b. Return to your VS Code environment. In the top-left corner of the interface, with your Ansible Lightspeed folder selected, click the New File... button. c. Name the file to a description of your choosing, ending with .yml as the filetype. Set it to CustomPlaybook.yml , for example. Save it to one of the directories in the ansible-wca-demo-kit folder. d. Paste the clipboard contents into the YAML file and follow along with the suggestions below. COPY AND PASTE CODE WITHIN THE VM Information \"copied\" to your local machine's clipboard cannot be \"pasted\" directly into the virtual machine (VM) environment or VS Code. If you wish to copy and paste instructions directly from the lab documentation, it is recommended that you open the GitHub instructions inside the VM's web browser (Firefox). This will allow you to copy instructions to the VM's clipboard and paste instructions inside the VS Code editor. ANSIBLE LIGHTSPEED IS MISSING OR CODE RECOMMENDATIONS ARE NOT GENERATING Ansible Lightspeed and WCA will only generate code recommendations for Ansible Playbooks and YAML files. VS Code will typically auto-detect the programming language of the document you're working with, but on occassion you may need to manually specify the language. Even if working with a YAML file, you'll still need to specify the language mode as Ansible for the Lightspeed plugin to engage. To set the language mode correctly: In the bottom-right corner of the VS Code interface, hover over the Select Language Mode toggle [A] A console will appear at the top of VS Code with a drop-down list of options [B] Click Ansible from the suggested languages, or enter the text yourself and hit Enter Confirm that the Select Language Mode toggle in the bottom-right corner displays Ansible ~/Documents/ansible-wca-demo-kit/create_ec2_single_multi.yml 1 2 3 4 --- - name : Provision an EC2 instance hosts : all tasks : First, configure the Playbook for single task generation by adding the following code snippet into Line 5 (remembering to properly indent with Tab or Space characters): - name : create vpc named demo Your workspace Playbook should look identical to the TEMPLATE tab below. Place your cursor at the end of the newly-created Line 5 and hit Enter to execute single task code generation. Compare your results to the SOLUTION tab. Record your results to a notepad so that you can compare the results later. TEMPLATE SOLUTION 1 2 3 4 5 --- - name : Provision an EC2 instance hosts : all tasks : - name : create vpc named demo 1 2 3 4 5 6 7 8 9 10 11 12 --- - name : Provision an EC2 instance hosts : all tasks : - name : create vpc named demo amazon.aws.ec2_vpc_net : name : demo cidr_block : 10.0.0.80/16 tags : Name : demo tenancy : default register : vpc Now it's time to re-write the Playbook for multi-task code generation. The beauty of natural language statements is that your approach can be as terse or verbose as you want. The more verbose and descriptive, the more prescriptive you can be in terms of influencing the AI-generated code recommendations from WCA. The topic of \"prompt tuning\" will be explored in much greater detail in the Task Description Tuning and Model Customization module. But for now: Replace all of the code inside create_ec2_single_multi.yml with the TEMPLATE code block (below) Place your cursor at the end of Line 5 (remember not to remove the # character) and hit Enter , then accept the recommendation with Tab Compare your results to the SOLUTION tab and to the result from Step 13 TEMPLATE SOLUTION 1 2 3 4 5 --- - name : Provision an EC2 instance hosts : all tasks : # create vpc named demo & create security group for demo vpc and allow ssh and http & create internet gateway named demogateway for demo vpc & create subnet named demosubnet in demo vpc & create route table for subnet demosubnet and gateway demogateway & create t2.small ec2 instance in subnet demosubnet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 --- - name : Provision an EC2 instance hosts : all tasks : # create vpc named demo & create security group for demo vpc and allow ssh and http & create internet gateway named demogateway for demo vpc & create subnet named demosubnet in demo vpc & create route table for subnet demosubnet and gateway demogateway & create t2.small ec2 instance in subnet demosubnet - name : Create vpc named demo amazon.aws.ec2_vpc_net : name : demo cidr_block : 10.0.0.80/16 tags : Name : demo tenancy : default register : vpc - name : Create security group for demo vpc and allow ssh and http amazon.aws.ec2_security_group : name : demo description : demo security group vpc_id : \"{ vpc.vpc.id }\" rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 register : sg - name : Create internet gateway named demogateway for demo vpc amazon.aws.ec2_vpc_igw : vpc_id : \"{ vpc.vpc.id }\" tags : Name : demogateway register : igw - name : Create subnet named demosubnet in demo vpc amazon.aws.ec2_vpc_subnet : cidr : 10.0.0.80/24 vpc_id : \"{ vpc.vpc.id }\" tags : Name : demosubnet register : subnet - name : Create route table for subnet demosubnet and gateway demogateway amazon.aws.ec2_vpc_route_table : vpc_id : \"{ vpc.vpc.id }\" tags : Name : demosubnet subnets : - \"{ subnet.subnet.id }\" routes : - dest : 0.0.0.0/0 gateway_id : \"{ igw.gateway_id }\" register : route_table - name : Create t2.small ec2 instance in subnet demosubnet amazon.aws.ec2_instance : key_name : \"{ _key_name_ }\" instance_type : t2.small image : \"{ _image_ }\" wait : true vpc_subnet_id : \"{ subnet.subnet.id }\" security_group : demo register : ec2 TIMEOUT WARNING It may take several moments for WCA to process and return code recommendations for a multi-task description as complex as this one. If you receive a time-out warning, try executing the code generation step by pressing Enter a second time. iv. Next steps In the next section, you will examine in detail WCA's post-processing and content source attribution capabilities.","title":"Generating"},{"location":"generating/#_1","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min]","title":""},{"location":"generating/#i-generating-code-with-ibm-watsonx-code-assistant-for-red-hat-ansible-lightspeed","text":"An Ansible Task is a statement in Ansible's automation script (the YAML-based Playbooks you will be working with) that declares a single action to be executed. This might be installing a package, copying a file, or shutting down a service on a remote machine. Each Task represents an idempotent operation (an action that can be repeated multiple times and deliver the same result every time) that aligns the remote managed node to the specified state. Idempotent operations also ensure consistency across multiple executions, guaranteeing the same steps are taken on each execution of the task. After you have learned the fundamentals of generating Ansible Task code blocks using IBM watsonx Code Assistant for Red Hat Ansible Lightspeed ( WCA ), you'll be ready to shape and tailor the AI-generated code recommendations using WCA's model tuning capabilities.","title":"i. Generating Code with IBM watsonx Code Assistant for Red Hat Ansible Lightspeed"},{"location":"generating/#ii-single-task-ansible-operations","text":"The process of creating AI-generated code recommendations is as simple as modifying the natural language (plain English) Task descriptions of an action that is to be executed, which always start with - name: and are followed by some description of the task to be performed. Ansible Tasks are often preceded with the prefix # , indicating developer comments or documentation. After the natural language description of the automation Task has been set by the user, WCA handles the rest. WCA is also capable of generating multiple Ansible Tasks from more complex natural language descriptions\u2014 what is referred to as multi-task code generation \u2014which you will experiment with later in this module. However, to get started, let's begin with the basics of generating code for single task use cases. Begin by opening the install_cockpit_single-task.yml Playbook from the list of assets in the Explorer browser. Click the Explorer tab from the left-hand interface [A] Drill down into the Install and configure Cockpit using Ansible subdirectory [B] Double-click the install_cockpit_single-task.yml Playbook A replica of the Playbook code is also included below in the documentation The red highlighting within the editor reminds users that the tasks: section contains no valid -name: task definitions. This is part of WCA's code validation process which runs automatically and alerts users to syntax errors in their code. You can safely ignore these warnings for now, as you will be un-commenting and generating valid -name: task definitions in the following steps. ~/Documents/ansible-wca-demo-kit/install and configure Cockpit using Ansible/install_cockpit_single-task.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 --- - name : Install and configure Cockpit hosts : rhel become : true # module_defaults: # ansible.builtin.service: # enabled: true # state: started tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - The suggestion included Ansible best practices by using Fully Qualified Collection name. # - name: Install cockpit package # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used \"ansible.builtin.template\" module based on the \".j2\" file extension. # # Note - The suggestion set the file permissions (\"0644\"), owner, and group based on Ansible best practices. # - name: Copy cockpit.conf.j2 to /etc/cockpit # TASK 3 # # 3a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used the generic \"Start and enable service\" prompt # # and full Playbook context to infer the recommendation should start the \"cockpit\" service. # # 3b. Uncomment the \"module_defaults\" section at the top of the Playbook. # # 3c. Clear current task suggestion and request updated suggestion. # # Note - Ansible Lightspeed used the full Playbook context and evaluated the \"module_defaults\" # # when generating a suggestion. # # The updated suggestion no longer includes \"enabled:\" and \"state:\" arguments. # - name: Start and enable service # TASK 4 # # 4a. Uncomment task description below and generate an Ansible Playbook task suggestion. # # 4b. Save the Playbook. # - name: Wait 15 seconds port 9090 The install_cockpit_single-task.yml Playbook code above warrants some explanation before we move on with making AI-generated modifications to it: Line 2 essentially marks the beginning of the Playbook instructions, the purpose of which is to automate the process of installing and configuring Cockpit for Red Hat Ansible. Lines 3-4 define variables that will remain static throughout the remainder of the Playbook. These variables will be referenced by the AI-generated code suggestions at a later stage. This is a key capability of the offering and one which you will explore in much finer details later on in this module. Lines 6-9 are variables which have been commented out and therefore are invisible to the execution of the Ansible script and not examined by WCA for context when generating code recommendations. You will experiment with how removing the # comment blocks impacts the recommendations of task block code. \"Uncommenting\" these lines of code will make them viable for execution and these lines will afterwards be considered as valid Playbook \"context\" for AI code generation. Locate TASK 1 on Line 15 of the YAML file, which handles installation of Cockpit for Ansible. Cockpit is an interactive server administration interface that provides a graphical overview of statistics and configurations for a system or systems within a network. # - name: Install cockpit package Pay attention to the indentation and characters used on Line 15 , which in sequence from left to right are as follows: begins with Tab (or Space whitespaces) for indentation a # character to \"comment out\" the line's contents a whitespace Space character - name: which signifies the start of a Task definition and finally the natural language description of the Task INDENTATION LEVELS AND WHITESPACE Similar to Python, Ansible and YAML-based Playbooks are very sensitive to whitespacing and indentation. Indentations (such as the Tab in this example) denote different hierarchies and code nesting levels within the YAML structure. You may use Space instead of Tab if you prefer, but be sure to use indentations consistently : choose to use either Tab or Space for indenting lines of code, and do not interchange between the two. To generate code for TASK 1 , first uncomment the line of code (remove the # character from the start of a line). Highlight the line(s) of code you wish to uncomment and then press Cmd + ? for macOS or Ctrl + ? for Windows You can repeat those keystrokes with the line(s) selected to toggle between commenting or uncommenting lines of code Tip: commented out lines of code in VS Code will appear as green text Afterwards, Line 15 should look like the following \u2014 beginning with a single Tab - name : Install cockpit package Now you are ready to begin generating code. Place your cursor on Line 15 and hit Enter Wait for WCA to engage and generate the suggested (in grey, italicized text ) code block for executing the task This temporary code suggestion is entirely generated by AI As a user, you have the option to either: Accept the code recommendation as-given by pressing Tab Modify the recommended code by highlighting and replacing the italicized text FAILED TO CONNECT TO THE SERVER / \"YOU DON'T HAVE ACCESS TO IBM WATSONX...\" This warning will occur when the Ansible plugin for VS Code needs to be re-authenticated with WCA. It can occur after an extended period of inactivity or a system restart. For example, if your lab environment is running inside a VM, pausing or restarting the VM may produce this error. To re-authenticate: Sign out from the VS Code application by clicking the User icon [A] in the bottom-left corner of the interface, hover over your username, and then click Sign Out [B] If you are running this environment inside a virtual machine (VM) , closing and restarting the VM will not resolve the issue \u2014 you must sign out from the VS Code application, not the VM Once logged out, follow from Step 7 of the Setup & Troubleshooting to re-authenticate with WCA CODE RECOMMENDATIONS ARE NOT GENERATING Ansible Lightspeed and WCA will only generate code recommendations for Ansible Playbooks and YAML files. VS Code will typically auto-detect the programming language of the document you're working with, but on occassion you may need to manually specify the language. Even if working with a YAML file, you'll still need to specify the language mode as Ansible for the Lightspeed plugin to engage. To set the language mode correctly: In the bottom-right corner of the VS Code interface, hover over the Select Language Mode toggle [A] A console will appear at the top of VS Code with a drop-down list of options [B] Click Ansible from the suggested languages, or enter the text yourself and hit Enter Confirm that the Select Language Mode toggle in the bottom-right corner displays Ansible \"ANSIBLE-LINT IS NOT AVAILABLE.\" ansible-lint checks Playbooks for practices and behavior that could potentially be improved and can fix some of the most common ones for you. It will constantly check your Ansible syntax as you type and provide recommendations for how to improve it. You can safely ignore this error if it occurs during the lab exercises If you wish to install ansible-lint on your local machine, execute the following instruction within a Terminal console: python3 -m pip install --upgrade --user ansible-lint Hit Tab to accept the suggested code and then compare with the SOLUTION tab below. TEMPLATE SOLUTION # TASK 1 - name : Install cockpit package # TASK 1 - name : Install cockpit package ansible.builtin.package : name : cockpit state : present As part of the plain-text description of the Task, WCA was asked to include the cockpit Role, part of the Red Hat Enterprise Linux System Roles Certified Content Collection. The AI-generated code suggestion invoked a Fully Qualified Collection Name ( FQCN ) - ansible.builtin.package Making use of FQCNs where possible is a recommended best practice and is a prime example of the many ways in which the offering infuses post-processing capabilities within the AI-generated code produced by WCA. Additional examples of infusing best-practices into AI-generated code recommendations can be found in TASK 2 ( Line 21 of the unmodified template or Line 25 after Step 5 ): Uncomment - name: Copy cockpit.conf.j2 to /etc/cockpit Hit Enter to generate the task code recommendation and accept the AI-suggested code (without modifications) by pressing Tab Compare your results with the SOLUTION tab below TEMPLATE SOLUTION # TASK 2 - name : Copy cockpit.conf.j2 to /etc/cockpit # TASK 2 - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' The AI-generated code recommendation will copy cockpit.conf to the target host. Take note of the fact that the recommendation included the mode: argument and set the Linux file permissions to 0644 , neither of which were things explicitly requested in the Task -name description, but are both additions which adhere to best practices around defining Ansible automaton tasks. Setting a file permission to 0644 specifies read and write permissions for User and Group levels within the Linux OS, and provides only read permissions to all others.","title":"ii. Single task Ansible operations"},{"location":"generating/#iii-multi-task-ansible-operations","text":"Up to this point, we've kept a narrow aperture on AI-generated recommendations for single tasks \u2014 examining and experimenting with generating Ansible code task by task, one at a time. However, a powerful WCA feature is the ability to combine multiple task descriptions into a single natural language prompt; in turn, WCA is able to parse that instruction, decompose the instruction into discrete Ansible Task parts, and return a complete code recommendation for achieving the author's intended goal. Syntactically, multiple tasks are combined into a single natural language expression through the use of ampersand ( & ) characters. Simply write out all the automation task descriptions on a single line, separating each description with a & character. The line must also begin with a # character for reasons that will be explained shortly. To illustrate, let's look at a multi-task Ansible Playbook: install_cockpit_multi-task.yml The contents of this Playbook should look familiar to you already: it is essentially the same Playbook examined in Steps 1-6 ( install_cockpit_single-task.yml ), re-written in an equivalent multi-task expression Each of the Task descriptions from the previous Playbook have been consolidated into a single description on Line 12 , separated by & characters ~/Documents/ansible-wca-demo-kit/install and configure Cockpit using Ansible/install_cockpit_multi-task.yml 1 2 3 4 5 6 7 8 9 10 11 12 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : # Install cockpit package & Copy cockpit.conf.j2 to /etc/cockpit & Start and enable service & Wait 15 seconds port 9090 There are two crucial distinctions between single task and multi-task code generation: formatting and execution. Formatting : Notice that Line 12 does not begin with -name: , as was the case with single task descriptions Execution : In order to generate AI code recommendations for multi-task descriptions, Line 12 must stay commented out (the # must remain at the start of the line) What is the rationale behind this? When WCA's generative AI capabilities parse Line 12 , its output will include multiple -name: tasks, each containing potentially multiple lines of instructions, based on how many & -delineated task descriptions are included on the line. Therefore, the way in which the code generation step is executed on Line 12 is a consequence of the formatting decision. Execution of a code generation step on a commented-out ( # ) line containing & delineators is recognized by WCA as a unique case that will be acted upon as a multi-task statement. Place your cursor at the end of Line 12 , and without removing the # character, press Enter to execute the code generation step. Be aware that generating code for multi-task descriptions will take longer compared to a single task. Compare the MULTI-TASK solution tab with the SINGLE TASK solution (copied over from Step 6 ). How did the multi-task code generation fare compared to the single task approach? MULTI-TASK SINGLE TASK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : - name : Install cockpit package ansible.builtin.package : name : cockpit state : present - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' - name : Start and enable service ansible.builtin.service : name : cockpit.socket - name : Wait 15 seconds port 9090 ansible.builtin.wait_for : port : 9090 delay : 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 --- - name : Install and configure Cockpit hosts : rhel become : true module_defaults : ansible.builtin.service : enabled : true state : started tasks : - name : Install cockpit package ansible.builtin.package : name : cockpit state : present - name : Copy cockpit.conf.j2 to /etc/cockpit ansible.builtin.template : src : cockpit.conf.j2 dest : /etc/cockpit/cockpit.conf owner : root group : root mode : '0644' - name : Start and enable service ansible.builtin.service : name : cockpit state : started enabled : true - name : Wait 15 seconds port 9090 ansible.builtin.wait_for : port : 9090 delay : 15 Comparing the two results, the only notable difference between the two approaches are Lines 27-29 from the SINGLE TASK generative AI approach. Both the SINGLE TASK and MULTI-TASK suggestions for that particular task satisfy the request made by the user. However, whether the single task or multi-task approach resulted in a better code suggestion is up to the judgement of the programmer. Nearly 90% of the remaining code was identical between the two approaches and was achieved in far fewer lines of code (and less typing) using the multi-task approach. The variability of generative AI suggestions is a fascinating topic and one that we will dive more deeply into with the module ahead. Before moving on to other product features, experiment by creating a new Ansible Playbook in your workspace using the code template below. Suggestions will be given on how to perform the same automation task using single and multi-task generation approaches. Save the YAML file as create_ec2_single_multi.yml (if you forget to save the file, WCA will not generate recommendations) Copy the following code block to your clipboard using the + icon in the top-right corner of the panel and paste into the newly created YAML file HOW TO CREATE NEW YAML PLAYBOOKS Note: You need to copy and paste the contents of the Playbook into a New File... within the same Lightspeed project directory that was used for the previous lab modules in order for the VS Code extension to engage. To create a new YAML Playbook within a VS Code environment: a. Copy the contents of the Playbook to clipboard using the button in the top-right corner of the lab guide code block. b. Return to your VS Code environment. In the top-left corner of the interface, with your Ansible Lightspeed folder selected, click the New File... button. c. Name the file to a description of your choosing, ending with .yml as the filetype. Set it to CustomPlaybook.yml , for example. Save it to one of the directories in the ansible-wca-demo-kit folder. d. Paste the clipboard contents into the YAML file and follow along with the suggestions below. COPY AND PASTE CODE WITHIN THE VM Information \"copied\" to your local machine's clipboard cannot be \"pasted\" directly into the virtual machine (VM) environment or VS Code. If you wish to copy and paste instructions directly from the lab documentation, it is recommended that you open the GitHub instructions inside the VM's web browser (Firefox). This will allow you to copy instructions to the VM's clipboard and paste instructions inside the VS Code editor. ANSIBLE LIGHTSPEED IS MISSING OR CODE RECOMMENDATIONS ARE NOT GENERATING Ansible Lightspeed and WCA will only generate code recommendations for Ansible Playbooks and YAML files. VS Code will typically auto-detect the programming language of the document you're working with, but on occassion you may need to manually specify the language. Even if working with a YAML file, you'll still need to specify the language mode as Ansible for the Lightspeed plugin to engage. To set the language mode correctly: In the bottom-right corner of the VS Code interface, hover over the Select Language Mode toggle [A] A console will appear at the top of VS Code with a drop-down list of options [B] Click Ansible from the suggested languages, or enter the text yourself and hit Enter Confirm that the Select Language Mode toggle in the bottom-right corner displays Ansible ~/Documents/ansible-wca-demo-kit/create_ec2_single_multi.yml 1 2 3 4 --- - name : Provision an EC2 instance hosts : all tasks : First, configure the Playbook for single task generation by adding the following code snippet into Line 5 (remembering to properly indent with Tab or Space characters): - name : create vpc named demo Your workspace Playbook should look identical to the TEMPLATE tab below. Place your cursor at the end of the newly-created Line 5 and hit Enter to execute single task code generation. Compare your results to the SOLUTION tab. Record your results to a notepad so that you can compare the results later. TEMPLATE SOLUTION 1 2 3 4 5 --- - name : Provision an EC2 instance hosts : all tasks : - name : create vpc named demo 1 2 3 4 5 6 7 8 9 10 11 12 --- - name : Provision an EC2 instance hosts : all tasks : - name : create vpc named demo amazon.aws.ec2_vpc_net : name : demo cidr_block : 10.0.0.80/16 tags : Name : demo tenancy : default register : vpc Now it's time to re-write the Playbook for multi-task code generation. The beauty of natural language statements is that your approach can be as terse or verbose as you want. The more verbose and descriptive, the more prescriptive you can be in terms of influencing the AI-generated code recommendations from WCA. The topic of \"prompt tuning\" will be explored in much greater detail in the Task Description Tuning and Model Customization module. But for now: Replace all of the code inside create_ec2_single_multi.yml with the TEMPLATE code block (below) Place your cursor at the end of Line 5 (remember not to remove the # character) and hit Enter , then accept the recommendation with Tab Compare your results to the SOLUTION tab and to the result from Step 13 TEMPLATE SOLUTION 1 2 3 4 5 --- - name : Provision an EC2 instance hosts : all tasks : # create vpc named demo & create security group for demo vpc and allow ssh and http & create internet gateway named demogateway for demo vpc & create subnet named demosubnet in demo vpc & create route table for subnet demosubnet and gateway demogateway & create t2.small ec2 instance in subnet demosubnet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 --- - name : Provision an EC2 instance hosts : all tasks : # create vpc named demo & create security group for demo vpc and allow ssh and http & create internet gateway named demogateway for demo vpc & create subnet named demosubnet in demo vpc & create route table for subnet demosubnet and gateway demogateway & create t2.small ec2 instance in subnet demosubnet - name : Create vpc named demo amazon.aws.ec2_vpc_net : name : demo cidr_block : 10.0.0.80/16 tags : Name : demo tenancy : default register : vpc - name : Create security group for demo vpc and allow ssh and http amazon.aws.ec2_security_group : name : demo description : demo security group vpc_id : \"{ vpc.vpc.id }\" rules : - proto : tcp from_port : 22 to_port : 22 cidr_ip : 0.0.0.0/0 - proto : tcp from_port : 80 to_port : 80 cidr_ip : 0.0.0.0/0 register : sg - name : Create internet gateway named demogateway for demo vpc amazon.aws.ec2_vpc_igw : vpc_id : \"{ vpc.vpc.id }\" tags : Name : demogateway register : igw - name : Create subnet named demosubnet in demo vpc amazon.aws.ec2_vpc_subnet : cidr : 10.0.0.80/24 vpc_id : \"{ vpc.vpc.id }\" tags : Name : demosubnet register : subnet - name : Create route table for subnet demosubnet and gateway demogateway amazon.aws.ec2_vpc_route_table : vpc_id : \"{ vpc.vpc.id }\" tags : Name : demosubnet subnets : - \"{ subnet.subnet.id }\" routes : - dest : 0.0.0.0/0 gateway_id : \"{ igw.gateway_id }\" register : route_table - name : Create t2.small ec2 instance in subnet demosubnet amazon.aws.ec2_instance : key_name : \"{ _key_name_ }\" instance_type : t2.small image : \"{ _image_ }\" wait : true vpc_subnet_id : \"{ subnet.subnet.id }\" security_group : demo register : ec2 TIMEOUT WARNING It may take several moments for WCA to process and return code recommendations for a multi-task description as complex as this one. If you receive a time-out warning, try executing the code generation step by pressing Enter a second time.","title":"iii. Multi-task Ansible operations"},{"location":"generating/#iv-next-steps","text":"In the next section, you will examine in detail WCA's post-processing and content source attribution capabilities.","title":"iv. Next steps"},{"location":"on-premises/1/","text":"Objectives and Requirements On-Premises Installation and Deployment i. Introduction and hands-on objectives The On-Premises Installation and Deployment module provides comprehensive instructions for how to prepare, configure, and deploy a simulated on-premises cluster for IBM watsonx Code Assistant (WCA) on environments hosted by IBM Technology Zone (ITZ). By completing this module, participants will have learned and applied the skills necessary for deploying WCA on a client's on-premises infrastructure. The complete stack of technologies and services that you will deploy include: Red Hat OpenShift Container Platform v4.18 : a unified application development platform that lets clients build, modernize, and deploy applications at scale on their choice of hybrid cloud infrastructure. IBM Cloud Pak for Data v5.1.x : a set of services comprising a data fabric solution for data governance, data engineering, data analysis, and AI lifecycle tasks. IBM Software Hub v5.1 : a cloud-native solution that clients use to install, manage, and monitor IBM solutions on Red Hat OpenShift Container Platform. IBM watsonx Code Assistant v5.1 : a generative AI coding companion that provides contextually aware assistance for programming languages. Special acknowledgement and thanks to IBM colleagues Coralie Jonvel, Nelson Nunes, and Noe Samaille for adaptation of their deployment instructions for watsonx.ai on Red Hat OpenShift. INSERT ARCHITECTURE GRAPHIC AND DESCRIPTION HERE NEEDS DISCLAIMER THAT INSTALLATION WILL NOT RESULT IN A FULLY FUNCTIONAL CLUSTER (GPUs) ii. Infrastructure and resource requirements Requirements specific to the hands-on environment are outlined in the section below. Comprehensive details about the hardware requirements for x86_64 cluster services are available from IBM Software Hub documentation. Although the hands-on environment that will be provisioned in the next module utilizes a templated, pre-defined ITZ infrastructure configuration, it will be useful for those enrolled to understand the resources required to reproduce a similar cluster in real-world client scenarios. This includes details about the CPU, memory, GPU, and other hardware components required to support the necessary cluster services. IBM Software Hub platform Additional details available from IBM Documentation Node Role Number of Services Minimum Available vCPU Minimum Memory Minimum Storage Control plane 3 (for high availability) 4 vCPU per node (This configuration supports up to 24 worker nodes.) 16 GB RAM per node. This configuration supports up to 24 worker nodes. No additional storage is needed for IBM Software Hub. Infra 3 (recommended) 4 vCPU per node. This configuration supports up to 27 worker nodes. 24 GB RAM per node (This configuration supports up to 27 worker nodes.) See the Red Hat OpenShift Container Platform documentation for sizing guidance. Worker (compute) 3 or more worker (compute) nodes 16 vCPU per node Minimum : 64 GB RAM per node Recommended : 128 GB RAM per node 300 GB of storage space per node for storing container images locally. If you plan to install watsonx.ai, increase the storage to 500 GB per node. Load balancer 2 load balancer nodes 2 vCPU per node 4 GB RAM per node. Add another 4 GB of RAM for access restrictions and security control. Add 100 GB of root storage for access restrictions and security control. IBM Cloud Pak Foundational Services Additional details available from IBM Documentation vCPU Memory Storage Notes 4 vCPU 5 GB RAM Reference the v4.10 hardware requirements and recommendations . Required. IBM Cloud Pak Foundational Services are installed once for each instance of IBM Software Hub on the cluster. Red Hat OpenShift Container Platform (single node) Additional details available from IBM Documentation VM Role Minimum Available vCPU Minimum Memory Minimum Storage Bastion node 4 vCPU 8 GB RAM Allocate a minimum of 500 GB of disk space. The disk can be: in the same disk as the general bastion node storage; in a separate disk on the bastion node; or on external storage. Worker (compute) 16 vCPU 64 GB RAM Allocate a minimum of 300 GB of disk space on the node for image storage. IBM watsonx Code Assistant Additional details available from IBM Documentation vCPU Memory Storage Notes Operator pods: 0.1 vCPU Operator pods: 0.256 GB RAM Persistent storage: 120 GB Minimum resources for an installation with a single replica per service Catalog pods: 0.01 vCPU Catalog pods: 0.05 GB RAM Ephemeral storage: 0.4 GB The service requires at least two GPUs Operand: 7 vCPU Operand: 25 GB RAM Image storage: Up to 107 GB with all models GPU support is limited to: NVIDIA H100 GPUs with 80 GB RAM iii. Prerequisites checklist Register for an IBM Technology Zone account Participants require access to ITZ in order to reserve an environment and complete the hands-on work. If you do not yet have an account with the ITZ, you will need to register for one . Obtain an IBM Entitlement API key Participants require an entitlement API key to proceed with the on-premises installation. In order to retrieve the key: Use your IBMid and password to log in to the Container Software Library . Click the Entitlement keys tab from the navigation menu. Select Copy to capture the entitlement key to the clipboard. Paste and save the entitlement key to a text file on your local machine. iv. Next steps In the following module, you will provision an OpenShift Container Platform cluster via IBM Technology Zone, which will serve as the basis for the on-premises environment.","title":"1. Objectives and requirements"},{"location":"on-premises/1/#objectives-and-requirementson-premises-installation-and-deployment","text":"","title":"Objectives and RequirementsOn-Premises Installation and Deployment"},{"location":"on-premises/1/#i-introduction-and-hands-on-objectives","text":"The On-Premises Installation and Deployment module provides comprehensive instructions for how to prepare, configure, and deploy a simulated on-premises cluster for IBM watsonx Code Assistant (WCA) on environments hosted by IBM Technology Zone (ITZ). By completing this module, participants will have learned and applied the skills necessary for deploying WCA on a client's on-premises infrastructure. The complete stack of technologies and services that you will deploy include: Red Hat OpenShift Container Platform v4.18 : a unified application development platform that lets clients build, modernize, and deploy applications at scale on their choice of hybrid cloud infrastructure. IBM Cloud Pak for Data v5.1.x : a set of services comprising a data fabric solution for data governance, data engineering, data analysis, and AI lifecycle tasks. IBM Software Hub v5.1 : a cloud-native solution that clients use to install, manage, and monitor IBM solutions on Red Hat OpenShift Container Platform. IBM watsonx Code Assistant v5.1 : a generative AI coding companion that provides contextually aware assistance for programming languages. Special acknowledgement and thanks to IBM colleagues Coralie Jonvel, Nelson Nunes, and Noe Samaille for adaptation of their deployment instructions for watsonx.ai on Red Hat OpenShift. INSERT ARCHITECTURE GRAPHIC AND DESCRIPTION HERE NEEDS DISCLAIMER THAT INSTALLATION WILL NOT RESULT IN A FULLY FUNCTIONAL CLUSTER (GPUs)","title":"i. Introduction and hands-on objectives"},{"location":"on-premises/1/#ii-infrastructure-and-resource-requirements","text":"Requirements specific to the hands-on environment are outlined in the section below. Comprehensive details about the hardware requirements for x86_64 cluster services are available from IBM Software Hub documentation. Although the hands-on environment that will be provisioned in the next module utilizes a templated, pre-defined ITZ infrastructure configuration, it will be useful for those enrolled to understand the resources required to reproduce a similar cluster in real-world client scenarios. This includes details about the CPU, memory, GPU, and other hardware components required to support the necessary cluster services. IBM Software Hub platform Additional details available from IBM Documentation Node Role Number of Services Minimum Available vCPU Minimum Memory Minimum Storage Control plane 3 (for high availability) 4 vCPU per node (This configuration supports up to 24 worker nodes.) 16 GB RAM per node. This configuration supports up to 24 worker nodes. No additional storage is needed for IBM Software Hub. Infra 3 (recommended) 4 vCPU per node. This configuration supports up to 27 worker nodes. 24 GB RAM per node (This configuration supports up to 27 worker nodes.) See the Red Hat OpenShift Container Platform documentation for sizing guidance. Worker (compute) 3 or more worker (compute) nodes 16 vCPU per node Minimum : 64 GB RAM per node Recommended : 128 GB RAM per node 300 GB of storage space per node for storing container images locally. If you plan to install watsonx.ai, increase the storage to 500 GB per node. Load balancer 2 load balancer nodes 2 vCPU per node 4 GB RAM per node. Add another 4 GB of RAM for access restrictions and security control. Add 100 GB of root storage for access restrictions and security control. IBM Cloud Pak Foundational Services Additional details available from IBM Documentation vCPU Memory Storage Notes 4 vCPU 5 GB RAM Reference the v4.10 hardware requirements and recommendations . Required. IBM Cloud Pak Foundational Services are installed once for each instance of IBM Software Hub on the cluster. Red Hat OpenShift Container Platform (single node) Additional details available from IBM Documentation VM Role Minimum Available vCPU Minimum Memory Minimum Storage Bastion node 4 vCPU 8 GB RAM Allocate a minimum of 500 GB of disk space. The disk can be: in the same disk as the general bastion node storage; in a separate disk on the bastion node; or on external storage. Worker (compute) 16 vCPU 64 GB RAM Allocate a minimum of 300 GB of disk space on the node for image storage. IBM watsonx Code Assistant Additional details available from IBM Documentation vCPU Memory Storage Notes Operator pods: 0.1 vCPU Operator pods: 0.256 GB RAM Persistent storage: 120 GB Minimum resources for an installation with a single replica per service Catalog pods: 0.01 vCPU Catalog pods: 0.05 GB RAM Ephemeral storage: 0.4 GB The service requires at least two GPUs Operand: 7 vCPU Operand: 25 GB RAM Image storage: Up to 107 GB with all models GPU support is limited to: NVIDIA H100 GPUs with 80 GB RAM","title":"ii. Infrastructure and resource requirements"},{"location":"on-premises/1/#iii-prerequisites-checklist","text":"Register for an IBM Technology Zone account Participants require access to ITZ in order to reserve an environment and complete the hands-on work. If you do not yet have an account with the ITZ, you will need to register for one . Obtain an IBM Entitlement API key Participants require an entitlement API key to proceed with the on-premises installation. In order to retrieve the key: Use your IBMid and password to log in to the Container Software Library . Click the Entitlement keys tab from the navigation menu. Select Copy to capture the entitlement key to the clipboard. Paste and save the entitlement key to a text file on your local machine.","title":"iii. Prerequisites checklist"},{"location":"on-premises/1/#iv-next-steps","text":"In the following module, you will provision an OpenShift Container Platform cluster via IBM Technology Zone, which will serve as the basis for the on-premises environment.","title":"iv. Next steps"},{"location":"on-premises/2/","text":"Reserve an Environment On-Premises Installation and Deployment If you require assistance or run into issues with the hands-on lab, help is available. Environment issues: The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues: If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #wca-ansible-techzone-support Slack channel. i. Configuring the IBM Technology Zone reservation The foundation for the on-premises environment utilizes the OpenShift Cluster (VMware on IBM Cloud) - UPI - Public template from the collection of IBM Technolgy Zone (ITZ) Certified Base Images . Click the link below to request a reservation directly from ITZ: URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options , select Reserve now [A] . Supply additional details about the ITZ reservation request: RESERVATON POLICY NOTICE After selecting Education for the Purpose field, you may receive a pop-up notification stating that this environment is now being redirected to the OCPv base image hosted On-Prem for Education and Test . You can safely ignore this notice and close it by clicking the X in the top-right corner. Do not configure using the Poughkeepsie-based resource that the notice attempts to redirect you to \u2014 it will not allow you to configure the necessary hardware specifications. Continue with the ITZ reservation request form as detailed below. If the pop-up appears again later in the configuration steps, continue to disregard the notice. Field Value Name Give your reservation a unique name. Purpose Education Purpose Description Give your reservation a unique description. Preferred Geography Select the region and data center geographically closest to your location. End Date and Time Select a time and date for when the reservation will expire. OpenShift Version 4.16 Worker Node Count 3 Worker Node Flavor 32 vCPU x 128 GB - 300 GB ephemeral storage Storage ODF - 2 TB OCP/Kubernetes Cluster Network 10.128.0.0/14 OCP/Kubernetes Service Network 172.30.0.0/16 Enable nested hardware virtualization on workers No When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . PROVISIONING TIMES Cluster provisioning and deployment takes approximately 90 to 120 minutes to complete from the time that you click submit. Navigate to the My Reservations tab of the ITZ to monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \"Ready\" before attempting to start the lab. ii. Accessing the cluster Once the cluster has been successfully deployed, you will receive an email with the header: Reservation Ready on IBM Technology Zone . Confirm that the ITZ email states that Status Update: Ready [A] . Follow the link provided in the email, or access the My Reservations tab on ITZ to access your reservation. Record the following connection details variables for the OpenShift Container Platform (OCP) cluster to a notepad: Username Password Click the blue Open your IBM Cloud environment button at the top of the page to launch a new browser window for accessing the OCP cluster. Choose the kube:admin log in option and then provide the following credentials: Username: kubeadmin Password: password recorded in Step 6 At this stage, you should have successfully logged in to the OCP dashboard. After logging into the OCP dashboard, copy the URL of the page (from your web browser) and record that to a notepad as OCP dashboard URL . This will be referenced in subsequent modules. iii. Next steps In the following module, you will access and configure the cluster's bastion node.","title":"2. Reserve an environment"},{"location":"on-premises/2/#reserve-an-environmenton-premises-installation-and-deployment","text":"If you require assistance or run into issues with the hands-on lab, help is available. Environment issues: The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues: If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #wca-ansible-techzone-support Slack channel.","title":"Reserve an EnvironmentOn-Premises Installation and Deployment"},{"location":"on-premises/2/#i-configuring-the-ibm-technology-zone-reservation","text":"The foundation for the on-premises environment utilizes the OpenShift Cluster (VMware on IBM Cloud) - UPI - Public template from the collection of IBM Technolgy Zone (ITZ) Certified Base Images . Click the link below to request a reservation directly from ITZ: URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options , select Reserve now [A] . Supply additional details about the ITZ reservation request: RESERVATON POLICY NOTICE After selecting Education for the Purpose field, you may receive a pop-up notification stating that this environment is now being redirected to the OCPv base image hosted On-Prem for Education and Test . You can safely ignore this notice and close it by clicking the X in the top-right corner. Do not configure using the Poughkeepsie-based resource that the notice attempts to redirect you to \u2014 it will not allow you to configure the necessary hardware specifications. Continue with the ITZ reservation request form as detailed below. If the pop-up appears again later in the configuration steps, continue to disregard the notice. Field Value Name Give your reservation a unique name. Purpose Education Purpose Description Give your reservation a unique description. Preferred Geography Select the region and data center geographically closest to your location. End Date and Time Select a time and date for when the reservation will expire. OpenShift Version 4.16 Worker Node Count 3 Worker Node Flavor 32 vCPU x 128 GB - 300 GB ephemeral storage Storage ODF - 2 TB OCP/Kubernetes Cluster Network 10.128.0.0/14 OCP/Kubernetes Service Network 172.30.0.0/16 Enable nested hardware virtualization on workers No When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . PROVISIONING TIMES Cluster provisioning and deployment takes approximately 90 to 120 minutes to complete from the time that you click submit. Navigate to the My Reservations tab of the ITZ to monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \"Ready\" before attempting to start the lab.","title":"i. Configuring the IBM Technology Zone reservation"},{"location":"on-premises/2/#ii-accessing-the-cluster","text":"Once the cluster has been successfully deployed, you will receive an email with the header: Reservation Ready on IBM Technology Zone . Confirm that the ITZ email states that Status Update: Ready [A] . Follow the link provided in the email, or access the My Reservations tab on ITZ to access your reservation. Record the following connection details variables for the OpenShift Container Platform (OCP) cluster to a notepad: Username Password Click the blue Open your IBM Cloud environment button at the top of the page to launch a new browser window for accessing the OCP cluster. Choose the kube:admin log in option and then provide the following credentials: Username: kubeadmin Password: password recorded in Step 6 At this stage, you should have successfully logged in to the OCP dashboard. After logging into the OCP dashboard, copy the URL of the page (from your web browser) and record that to a notepad as OCP dashboard URL . This will be referenced in subsequent modules.","title":"ii. Accessing the cluster"},{"location":"on-premises/2/#iii-next-steps","text":"In the following module, you will access and configure the cluster's bastion node.","title":"iii. Next steps"},{"location":"on-premises/3/","text":"Bastion Host Setup On-Premises Installation and Deployment The following section is based off of IBM Documentation write-ups that detail how to install IBM Software Hub on a Red Hat OpenShift Container Platform cluster. Reference the instructions in full at the following resource: Installing the IBM Software Hub command-line interface . i. Connect to the bastion host Before configuring the bastion host node, you will need to retrieve its connectivity and access control details. Return to the My Reservations page on the IBM Technology Zone (ITZ) and open the dashboard for the OpenShift Container Platform (OCP) cluster. Scroll down to the bottom of the summary page and locate the following key-value pairs related to the bastion host node. Copy these to a notepad for reference later: Bastion Password Bastion RDP Address Bastion SSH Connection Bastion Username There are also key-value pairs related to the OCP cluster that, for convenience, you should also record at this time \u2014 they will be used in later parts of the lab: Cluster Admin Username Cluster Admin Password OCP Console Open a Terminal (Windows Terminal or the Terminal built into VS Code are good alternatives if you're on a PC). Copy the Bastion SSH Connection recorded in Step 2 and paste it into the terminal console. Hit Enter to execute and create an SSH connection to the bastion host. When prompted Are you sure you want to continue connecting (yes/no/fingerprint)? , enter yes and hit Enter to proceed. The console will return a Welcome to IBM Technology Zone once connected to the bastion host, at which point you must authenticate. Authenticate when prompted to do so by providing the Bastion Password recorded in Step 2. If the console now reads [itzuser@localhost ~]$ then you have successfully accessed the bastion host. ii. OpenShift command line interface (oc) Next, install the OpenShift Command Line Interface (CLI), designated oc , to programmatically perform work with the bastion node. Retrieve the OCP dashboard URL (recorded in Step 8 of the previous module). Obtain the OpenShift base domain by extracting the portion of the URL that matches the position highlighted in the sample URL below. Extract the portion after .apps. until the end of the URL. The characters in your OCP dashboard URL will differ. https://console-openshift-console.apps. 678a250b79141644e78804e0.ocp.techzone.ibm.com In this example, the value of the OpenShift base domain is 678a250b79141644e78804e0.ocp.techzone.ibm.com Record your OCP cluster's value to a notepad for future reference. The following instruction set, when executed within a Terminal window, will install the OpenShift CLI on the bastion host node. However, the instructions require some modification before they will successfully execute. Install OpenShift CLI export OPENSHIFT_BASE_DOMAIN = <CHANGE_ME> wget --no-check-certificate https://downloads-openshift-console.apps. ${ OPENSHIFT_BASE_DOMAIN } /amd64/linux/oc.tar tar -xvf oc.tar chmod +x oc sudo mv oc /usr/local/bin/oc Copy the instructions above and paste into a notepad. Replace the highlighted <CHANGE_ME> text with the OpenShift base domain value recorded in Step 4. Copy the modified notepad instructions to your clipboard and paste into your Terminal console. Press Enter to execute the instructions. The installation should only take a few minutes to complete. Once finished, try typing oc into the console window and hit Enter . The console output should verify that oc (the OpenShift CLI) has been successfully installed on the bastion host. iii. Podman install IBM Cloud Pak for Data (CP4D)'s installer requires containers and as such Podman must be set up on the bastion host ahead of time. Using the connected Terminal console, execute the following instruction to install Podman: sudo yum install -y podman The process will take approximately 1 minute to complete. iv. Environment variables Next, you must set the environment variables needed for installation of CP4D on the cluster. The list is quite extensive and long, so rather than set these one at a time it's recommended that you first compile them into a single file on the bastion host. Afterwards you can set all the variables automatically using the single file. Below is a code block containing all of the necessary CP4D environment variables. Copy the contents of the entire block to your clipboard and paste into a notepad. CP4D Environment Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #=============================================================================== # Cloud Pak for Data installation variables #=============================================================================== #------------------------------------------------------------------------------ # Client workstation #------------------------------------------------------------------------------ # Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI. # # To export these variables, you must uncomment each command in this sec-tion. #export CPD_CLI_MANAGE_WORKSPACE=<enter a fully qualified directory> # following lines could be used for environment with self-signed certificates and hostnames not resolved by DNS server #export OLM_UTILS_LAUNCH_ARGS=\"-v ./api-wxai.pem:/etc/k8scert --env K8S_AUTH_SSL_CA_CERT=/etc/k8scert --add-host oauth-openshift.apps.ocpinstall.gym.lan:192.168.252.4 --add-host api.ocpinstall.gym.lan:192.168.252.3\" export PATH = \"/home/itzuser/cpd-cli-linux-EE-14.1.0-1189\" : $PATH #----------------------------------------------------------------------------- # Cluster #------------------------------------------------------------------------------ export OCP_URL = api.ocpinstall.gym.lan:6443 #export OPENSHIFT_TYPE=<enter your deployment type> #export IMAGE_ARCH=amd64 export OCP_USERNAME = kubeadmin export OCP_PASSWORD = mcnfM-MdnmC-y6ZES-cAKjj # export OCP_TOKEN=<enter your token> export SERVER_ARGUMENTS = \"--server= ${ OCP_URL } \" export LOGIN_ARGUMENTS = \"--username= ${ OCP_USERNAME } --password= ${ OCP_PASSWORD } \" # export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\" export CPDM_OC_LOGIN = \"cpd-cli manage login-to-ocp ${ SERVER_ARGUMENTS } ${ LOGIN_ARGUMENTS } \" export OC_LOGIN = \"oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } \" #------------------------------------------------------------------------------ # Projects #------------------------------------------------------------------------------ export PROJECT_LICENSE_SERVICE = cpd-license export PROJECT_SCHEDULING_SERVICE = cpd-scheduling export PROJECT_CPD_INST_OPERATORS = cpd-operators export PROJECT_CPD_INST_OPERANDS = cpd-watsonx # export PROJECT_CPD_INSTANCE_TETHERED=<enter your tethered project> # export PROJECT_CPD_INSTANCE_TETHERED_LIST=<a comma-separated list of teth-ered projects> #------------------------------------------------------------------------------ # Storage #------------------------------------------------------------------------------ export STG_CLASS_BLOCK = ocs-storagecluster-ceph-rbd export STG_CLASS_FILE = ocs-storagecluster-cephfs #------------------------------------------------------------------------------ # IBM Entitled Registry #------------------------------------------------------------------------------ export IBM_ENTITLEMENT_KEY = XXX.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE3MDU5MzkzNzcsImp0aSI6IjVjM2EzMzQ1MzdlNTQ5ZWE5MzRmNGZmYzNmYzc5MWNjIn0.Xa0C23QmwCsOR9whmf8OkiId--SXVZ4pxw6lrIvvb_M #------------------------------------------------------------------------------ # Cloud Pak for Data version #------------------------------------------------------------------------------ export VERSION = 5 .1.0 #------------------------------------------------------------------------------ # Components #------------------------------------------------------------------------------ #export COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_plat-form # export COMPONENTS_TO_SKIP=<component-ID-1>,<component-ID-2> #export COMPONENTS=ibm-cert-manager,ibm-licensing,cpfs,scheduler,cpd_plat-form,wml,ws,watsonx_ai export COMPONENTS = cpd_platform,watsonx_ai You must make modifications to Line 19 , Line 23 , and Line 49 of the CP4D Environment Variables to tailor the variables to your specific cluster. Line 19: set the value of export OCP_URL= equal to the value of OCP dashboard URL that was recorded in Step 8 of the previous module. For example: export OCP_URL = https://console-openshift-console.apps.678a250b79141644e78804e0.ocp.techzone.ibm.com Line 23: set the value of export OCP_PASSWORD= equal to the value of Cluster Admin Password recorded in Step 2 of this module. For example: export OCP_PASSWORD = password1234 Line 49: set the value of export IBM_ENTITLEMENT_KEY= equal to the value of the key specific to your IBM account. Reference the IBM Entitlement API Key that was generated in the iii. Prerequisites checklist section of Module 1 (Objectives and Requirements) . Instructions for how to generate an IBM Entitlement API Key are provided in that section. For example: export IBM_ENTITLEMENT_KEY = verylongAPIkey1234 With your Terminal console, execute the following instruction to open the vi editor and create a shell script named cpd_vars.sh on the bastion host: vi cpd_vars.sh Copy the modified CP4D Environment Variables contents from the notepad (Step 9) to your machine's clipboard. Switch back to the Terminal where the VI editor is now open and press the I key to enable inserting text. Press Cmd + V (or Ctrl + V ) to paste the contents from your clipboard. To save, press Esc and then type :wq followed by Enter to write the file and exit the editor. v. Cloud Pak for Data command line interface (cpd-cli) Now that the environment variables have been set, the next step towards installing CP4D is preparing the command line interface ( cpd-cli ). First, execute the following command in the Terminal console so that subsequent actions taken are done with elevated permissions: sudo bash Then copy the following code block and execute it within the console to install cpd-cli . Installation should only take a few moments to complete. wget https://github.com/IBM/cpd-cli/releases/download/v14.1.0/cpd-cli-linux-EE-14.1.0.tgz tar -xzf cpd-cli-linux-EE-14.1.0.tgz export PATH = \" $( pwd ) /cpd-cli-linux-EE-14.1.0-1189\" : $PATH Verify that the CLI has been successfully integrated by typing cpd-cli and then Enter into the console. Engage the command line interface and ensure that the Terminal console is executing all subsequent instructions with full (root) privileges: echo $PATH cpd-cli manage restart-container This step will take a minute or two before completing. Once it has completed, you can verify the status of the restarted container by typing podman ps and Enter , which should return the result of a single container running on the bastion host. Source the newly-configured environment variables with the following command: source cpd_vars.sh Test that the login for oc command line is now functioning properly: oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } The command line should return back with a prompt describing You have access to ... projects which indicates that oc and the environment variables have been correctly configured. vi. Next steps At this stage the bastion host node has been fully configured ahead of installing the necessary software, which will be covered in the subsequent modules. SESSION TIMEOUTS AND LOGGING BACK IN Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <...> placeholders with values specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"3. Bastion host setup"},{"location":"on-premises/3/#bastion-host-setupon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation write-ups that detail how to install IBM Software Hub on a Red Hat OpenShift Container Platform cluster. Reference the instructions in full at the following resource: Installing the IBM Software Hub command-line interface .","title":"Bastion Host SetupOn-Premises Installation and Deployment"},{"location":"on-premises/3/#i-connect-to-the-bastion-host","text":"Before configuring the bastion host node, you will need to retrieve its connectivity and access control details. Return to the My Reservations page on the IBM Technology Zone (ITZ) and open the dashboard for the OpenShift Container Platform (OCP) cluster. Scroll down to the bottom of the summary page and locate the following key-value pairs related to the bastion host node. Copy these to a notepad for reference later: Bastion Password Bastion RDP Address Bastion SSH Connection Bastion Username There are also key-value pairs related to the OCP cluster that, for convenience, you should also record at this time \u2014 they will be used in later parts of the lab: Cluster Admin Username Cluster Admin Password OCP Console Open a Terminal (Windows Terminal or the Terminal built into VS Code are good alternatives if you're on a PC). Copy the Bastion SSH Connection recorded in Step 2 and paste it into the terminal console. Hit Enter to execute and create an SSH connection to the bastion host. When prompted Are you sure you want to continue connecting (yes/no/fingerprint)? , enter yes and hit Enter to proceed. The console will return a Welcome to IBM Technology Zone once connected to the bastion host, at which point you must authenticate. Authenticate when prompted to do so by providing the Bastion Password recorded in Step 2. If the console now reads [itzuser@localhost ~]$ then you have successfully accessed the bastion host.","title":"i. Connect to the bastion host"},{"location":"on-premises/3/#ii-openshift-command-line-interface-oc","text":"Next, install the OpenShift Command Line Interface (CLI), designated oc , to programmatically perform work with the bastion node. Retrieve the OCP dashboard URL (recorded in Step 8 of the previous module). Obtain the OpenShift base domain by extracting the portion of the URL that matches the position highlighted in the sample URL below. Extract the portion after .apps. until the end of the URL. The characters in your OCP dashboard URL will differ. https://console-openshift-console.apps. 678a250b79141644e78804e0.ocp.techzone.ibm.com In this example, the value of the OpenShift base domain is 678a250b79141644e78804e0.ocp.techzone.ibm.com Record your OCP cluster's value to a notepad for future reference. The following instruction set, when executed within a Terminal window, will install the OpenShift CLI on the bastion host node. However, the instructions require some modification before they will successfully execute. Install OpenShift CLI export OPENSHIFT_BASE_DOMAIN = <CHANGE_ME> wget --no-check-certificate https://downloads-openshift-console.apps. ${ OPENSHIFT_BASE_DOMAIN } /amd64/linux/oc.tar tar -xvf oc.tar chmod +x oc sudo mv oc /usr/local/bin/oc Copy the instructions above and paste into a notepad. Replace the highlighted <CHANGE_ME> text with the OpenShift base domain value recorded in Step 4. Copy the modified notepad instructions to your clipboard and paste into your Terminal console. Press Enter to execute the instructions. The installation should only take a few minutes to complete. Once finished, try typing oc into the console window and hit Enter . The console output should verify that oc (the OpenShift CLI) has been successfully installed on the bastion host.","title":"ii. OpenShift command line interface (oc)"},{"location":"on-premises/3/#iii-podman-install","text":"IBM Cloud Pak for Data (CP4D)'s installer requires containers and as such Podman must be set up on the bastion host ahead of time. Using the connected Terminal console, execute the following instruction to install Podman: sudo yum install -y podman The process will take approximately 1 minute to complete.","title":"iii. Podman install"},{"location":"on-premises/3/#iv-environment-variables","text":"Next, you must set the environment variables needed for installation of CP4D on the cluster. The list is quite extensive and long, so rather than set these one at a time it's recommended that you first compile them into a single file on the bastion host. Afterwards you can set all the variables automatically using the single file. Below is a code block containing all of the necessary CP4D environment variables. Copy the contents of the entire block to your clipboard and paste into a notepad. CP4D Environment Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #=============================================================================== # Cloud Pak for Data installation variables #=============================================================================== #------------------------------------------------------------------------------ # Client workstation #------------------------------------------------------------------------------ # Set the following variables if you want to override the default behavior of the Cloud Pak for Data CLI. # # To export these variables, you must uncomment each command in this sec-tion. #export CPD_CLI_MANAGE_WORKSPACE=<enter a fully qualified directory> # following lines could be used for environment with self-signed certificates and hostnames not resolved by DNS server #export OLM_UTILS_LAUNCH_ARGS=\"-v ./api-wxai.pem:/etc/k8scert --env K8S_AUTH_SSL_CA_CERT=/etc/k8scert --add-host oauth-openshift.apps.ocpinstall.gym.lan:192.168.252.4 --add-host api.ocpinstall.gym.lan:192.168.252.3\" export PATH = \"/home/itzuser/cpd-cli-linux-EE-14.1.0-1189\" : $PATH #----------------------------------------------------------------------------- # Cluster #------------------------------------------------------------------------------ export OCP_URL = api.ocpinstall.gym.lan:6443 #export OPENSHIFT_TYPE=<enter your deployment type> #export IMAGE_ARCH=amd64 export OCP_USERNAME = kubeadmin export OCP_PASSWORD = mcnfM-MdnmC-y6ZES-cAKjj # export OCP_TOKEN=<enter your token> export SERVER_ARGUMENTS = \"--server= ${ OCP_URL } \" export LOGIN_ARGUMENTS = \"--username= ${ OCP_USERNAME } --password= ${ OCP_PASSWORD } \" # export LOGIN_ARGUMENTS=\"--token=${OCP_TOKEN}\" export CPDM_OC_LOGIN = \"cpd-cli manage login-to-ocp ${ SERVER_ARGUMENTS } ${ LOGIN_ARGUMENTS } \" export OC_LOGIN = \"oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } \" #------------------------------------------------------------------------------ # Projects #------------------------------------------------------------------------------ export PROJECT_LICENSE_SERVICE = cpd-license export PROJECT_SCHEDULING_SERVICE = cpd-scheduling export PROJECT_CPD_INST_OPERATORS = cpd-operators export PROJECT_CPD_INST_OPERANDS = cpd-watsonx # export PROJECT_CPD_INSTANCE_TETHERED=<enter your tethered project> # export PROJECT_CPD_INSTANCE_TETHERED_LIST=<a comma-separated list of teth-ered projects> #------------------------------------------------------------------------------ # Storage #------------------------------------------------------------------------------ export STG_CLASS_BLOCK = ocs-storagecluster-ceph-rbd export STG_CLASS_FILE = ocs-storagecluster-cephfs #------------------------------------------------------------------------------ # IBM Entitled Registry #------------------------------------------------------------------------------ export IBM_ENTITLEMENT_KEY = XXX.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE3MDU5MzkzNzcsImp0aSI6IjVjM2EzMzQ1MzdlNTQ5ZWE5MzRmNGZmYzNmYzc5MWNjIn0.Xa0C23QmwCsOR9whmf8OkiId--SXVZ4pxw6lrIvvb_M #------------------------------------------------------------------------------ # Cloud Pak for Data version #------------------------------------------------------------------------------ export VERSION = 5 .1.0 #------------------------------------------------------------------------------ # Components #------------------------------------------------------------------------------ #export COMPONENTS=ibm-cert-manager,ibm-licensing,scheduler,cpfs,cpd_plat-form # export COMPONENTS_TO_SKIP=<component-ID-1>,<component-ID-2> #export COMPONENTS=ibm-cert-manager,ibm-licensing,cpfs,scheduler,cpd_plat-form,wml,ws,watsonx_ai export COMPONENTS = cpd_platform,watsonx_ai You must make modifications to Line 19 , Line 23 , and Line 49 of the CP4D Environment Variables to tailor the variables to your specific cluster. Line 19: set the value of export OCP_URL= equal to the value of OCP dashboard URL that was recorded in Step 8 of the previous module. For example: export OCP_URL = https://console-openshift-console.apps.678a250b79141644e78804e0.ocp.techzone.ibm.com Line 23: set the value of export OCP_PASSWORD= equal to the value of Cluster Admin Password recorded in Step 2 of this module. For example: export OCP_PASSWORD = password1234 Line 49: set the value of export IBM_ENTITLEMENT_KEY= equal to the value of the key specific to your IBM account. Reference the IBM Entitlement API Key that was generated in the iii. Prerequisites checklist section of Module 1 (Objectives and Requirements) . Instructions for how to generate an IBM Entitlement API Key are provided in that section. For example: export IBM_ENTITLEMENT_KEY = verylongAPIkey1234 With your Terminal console, execute the following instruction to open the vi editor and create a shell script named cpd_vars.sh on the bastion host: vi cpd_vars.sh Copy the modified CP4D Environment Variables contents from the notepad (Step 9) to your machine's clipboard. Switch back to the Terminal where the VI editor is now open and press the I key to enable inserting text. Press Cmd + V (or Ctrl + V ) to paste the contents from your clipboard. To save, press Esc and then type :wq followed by Enter to write the file and exit the editor.","title":"iv. Environment variables"},{"location":"on-premises/3/#v-cloud-pak-for-data-command-line-interface-cpd-cli","text":"Now that the environment variables have been set, the next step towards installing CP4D is preparing the command line interface ( cpd-cli ). First, execute the following command in the Terminal console so that subsequent actions taken are done with elevated permissions: sudo bash Then copy the following code block and execute it within the console to install cpd-cli . Installation should only take a few moments to complete. wget https://github.com/IBM/cpd-cli/releases/download/v14.1.0/cpd-cli-linux-EE-14.1.0.tgz tar -xzf cpd-cli-linux-EE-14.1.0.tgz export PATH = \" $( pwd ) /cpd-cli-linux-EE-14.1.0-1189\" : $PATH Verify that the CLI has been successfully integrated by typing cpd-cli and then Enter into the console. Engage the command line interface and ensure that the Terminal console is executing all subsequent instructions with full (root) privileges: echo $PATH cpd-cli manage restart-container This step will take a minute or two before completing. Once it has completed, you can verify the status of the restarted container by typing podman ps and Enter , which should return the result of a single container running on the bastion host. Source the newly-configured environment variables with the following command: source cpd_vars.sh Test that the login for oc command line is now functioning properly: oc login ${ OCP_URL } ${ LOGIN_ARGUMENTS } The command line should return back with a prompt describing You have access to ... projects which indicates that oc and the environment variables have been correctly configured.","title":"v. Cloud Pak for Data command line interface (cpd-cli)"},{"location":"on-premises/3/#vi-next-steps","text":"At this stage the bastion host node has been fully configured ahead of installing the necessary software, which will be covered in the subsequent modules. SESSION TIMEOUTS AND LOGGING BACK IN Be aware that SSH connections made over Terminal will time out after a long period of inactivity or due to a connection error. If you need to log back into the bastion terminal, follow the procedure below. Replace the <...> placeholders with values specific to your environment. Log back into the bastion node: ssh itzuser@api.67828ca5e432cac47ccc4230.ocp.techzone.ibm.com -p 40222 <BASTION_PWD> Engage the sudo (privileged access) session: sudo bash Source the environment variables stored in cpd_vars.sh : source cpd_vars.sh Log back into OpenShift: ${ OC_LOGIN } Log back into cpd-cli : ${ CPDM_OC_LOGIN }","title":"vi. Next steps"},{"location":"on-premises/4/","text":"Cluster Preparation On-Premises Installation and Deployment The following section is based off of IBM Documentation detailing how to prepare an OpenShift cluster for IBM Cloud Pak for Data. Reference the instructions in full at the following resource: Preparing Your Cluster for IBM Software Hub . i. Change the process IDs limit With a newly installed cluster, a KubeletConfig will need to be manually created before the cluster's process IDs can be modified. This file will define the podPidsLimit and maxPods variables for the environment. KUBELETCONFIG TEST You can test whether a KubeletConfig file exists on the system by executing the following command: oc get kubeletconfig Copy the contents of the following code block and then execute within your Terminal console to generate a new KubeletConfig file: oc apply -f - << EOF apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpd-watsonx-kubeletconfig spec: kubeletConfig: podPidsLimit: 16384 podsPerCore: 0 maxPods: 500 machineConfigPoolSelector: matchExpressions: - key: pools.operator.machineconfiguration.openshift.io/worker operator: Exists EOF Use the CP4D command line ( cpd-cli ) to log into OCP by executing the following code: cpd-cli manage login-to-ocp \\ --username = ${ OCP_USERNAME } \\ --password = ${ OCP_PASSWORD } \\ --server = ${ OCP_URL } The console will return two [SUCCESS] statements indicating that the login-to-ocp command ran successfully . ii. Update cluster global pull secret Use cpd-cli to manage the creation or updating of the global image pull secret via the add-icr-cred-to-global-pull-secret command. Execute the following command within the Terminal console: cpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key = ${ IBM_ENTITLEMENT_KEY } The console will return two [SUCCESS] statements indicating that the command ran successfully. Now you must update all nodes across the cluster using OpenShift command line ( oc ). Execute the following instructions via the Terminal console: oc login ${ OCP_URL } --username = ${ OCP_USERNAME } --password = ${ OCP_PASSWORD } oc get mcp Once completed, execute the following statement to check the status of the cluster nodes (this can be performed periodically to track the progress of the node updates): cpd-cli manage oc get nodes Wait until the STATUS returns for all nodes (3 master nodes, 3 storage nodes, and 3 worker nodes) all report as Ready . iii. Next steps In the following module, you will install the necessary prerequisite software required to deploy IBM Cloud Pak for Data and IBM watsonx Code Assistant.","title":"4. Cluster preparation"},{"location":"on-premises/4/#cluster-preparationon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation detailing how to prepare an OpenShift cluster for IBM Cloud Pak for Data. Reference the instructions in full at the following resource: Preparing Your Cluster for IBM Software Hub .","title":"Cluster PreparationOn-Premises Installation and Deployment"},{"location":"on-premises/4/#i-change-the-process-ids-limit","text":"With a newly installed cluster, a KubeletConfig will need to be manually created before the cluster's process IDs can be modified. This file will define the podPidsLimit and maxPods variables for the environment. KUBELETCONFIG TEST You can test whether a KubeletConfig file exists on the system by executing the following command: oc get kubeletconfig Copy the contents of the following code block and then execute within your Terminal console to generate a new KubeletConfig file: oc apply -f - << EOF apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpd-watsonx-kubeletconfig spec: kubeletConfig: podPidsLimit: 16384 podsPerCore: 0 maxPods: 500 machineConfigPoolSelector: matchExpressions: - key: pools.operator.machineconfiguration.openshift.io/worker operator: Exists EOF Use the CP4D command line ( cpd-cli ) to log into OCP by executing the following code: cpd-cli manage login-to-ocp \\ --username = ${ OCP_USERNAME } \\ --password = ${ OCP_PASSWORD } \\ --server = ${ OCP_URL } The console will return two [SUCCESS] statements indicating that the login-to-ocp command ran successfully .","title":"i. Change the process IDs limit"},{"location":"on-premises/4/#ii-update-cluster-global-pull-secret","text":"Use cpd-cli to manage the creation or updating of the global image pull secret via the add-icr-cred-to-global-pull-secret command. Execute the following command within the Terminal console: cpd-cli manage add-icr-cred-to-global-pull-secret --entitled_registry_key = ${ IBM_ENTITLEMENT_KEY } The console will return two [SUCCESS] statements indicating that the command ran successfully. Now you must update all nodes across the cluster using OpenShift command line ( oc ). Execute the following instructions via the Terminal console: oc login ${ OCP_URL } --username = ${ OCP_USERNAME } --password = ${ OCP_PASSWORD } oc get mcp Once completed, execute the following statement to check the status of the cluster nodes (this can be performed periodically to track the progress of the node updates): cpd-cli manage oc get nodes Wait until the STATUS returns for all nodes (3 master nodes, 3 storage nodes, and 3 worker nodes) all report as Ready .","title":"ii. Update cluster global pull secret"},{"location":"on-premises/4/#iii-next-steps","text":"In the following module, you will install the necessary prerequisite software required to deploy IBM Cloud Pak for Data and IBM watsonx Code Assistant.","title":"iii. Next steps"},{"location":"on-premises/5/","text":"Install Prerequisite Software On-Premises Installation and Deployment i. Install the Red Hat OpenShift cert-manager Following the release of IBM Software Hub , previous methods for installing IBM Cloud Pak for Data (CP4D) that relied on IBM Cert Manager are no longer required. IBM Software Hub is now the recommended path and will be the method adhered to with the following hands-on instructions. Review the latest documentation on IBM Software Hub to determine the appropriate version needed for your client opportunity and OpenShift cluster version. The following module follows the documentation for installing IBM Software Hub's cert-manager-operator.v.13.0 for OpenShift Container Platform (OCP) v4.16. Return to the OCP Dashboard ( Step 8 of Module 2 ) with a web browser. From the left-hand navigation menu, navigate to Operators > OperatorHub . Into the filter box, type cert-manager Operator for Red Hat OpenShift and hit Enter . Click the filtered result with the same name. From the configuration screen, select the following options: Channel: stable-v1 Version: 1.13.0 When ready, click the blue Install button in the top-left corner A new page will load, summarizing the settings of the proposed Operator. Verify that the details are correct, scroll down to the bottom of the page, and click the blue Install button to execute. The page will refresh periodically as the Operator is installed on the cluster. This may take several minutes to complete. You can track the progress of the Operator installation by drilling down into Operators > Installed Operators from the left-hand side of the OCP Dashboard. Scroll down until you locate the openshift-cert-manager-operator entry in the table. Monitor the progress by observing changes to the fourth column of the table. Wait until Operator shows a status of Upgrade Available \u2014 then click the hyperlinked text to navigate to the Subscription Details panel. Under the Subscription Details header, you should see three panels: Update Channel , Update Approval , and Update Status *. Click the hyperlinked 1 requires approval text next to the Update Status panel. A new page, InstallPlan details for install-49nfj , will load. Click the blue Preview InstallPlan button. When the panel refreshes, click the blue Approve button. From the left-hand navigation bar of the OCP Dashboard, once again navigate into Operators > Installed Operators to watch the progress of the update. The cert-manager Operator for Red Hat OpenShift will show a Status of Installing while the update is underway. Wait until the status changes to Succeeded . Verify that cert-manager pods are up and running by executing the following command with your Terminal console: oc get pods -n cert-manager The console should return output closely resembling the following: NAME READY STATUS RESTARTS AGE cert-manager-bd7fbb9fc-wvbbt 1 /1 Running 0 3m39s cert-manager-cainjector-56cc5f9868-7g9z7 1 /1 Running 0 4m5s cert-manager-webhook-d4f79d7f7-9dg9w 1 /1 Running 0 4m9s At this point, the OpenShift cert-manager has been successfully deployed on the cluster. ii. Operators for GPUs GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. Although GPUs will not be available to deploy or interact with for the On-Premises Installation and Deployment L4 training modules, participants will still be able to practice and learn the skills needed to prepare a cluster for GPUs. The following section is based off a selection of the complete IBM Documentation available for Installing operators for services that require GPUs . This section will cover all of the necessary configuration and setup required to make GPUs available to an IBM watsonx Code Asssitant service \u2014 shy of actually getting to use the GPUs with the on-premises deployment. Participants will still be able to interact with GPU-powered instances in the latter IBM Cloud (SaaS) modules of the L4 curriculum. Services such as IBM watsonx Code Assistant (on-premises), which requires access to GPUs, need to install several Operators on the OpenShift cluster to support the management of NVIDIA software components. Those components, in turn, are needed to provision the GPUs for access by the cluster. IBM watsonx Code Assistant requires that the following Operators be installed: - Node Feature Discovery Operator : within the openshift-nfd namespace - NVIDIA GPU Operator : within the nvidia-gpu-operator namespace - Red Hat OpenShift AI The following section will provide the instructions necessary to replicate this procedure. Participants are welcome to practice this with the L4 environment provided \u2014 just be aware that no physical GPU hardware will be available or connected at the conclusion of these steps. iii. Install the Node Feature Discovery Operator First, you will use the Terminal console to programmatically create a namespace openshift-nfd for the Node Feature Discovery (NDF) Operator. The following instruction set will create a namespace Custom Resource (CR) that defines the openshift-ndf namespace and then saves the YAML file to nfd-namespace.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-nfd labels: name: openshift-nfd openshift.io/cluster-monitoring: \"true\" EOF Install the NDF Operator within the openshift-nfd namespace that was created in Step 11 by first defining the following objects. This will create an OperatorGroup CR and subsequently save the YAML file to operatorgroup.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd EOF Create a Subscription CR and save the YAML file to nfd-sub.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Check on the status of the OpenShift pods with the following command: od get pods -n openshift-nfd Initially, the console may return a response such as No resources found in openshift-ndf namespace. This likely is because the pod is still provisioning. Check the status of the deployment with the following steps: Return to the OCP Dashboard and navigate to Operators > Installed Operators Within the table, look for a resource named nfd If the code from Step 13 was only recently executed, the Managed Namespaces of this resource may be showing as None and the Status as Unknown Wait a few moments (and refresh the page if you wish) until the nfd resource is replaced by Node Feature Discovery Operator , which should then belong to the openshift-nfd Managed Namespace and have a Succeeded Status Try executing the instruction from Step 14 a second time within the Terminal console and observe the updated pod status Create a NodeFeatureDiscovery CR and save the YAML file to NodeFeatureDiscovery.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: instance: \"\" # instance is empty by default operand: image: registry.redhat.io/openshift4/ose-node-feature-discovery-rhel9:v4.16 imagePullPolicy: Always workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time configurable ## and require a nfd-worker restart to take effect after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: cpu: cpuid: # NOTE: whitelist has priority over blacklist attributeBlacklist: - \"BMI1\" - \"BMI2\" - \"CLMUL\" - \"CMOV\" - \"CX16\" - \"ERMS\" - \"F16C\" - \"HTT\" - \"LZCNT\" - \"MMX\" - \"MMXEXT\" - \"NX\" - \"POPCNT\" - \"RDRAND\" - \"RDSEED\" - \"RDTSCP\" - \"SGX\" - \"SSE\" - \"SSE2\" - \"SSE3\" - \"SSE4.1\" - \"SSE4.2\" - \"SSSE3\" attributeWhitelist: kernel: kconfigFile: \"/path/to/kconfig\" configOpts: - \"NO_HZ\" - \"X86\" - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: - \"class\" - \"vendor\" EOF As before, check on the status of the OpenShift pods with the following command: od get pods -n openshift-nfd iv. Install the NVIDIA GPU Operator The following section is based off a selection of the complete NVIDIA Corporation documentation for Installing the NVIDIA GPU Operator on OpenShift . Create the nvidia-gpu-operator namespace by executing the following code block with a Terminal console: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Define the OperatorGroup within the same namespace: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Execute the following code block within a Terminal console to get the channel version (required for Step 20), as well as create the gpu-operator-certified Operator within the nvidia-gpu-operator namespace: export CHANNEL = $( oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath = '{.status.defaultChannel}' ; echo ) export CURRENT_CSV = $( oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"' $CHANNEL '\") | .currentCSV' ; echo ) oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"${CHANNEL}\" installPlanApproval: Manual name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"${CURRENT_CSV}\" EOF Execute the following command to verify the status of the install plan: oc get installplan -n nvidia-gpu-operator The console will return a statement that Operator's APPROVAL status is currently set to Manual and incomplete ( APPROVED state is set to false ). Therefore, you will need to approve the install plan by executing the following command: export INSTALL_PLAN = $( oc get installplan -n nvidia-gpu-operator -oname ) oc patch $INSTALL_PLAN -n nvidia-gpu-operator --type merge --patch '{\"spec\":{\"approved\":true }}' Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the update. Wait until the NVIDIA GPU Operator appears in the table with Status set to Succeeded before continuing. Create a ClusterPolicy by executing the following instructions: oc get csv -n nvidia-gpu-operator ${ CURRENT_CSV } -ojsonpath ={ .metadata.annotations.alm-examples } | jq . [ 0 ] > clusterpolicy.json oc apply -f clusterpolicy.json ATTENTION: It will take approximately 5 to 10 minutes for the ClusterPolicy effects to initialize. Take a 10 minute break and then return to carry out Step 22. After waiting, check the status of the pods. If GPUs are detected within the cluster (they won't be in this demonstration environment), they will be included in the read-out. Execute the following command with the Terminal console: oc get pods -n nvidia-gpu-operator NOTE TO THE AUTHOR THIS WILL BE THE SECTION YOU WILL NEED TO FILL OUT WITH A DEDICATED CLUSTER WITH GPU ACCESS, PURELY FOR DEMONSTRATION PURPOSES. REFER TO 00:21:00 TIMESTAMP IN THE RECORDING. v. Install Red Hat OpenShift AI The following steps are extracted from the complete documentation available from IBM Documentation for Installing Red Hat OpenShift AI . IBM watsonx Code Assistant (on-premises) requires installation and configuration of Red Hat OpenShift AI on the OCP cluster. In the following section, you will: Install the Operator for Red Hat OpenShift AI (v2.13) Create a DSCInitialization instance Create a DataScienceCluster instance Edit the model inferencing configuration Create the redhat-ods-operator Project on the OCP cluster with the following command: oc new-project redhat-ods-operator Define the rhods-operator Operator Group within the project defined in Step 23: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: rhods-operator namespace: redhat-ods-operator EOF Create the Operator Subscription for rhods-operator within the same project: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: rhods-operator namespace: redhat-ods-operator spec: name: rhods-operator channel: stable-2.13 source: redhat-operators sourceNamespace: openshift-marketplace config: env: - name: \"DISABLE_DSC_CONFIG\" EOF ATTENTION: It will take approximately 5 minutes for the Operator installation to complete. Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the deployment. Create a DSCInitialization object default-dsci in redhat-ods-monitoring project: cat <<EOF |oc apply -f - apiVersion: dscinitialization.opendatahub.io/v1 kind: DSCInitialization metadata: name: default-dsci spec: applicationsNamespace: redhat-ods-applications monitoring: managementState: Managed namespace: redhat-ods-monitoring serviceMesh: managementState: Removed trustedCABundle: managementState: Managed customCABundle: \"\" EOF Monitor the progress of the DSCInitialization object by executing the following command: oc get dscinitialization Wait for the DSCInitialization object ( dscinitialization ) to return a Ready status before continuing on to Step 27. Create a DataScienceCluster object named default-dsc within the same project: cat <<EOF |oc apply -f - apiVersion: datasciencecluster.opendatahub.io/v1 kind: DataScienceCluster metadata: name: default-dsc spec: components: codeflare: managementState: Removed dashboard: managementState: Removed datasciencepipelines: managementState: Removed kserve: managementState: Managed defaultDeploymentMode: RawDeployment serving: managementState: Removed name: knative-serving kueue: managementState: Removed modelmeshserving: managementState: Removed ray: managementState: Removed trainingoperator: managementState: Managed trustyai: managementState: Removed workbenches: managementState: Removed EOF The Red Hat OpenShift AI Operator automatically installs and manages services that are listed as Managed . Services with a status of Removed are ignored and will not be installed. Monitor the progress of the DataScienceCluster object by executing the following command: oc get default-dsc Wait for the DataScienceCluster object ( default-dsc ) to return a Ready status before continuing on to Step 28. Execute the following instruction to check the status of pods within the redhat-ods-applications project: oc get pods -n redhat-ods-applications Confirm that the following pods are deployed and have a running ( Ready ) status: - kserve-controller-manager-0123xyz... - kubeflow-training-operator-0123xyz... - odh-model-controller-0123xyz... Now you must edit the inferenceservice-config configuration map within the redhat-ods-applications project. This is best achieved by using the OCP Dashboard web interface. From the OCP Dashboard, navigate into Workloads > Configmaps** From the Project list, select redhat-ods-applications Click the inferenceservice-config resource and open the YAML tab In the metadata.annotations section of the file, add a line which reads opendatahub.io/managed: 'false' The resulting YAML file should resemble the following: metadata : annotations : internal.config.kubernetes.io/previousKinds : ConfigMap internal.config.kubernetes.io/previousNames : inferenceservice-config internal.config.kubernetes.io/previousNamespaces : opendatahub opendatahub.io/managed : 'false' Within the same YAML file, look for the following line (under the ingress section): \"domainTemplate\" : \"{ .Name }-{ .Namespace }.{ .IngressDomain }\" , Replace the line with the following: \"domainTemplate\" : \"example.com\" , Click Save to finalize the changes to the YAML file. vi. Next steps At this stage, all of the necessary prerequisites have been installed and you are ready to begin installation of an IBM Software Hub instance on the OCP cluster.","title":"5. Install prerequisite software"},{"location":"on-premises/5/#install-prerequisite-softwareon-premises-installation-and-deployment","text":"","title":"Install Prerequisite SoftwareOn-Premises Installation and Deployment"},{"location":"on-premises/5/#i-install-the-red-hat-openshift-cert-manager","text":"Following the release of IBM Software Hub , previous methods for installing IBM Cloud Pak for Data (CP4D) that relied on IBM Cert Manager are no longer required. IBM Software Hub is now the recommended path and will be the method adhered to with the following hands-on instructions. Review the latest documentation on IBM Software Hub to determine the appropriate version needed for your client opportunity and OpenShift cluster version. The following module follows the documentation for installing IBM Software Hub's cert-manager-operator.v.13.0 for OpenShift Container Platform (OCP) v4.16. Return to the OCP Dashboard ( Step 8 of Module 2 ) with a web browser. From the left-hand navigation menu, navigate to Operators > OperatorHub . Into the filter box, type cert-manager Operator for Red Hat OpenShift and hit Enter . Click the filtered result with the same name. From the configuration screen, select the following options: Channel: stable-v1 Version: 1.13.0 When ready, click the blue Install button in the top-left corner A new page will load, summarizing the settings of the proposed Operator. Verify that the details are correct, scroll down to the bottom of the page, and click the blue Install button to execute. The page will refresh periodically as the Operator is installed on the cluster. This may take several minutes to complete. You can track the progress of the Operator installation by drilling down into Operators > Installed Operators from the left-hand side of the OCP Dashboard. Scroll down until you locate the openshift-cert-manager-operator entry in the table. Monitor the progress by observing changes to the fourth column of the table. Wait until Operator shows a status of Upgrade Available \u2014 then click the hyperlinked text to navigate to the Subscription Details panel. Under the Subscription Details header, you should see three panels: Update Channel , Update Approval , and Update Status *. Click the hyperlinked 1 requires approval text next to the Update Status panel. A new page, InstallPlan details for install-49nfj , will load. Click the blue Preview InstallPlan button. When the panel refreshes, click the blue Approve button. From the left-hand navigation bar of the OCP Dashboard, once again navigate into Operators > Installed Operators to watch the progress of the update. The cert-manager Operator for Red Hat OpenShift will show a Status of Installing while the update is underway. Wait until the status changes to Succeeded . Verify that cert-manager pods are up and running by executing the following command with your Terminal console: oc get pods -n cert-manager The console should return output closely resembling the following: NAME READY STATUS RESTARTS AGE cert-manager-bd7fbb9fc-wvbbt 1 /1 Running 0 3m39s cert-manager-cainjector-56cc5f9868-7g9z7 1 /1 Running 0 4m5s cert-manager-webhook-d4f79d7f7-9dg9w 1 /1 Running 0 4m9s At this point, the OpenShift cert-manager has been successfully deployed on the cluster.","title":"i. Install the Red Hat OpenShift cert-manager"},{"location":"on-premises/5/#ii-operators-for-gpus","text":"GPUs NOT SUPPORTED FOR ON-PREMISES DEPLOYMENTS Resource and budget constraints for IBM Technology Zone and the IBM Enablement teams means that GPUs are unavailable for the on-premises portion of the Level 4 curriculum. The NVIDIA A100 or H100 GPUs required are simply too cost-prohibitive to be made available for individual IBMers and business partners. GPUs cannot be shared in a multi-tenant access pattern for IBM watsonx Code Assistant \u2014 and as such at minimum two of such cards would need to be made available for every L4 reservation. These costs are beyond the scope of what can be supported by this training. Participants will have access to GPUs for the IBM Cloud (SaaS) portion of the Level 4 curriculum. Although GPUs will not be available to deploy or interact with for the On-Premises Installation and Deployment L4 training modules, participants will still be able to practice and learn the skills needed to prepare a cluster for GPUs. The following section is based off a selection of the complete IBM Documentation available for Installing operators for services that require GPUs . This section will cover all of the necessary configuration and setup required to make GPUs available to an IBM watsonx Code Asssitant service \u2014 shy of actually getting to use the GPUs with the on-premises deployment. Participants will still be able to interact with GPU-powered instances in the latter IBM Cloud (SaaS) modules of the L4 curriculum. Services such as IBM watsonx Code Assistant (on-premises), which requires access to GPUs, need to install several Operators on the OpenShift cluster to support the management of NVIDIA software components. Those components, in turn, are needed to provision the GPUs for access by the cluster. IBM watsonx Code Assistant requires that the following Operators be installed: - Node Feature Discovery Operator : within the openshift-nfd namespace - NVIDIA GPU Operator : within the nvidia-gpu-operator namespace - Red Hat OpenShift AI The following section will provide the instructions necessary to replicate this procedure. Participants are welcome to practice this with the L4 environment provided \u2014 just be aware that no physical GPU hardware will be available or connected at the conclusion of these steps.","title":"ii. Operators for GPUs"},{"location":"on-premises/5/#iii-install-the-node-feature-discovery-operator","text":"First, you will use the Terminal console to programmatically create a namespace openshift-nfd for the Node Feature Discovery (NDF) Operator. The following instruction set will create a namespace Custom Resource (CR) that defines the openshift-ndf namespace and then saves the YAML file to nfd-namespace.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: openshift-nfd labels: name: openshift-nfd openshift.io/cluster-monitoring: \"true\" EOF Install the NDF Operator within the openshift-nfd namespace that was created in Step 11 by first defining the following objects. This will create an OperatorGroup CR and subsequently save the YAML file to operatorgroup.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd spec: targetNamespaces: - openshift-nfd EOF Create a Subscription CR and save the YAML file to nfd-sub.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Check on the status of the OpenShift pods with the following command: od get pods -n openshift-nfd Initially, the console may return a response such as No resources found in openshift-ndf namespace. This likely is because the pod is still provisioning. Check the status of the deployment with the following steps: Return to the OCP Dashboard and navigate to Operators > Installed Operators Within the table, look for a resource named nfd If the code from Step 13 was only recently executed, the Managed Namespaces of this resource may be showing as None and the Status as Unknown Wait a few moments (and refresh the page if you wish) until the nfd resource is replaced by Node Feature Discovery Operator , which should then belong to the openshift-nfd Managed Namespace and have a Succeeded Status Try executing the instruction from Step 14 a second time within the Terminal console and observe the updated pod status Create a NodeFeatureDiscovery CR and save the YAML file to NodeFeatureDiscovery.yaml . Copy and paste the following code block into the Terminal console, then hit Enter to execute: oc apply -f - <<EOF apiVersion: nfd.openshift.io/v1 kind: NodeFeatureDiscovery metadata: name: nfd-instance namespace: openshift-nfd spec: instance: \"\" # instance is empty by default operand: image: registry.redhat.io/openshift4/ose-node-feature-discovery-rhel9:v4.16 imagePullPolicy: Always workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time configurable ## and require a nfd-worker restart to take effect after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: cpu: cpuid: # NOTE: whitelist has priority over blacklist attributeBlacklist: - \"BMI1\" - \"BMI2\" - \"CLMUL\" - \"CMOV\" - \"CX16\" - \"ERMS\" - \"F16C\" - \"HTT\" - \"LZCNT\" - \"MMX\" - \"MMXEXT\" - \"NX\" - \"POPCNT\" - \"RDRAND\" - \"RDSEED\" - \"RDTSCP\" - \"SGX\" - \"SSE\" - \"SSE2\" - \"SSE3\" - \"SSE4.1\" - \"SSE4.2\" - \"SSSE3\" attributeWhitelist: kernel: kconfigFile: \"/path/to/kconfig\" configOpts: - \"NO_HZ\" - \"X86\" - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: - \"class\" - \"vendor\" EOF As before, check on the status of the OpenShift pods with the following command: od get pods -n openshift-nfd","title":"iii. Install the Node Feature Discovery Operator"},{"location":"on-premises/5/#iv-install-the-nvidia-gpu-operator","text":"The following section is based off a selection of the complete NVIDIA Corporation documentation for Installing the NVIDIA GPU Operator on OpenShift . Create the nvidia-gpu-operator namespace by executing the following code block with a Terminal console: oc apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Define the OperatorGroup within the same namespace: oc apply -f - <<EOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Execute the following code block within a Terminal console to get the channel version (required for Step 20), as well as create the gpu-operator-certified Operator within the nvidia-gpu-operator namespace: export CHANNEL = $( oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath = '{.status.defaultChannel}' ; echo ) export CURRENT_CSV = $( oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"' $CHANNEL '\") | .currentCSV' ; echo ) oc apply -f - <<EOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"${CHANNEL}\" installPlanApproval: Manual name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"${CURRENT_CSV}\" EOF Execute the following command to verify the status of the install plan: oc get installplan -n nvidia-gpu-operator The console will return a statement that Operator's APPROVAL status is currently set to Manual and incomplete ( APPROVED state is set to false ). Therefore, you will need to approve the install plan by executing the following command: export INSTALL_PLAN = $( oc get installplan -n nvidia-gpu-operator -oname ) oc patch $INSTALL_PLAN -n nvidia-gpu-operator --type merge --patch '{\"spec\":{\"approved\":true }}' Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the update. Wait until the NVIDIA GPU Operator appears in the table with Status set to Succeeded before continuing. Create a ClusterPolicy by executing the following instructions: oc get csv -n nvidia-gpu-operator ${ CURRENT_CSV } -ojsonpath ={ .metadata.annotations.alm-examples } | jq . [ 0 ] > clusterpolicy.json oc apply -f clusterpolicy.json ATTENTION: It will take approximately 5 to 10 minutes for the ClusterPolicy effects to initialize. Take a 10 minute break and then return to carry out Step 22. After waiting, check the status of the pods. If GPUs are detected within the cluster (they won't be in this demonstration environment), they will be included in the read-out. Execute the following command with the Terminal console: oc get pods -n nvidia-gpu-operator NOTE TO THE AUTHOR THIS WILL BE THE SECTION YOU WILL NEED TO FILL OUT WITH A DEDICATED CLUSTER WITH GPU ACCESS, PURELY FOR DEMONSTRATION PURPOSES. REFER TO 00:21:00 TIMESTAMP IN THE RECORDING.","title":"iv. Install the NVIDIA GPU Operator"},{"location":"on-premises/5/#v-install-red-hat-openshift-ai","text":"The following steps are extracted from the complete documentation available from IBM Documentation for Installing Red Hat OpenShift AI . IBM watsonx Code Assistant (on-premises) requires installation and configuration of Red Hat OpenShift AI on the OCP cluster. In the following section, you will: Install the Operator for Red Hat OpenShift AI (v2.13) Create a DSCInitialization instance Create a DataScienceCluster instance Edit the model inferencing configuration Create the redhat-ods-operator Project on the OCP cluster with the following command: oc new-project redhat-ods-operator Define the rhods-operator Operator Group within the project defined in Step 23: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: rhods-operator namespace: redhat-ods-operator EOF Create the Operator Subscription for rhods-operator within the same project: cat <<EOF |oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: rhods-operator namespace: redhat-ods-operator spec: name: rhods-operator channel: stable-2.13 source: redhat-operators sourceNamespace: openshift-marketplace config: env: - name: \"DISABLE_DSC_CONFIG\" EOF ATTENTION: It will take approximately 5 minutes for the Operator installation to complete. Return to the OCP Dashboard and drill down into Operators > Installed Operators to monitor the progress of the deployment. Create a DSCInitialization object default-dsci in redhat-ods-monitoring project: cat <<EOF |oc apply -f - apiVersion: dscinitialization.opendatahub.io/v1 kind: DSCInitialization metadata: name: default-dsci spec: applicationsNamespace: redhat-ods-applications monitoring: managementState: Managed namespace: redhat-ods-monitoring serviceMesh: managementState: Removed trustedCABundle: managementState: Managed customCABundle: \"\" EOF Monitor the progress of the DSCInitialization object by executing the following command: oc get dscinitialization Wait for the DSCInitialization object ( dscinitialization ) to return a Ready status before continuing on to Step 27. Create a DataScienceCluster object named default-dsc within the same project: cat <<EOF |oc apply -f - apiVersion: datasciencecluster.opendatahub.io/v1 kind: DataScienceCluster metadata: name: default-dsc spec: components: codeflare: managementState: Removed dashboard: managementState: Removed datasciencepipelines: managementState: Removed kserve: managementState: Managed defaultDeploymentMode: RawDeployment serving: managementState: Removed name: knative-serving kueue: managementState: Removed modelmeshserving: managementState: Removed ray: managementState: Removed trainingoperator: managementState: Managed trustyai: managementState: Removed workbenches: managementState: Removed EOF The Red Hat OpenShift AI Operator automatically installs and manages services that are listed as Managed . Services with a status of Removed are ignored and will not be installed. Monitor the progress of the DataScienceCluster object by executing the following command: oc get default-dsc Wait for the DataScienceCluster object ( default-dsc ) to return a Ready status before continuing on to Step 28. Execute the following instruction to check the status of pods within the redhat-ods-applications project: oc get pods -n redhat-ods-applications Confirm that the following pods are deployed and have a running ( Ready ) status: - kserve-controller-manager-0123xyz... - kubeflow-training-operator-0123xyz... - odh-model-controller-0123xyz... Now you must edit the inferenceservice-config configuration map within the redhat-ods-applications project. This is best achieved by using the OCP Dashboard web interface. From the OCP Dashboard, navigate into Workloads > Configmaps** From the Project list, select redhat-ods-applications Click the inferenceservice-config resource and open the YAML tab In the metadata.annotations section of the file, add a line which reads opendatahub.io/managed: 'false' The resulting YAML file should resemble the following: metadata : annotations : internal.config.kubernetes.io/previousKinds : ConfigMap internal.config.kubernetes.io/previousNames : inferenceservice-config internal.config.kubernetes.io/previousNamespaces : opendatahub opendatahub.io/managed : 'false' Within the same YAML file, look for the following line (under the ingress section): \"domainTemplate\" : \"{ .Name }-{ .Namespace }.{ .IngressDomain }\" , Replace the line with the following: \"domainTemplate\" : \"example.com\" , Click Save to finalize the changes to the YAML file.","title":"v. Install Red Hat OpenShift AI"},{"location":"on-premises/5/#vi-next-steps","text":"At this stage, all of the necessary prerequisites have been installed and you are ready to begin installation of an IBM Software Hub instance on the OCP cluster.","title":"vi. Next steps"},{"location":"on-premises/6/","text":"Install IBM Software Hub On-Premises Installation and Deployment The following section is based off of IBM Documentation for Preparing to install an instance of IBM Software Hub . i. Install shared components Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the Red Hat OpenShift Container Platform (OCP) cluster: a license service and a scheduler . Three environment variables were \"set\" earlier in this lab, which are pertinent now to the license service (as well as the scheduler) that IBM Software Hub requires. These include: $PROJECT_LICENSE_SERVICE $PROJECT_SCHEDULING_SERVICE $VERSION These variables were defined within the Cloud Pak for Data (CP4D) Environment Variable ( Step 8 of Module 3 ) on Lines 35, 36, and 53, respectively. Refresh yourself on these variable's values by executing the following within a Terminal console: echo $PROJECT_LICENSE_SERVICE echo $PROJECT_SCHEDULING_SERVICE echo $VERSION Install the license service components by executing the following command: cpd-cli manage apply-cluster-components \\ --release = ${ VERSION } \\ --license_acceptance = true --licensing_ns = ${ PROJECT_LICENSE_SERVICE } ATTENTION: The installation process will take approximately 6 minutes to complete. Install the scheduling service by executing the following command: cpd-cli manage apply-scheduler \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --scheduler_ns = ${ PROJECT_SCHEDULING_SERVICE } The scheduler service should be installed and deployed relatively quickly. At this stage, the cluster is ready for installation of IBM Software Hub. ii. Install an instance of IBM Software Hub Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the Red Hat OpenShift Container Platform (OCP) cluster: a license service and a scheduler .","title":"6. Install IBM Software Hub"},{"location":"on-premises/6/#install-ibm-software-hubon-premises-installation-and-deployment","text":"The following section is based off of IBM Documentation for Preparing to install an instance of IBM Software Hub .","title":"Install IBM Software HubOn-Premises Installation and Deployment"},{"location":"on-premises/6/#i-install-shared-components","text":"Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the Red Hat OpenShift Container Platform (OCP) cluster: a license service and a scheduler . Three environment variables were \"set\" earlier in this lab, which are pertinent now to the license service (as well as the scheduler) that IBM Software Hub requires. These include: $PROJECT_LICENSE_SERVICE $PROJECT_SCHEDULING_SERVICE $VERSION These variables were defined within the Cloud Pak for Data (CP4D) Environment Variable ( Step 8 of Module 3 ) on Lines 35, 36, and 53, respectively. Refresh yourself on these variable's values by executing the following within a Terminal console: echo $PROJECT_LICENSE_SERVICE echo $PROJECT_SCHEDULING_SERVICE echo $VERSION Install the license service components by executing the following command: cpd-cli manage apply-cluster-components \\ --release = ${ VERSION } \\ --license_acceptance = true --licensing_ns = ${ PROJECT_LICENSE_SERVICE } ATTENTION: The installation process will take approximately 6 minutes to complete. Install the scheduling service by executing the following command: cpd-cli manage apply-scheduler \\ --release = ${ VERSION } \\ --license_acceptance = true \\ --scheduler_ns = ${ PROJECT_SCHEDULING_SERVICE } The scheduler service should be installed and deployed relatively quickly. At this stage, the cluster is ready for installation of IBM Software Hub.","title":"i. Install shared components"},{"location":"on-premises/6/#ii-install-an-instance-of-ibm-software-hub","text":"Before an instance of IBM Software Hub can be installed, you need to install a set of shared services on the Red Hat OpenShift Container Platform (OCP) cluster: a license service and a scheduler .","title":"ii. Install an instance of IBM Software Hub"},{"location":"on-premises/7/","text":"Install IBM watsonx Code Assistant On-Premises Installation and Deployment Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min] i. Part 1 And ii. Part 2 test","title":"7. Install watsonx Code Assistant"},{"location":"on-premises/7/#install-ibm-watsonx-code-assistanton-premises-installation-and-deployment","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min]","title":"Install IBM watsonx Code AssistantOn-Premises Installation and Deployment"},{"location":"on-premises/7/#i-part-1","text":"And","title":"i. Part 1"},{"location":"on-premises/7/#ii-part-2","text":"test","title":"ii. Part 2"},{"location":"on-premises/8/","text":"Conclusion On-Premises Installation and Deployment Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min] i. Part 1 And ii. Part 2 test","title":"8. Conclusion"},{"location":"on-premises/8/#conclusionon-premises-installation-and-deployment","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) demonstrates key elements and hands-on components of the Generating Code module. [10 min]","title":"ConclusionOn-Premises Installation and Deployment"},{"location":"on-premises/8/#i-part-1","text":"And","title":"i. Part 1"},{"location":"on-premises/8/#ii-part-2","text":"test","title":"ii. Part 2"}]}